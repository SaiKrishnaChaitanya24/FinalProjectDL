{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GIN Using Default values: gossipcop and content"
      ],
      "metadata": {
        "id": "qjgm1eanNPSq"
      },
      "id": "qjgm1eanNPSq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63beec2a",
      "metadata": {
        "id": "63beec2a",
        "outputId": "e326c4cd-c008-453f-8ff4-af8a175ec8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-scatter in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-sparse in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Requirement already satisfied: torch-geometric in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.4.0)\n",
            "Requirement already satisfied: numpy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: pyparsing in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: tqdm in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df7bff1",
      "metadata": {
        "id": "9df7bff1"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import UPFD  #importing the UPFD Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143934f8",
      "metadata": {
        "id": "143934f8",
        "outputId": "af24cd49-dbe0-48e6-c70b-8468e16dff01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gossipcop Dataset\n",
            "Train Samples:  1092\n",
            "Validation Samples:  546\n",
            "Test Samples:  3826\n"
          ]
        }
      ],
      "source": [
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\",split=\"test\")\n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"val\")\n",
        "print(\"Gossipcop Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_gos))\n",
        "print(\"Validation Samples: \", len(val_data_gos))\n",
        "print(\"Test Samples: \", len(test_data_gos))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3024a14f",
      "metadata": {
        "id": "3024a14f",
        "outputId": "5f54c473-0030-43a8-c911-2cbd1563da21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          1, 70, 74],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "         73, 74, 75]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_gos[0].edge_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Dataset Using DataLoader for train data and test data of Politifact"
      ],
      "metadata": {
        "id": "LONmSjJeNeSY"
      },
      "id": "LONmSjJeNeSY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622da001",
      "metadata": {
        "id": "622da001"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_data_gos, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_data_gos, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a0204ba",
      "metadata": {
        "id": "5a0204ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from torch.nn import LeakyReLU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining Architecture of GIN Using 3 GIN Convolutional layers  with 3 unit MLP"
      ],
      "metadata": {
        "id": "QO-B5DkANgbX"
      },
      "id": "QO-B5DkANgbX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf36bc1",
      "metadata": {
        "id": "faf36bc1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU\n",
        "from torch_geometric.nn import GINConv, global_max_pool\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(Linear(in_channels, hidden_channels[0]), ReLU()))\n",
        "        self.conv2 = GINConv(Sequential(Linear(hidden_channels[0], hidden_channels[1]), ReLU()))\n",
        "        self.conv3 = GINConv(Sequential(Linear(hidden_channels[1], hidden_channels[2]), ReLU()))\n",
        "\n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
        "\n",
        "        # dropouts\n",
        "        self.dp1 = torch.nn.Dropout(0.2)\n",
        "        self.dp2 = torch.nn.Dropout(0.2)\n",
        "        self.dp3 = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = F.relu(h)\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp3(h)\n",
        "\n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad66f3dd",
      "metadata": {
        "id": "ad66f3dd"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train and Test of model using Hyperparameters"
      ],
      "metadata": {
        "id": "Q804hDI3Nj2-"
      },
      "id": "Q804hDI3Nj2-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e291935d",
      "metadata": {
        "id": "e291935d",
        "outputId": "c2ad7c74-2835-4af2-8323-e81af3f7165c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #using adam optimiser and the learning rate as lr= 0.0001\n",
        "lossff = torch.nn.BCELoss() #binary cross entropy loss\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438b443e",
      "metadata": {
        "id": "438b443e"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bed326",
      "metadata": {
        "id": "61bed326",
        "outputId": "ff2b69ac-0fa3-43a1-9249-b81d1b66fa34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69264 | TestLoss: 0.69305 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 01 |  TrainLoss: 0.69237 | TestLoss: 0.69214 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 02 |  TrainLoss: 0.69181 | TestLoss: 0.69109 | TestAcc: 0.50000 | TestF1: 0.00\n",
            "Epoch: 03 |  TrainLoss: 0.69049 | TestLoss: 0.69028 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 04 |  TrainLoss: 0.68904 | TestLoss: 0.68799 | TestAcc: 0.50000 | TestF1: 0.00\n",
            "Epoch: 05 |  TrainLoss: 0.68727 | TestLoss: 0.68462 | TestAcc: 0.59279 | TestF1: 0.32\n",
            "Epoch: 06 |  TrainLoss: 0.68481 | TestLoss: 0.67971 | TestAcc: 0.75039 | TestF1: 0.68\n",
            "Epoch: 07 |  TrainLoss: 0.68064 | TestLoss: 0.67321 | TestAcc: 0.69524 | TestF1: 0.57\n",
            "Epoch: 08 |  TrainLoss: 0.67185 | TestLoss: 0.66235 | TestAcc: 0.83116 | TestF1: 0.82\n",
            "Epoch: 09 |  TrainLoss: 0.66185 | TestLoss: 0.64721 | TestAcc: 0.84658 | TestF1: 0.86\n",
            "Epoch: 10 |  TrainLoss: 0.64587 | TestLoss: 0.62448 | TestAcc: 0.84344 | TestF1: 0.86\n",
            "Epoch: 11 |  TrainLoss: 0.62661 | TestLoss: 0.59439 | TestAcc: 0.84449 | TestF1: 0.86\n",
            "Epoch: 12 |  TrainLoss: 0.59294 | TestLoss: 0.55793 | TestAcc: 0.77313 | TestF1: 0.71\n",
            "Epoch: 13 |  TrainLoss: 0.55309 | TestLoss: 0.52926 | TestAcc: 0.77993 | TestF1: 0.82\n",
            "Epoch: 14 |  TrainLoss: 0.52836 | TestLoss: 0.48188 | TestAcc: 0.77836 | TestF1: 0.72\n",
            "Epoch: 15 |  TrainLoss: 0.50571 | TestLoss: 0.42197 | TestAcc: 0.84344 | TestF1: 0.83\n",
            "Epoch: 16 |  TrainLoss: 0.44362 | TestLoss: 0.39026 | TestAcc: 0.85389 | TestF1: 0.85\n",
            "Epoch: 17 |  TrainLoss: 0.41125 | TestLoss: 0.37255 | TestAcc: 0.83534 | TestF1: 0.82\n",
            "Epoch: 18 |  TrainLoss: 0.39117 | TestLoss: 0.33942 | TestAcc: 0.86304 | TestF1: 0.86\n",
            "Epoch: 19 |  TrainLoss: 0.34876 | TestLoss: 0.33729 | TestAcc: 0.84161 | TestF1: 0.82\n",
            "Epoch: 20 |  TrainLoss: 0.33490 | TestLoss: 0.30664 | TestAcc: 0.87271 | TestF1: 0.88\n",
            "Epoch: 21 |  TrainLoss: 0.30946 | TestLoss: 0.29156 | TestAcc: 0.87350 | TestF1: 0.87\n",
            "Epoch: 22 |  TrainLoss: 0.29651 | TestLoss: 0.27853 | TestAcc: 0.88291 | TestF1: 0.88\n",
            "Epoch: 23 |  TrainLoss: 0.29833 | TestLoss: 0.30831 | TestAcc: 0.87376 | TestF1: 0.88\n",
            "Epoch: 24 |  TrainLoss: 0.32910 | TestLoss: 0.31488 | TestAcc: 0.84527 | TestF1: 0.82\n",
            "Epoch: 25 |  TrainLoss: 0.29487 | TestLoss: 0.28439 | TestAcc: 0.86200 | TestF1: 0.85\n",
            "Epoch: 26 |  TrainLoss: 0.26936 | TestLoss: 0.27457 | TestAcc: 0.87271 | TestF1: 0.86\n",
            "Epoch: 27 |  TrainLoss: 0.26242 | TestLoss: 0.24533 | TestAcc: 0.90617 | TestF1: 0.91\n",
            "Epoch: 28 |  TrainLoss: 0.28840 | TestLoss: 0.24125 | TestAcc: 0.90120 | TestF1: 0.90\n",
            "Epoch: 29 |  TrainLoss: 0.24870 | TestLoss: 0.23527 | TestAcc: 0.91401 | TestF1: 0.92\n",
            "Epoch: 30 |  TrainLoss: 0.26601 | TestLoss: 0.22821 | TestAcc: 0.91375 | TestF1: 0.91\n",
            "Epoch: 31 |  TrainLoss: 0.25170 | TestLoss: 0.22435 | TestAcc: 0.91610 | TestF1: 0.92\n",
            "Epoch: 32 |  TrainLoss: 0.21878 | TestLoss: 0.22509 | TestAcc: 0.91061 | TestF1: 0.91\n",
            "Epoch: 33 |  TrainLoss: 0.20887 | TestLoss: 0.21722 | TestAcc: 0.91950 | TestF1: 0.92\n",
            "Epoch: 34 |  TrainLoss: 0.20777 | TestLoss: 0.21702 | TestAcc: 0.91505 | TestF1: 0.91\n",
            "Epoch: 35 |  TrainLoss: 0.19702 | TestLoss: 0.26822 | TestAcc: 0.87899 | TestF1: 0.87\n",
            "Epoch: 36 |  TrainLoss: 0.21585 | TestLoss: 0.26124 | TestAcc: 0.88186 | TestF1: 0.87\n",
            "Epoch: 37 |  TrainLoss: 0.25370 | TestLoss: 0.22433 | TestAcc: 0.90721 | TestF1: 0.90\n",
            "Epoch: 38 |  TrainLoss: 0.21255 | TestLoss: 0.23574 | TestAcc: 0.89728 | TestF1: 0.89\n",
            "Epoch: 39 |  TrainLoss: 0.22211 | TestLoss: 0.21150 | TestAcc: 0.91532 | TestF1: 0.91\n",
            "Epoch: 40 |  TrainLoss: 0.18832 | TestLoss: 0.20054 | TestAcc: 0.92525 | TestF1: 0.92\n",
            "Epoch: 41 |  TrainLoss: 0.18479 | TestLoss: 0.20163 | TestAcc: 0.92185 | TestF1: 0.92\n",
            "Epoch: 42 |  TrainLoss: 0.18368 | TestLoss: 0.18805 | TestAcc: 0.93257 | TestF1: 0.93\n",
            "Epoch: 43 |  TrainLoss: 0.16530 | TestLoss: 0.19800 | TestAcc: 0.93100 | TestF1: 0.93\n",
            "Epoch: 44 |  TrainLoss: 0.16355 | TestLoss: 0.18278 | TestAcc: 0.93466 | TestF1: 0.94\n",
            "Epoch: 45 |  TrainLoss: 0.15507 | TestLoss: 0.17175 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 46 |  TrainLoss: 0.15033 | TestLoss: 0.17160 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 47 |  TrainLoss: 0.17003 | TestLoss: 0.17394 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 48 |  TrainLoss: 0.16025 | TestLoss: 0.31577 | TestAcc: 0.86853 | TestF1: 0.85\n",
            "Epoch: 49 |  TrainLoss: 0.21228 | TestLoss: 0.18142 | TestAcc: 0.93335 | TestF1: 0.93\n",
            "Epoch: 50 |  TrainLoss: 0.16504 | TestLoss: 0.22841 | TestAcc: 0.90748 | TestF1: 0.90\n",
            "Epoch: 51 |  TrainLoss: 0.17162 | TestLoss: 0.17469 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 52 |  TrainLoss: 0.15772 | TestLoss: 0.19292 | TestAcc: 0.92838 | TestF1: 0.92\n",
            "Epoch: 53 |  TrainLoss: 0.16610 | TestLoss: 0.16279 | TestAcc: 0.94224 | TestF1: 0.94\n",
            "Epoch: 54 |  TrainLoss: 0.13967 | TestLoss: 0.16281 | TestAcc: 0.94407 | TestF1: 0.95\n",
            "Epoch: 55 |  TrainLoss: 0.14407 | TestLoss: 0.18368 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 56 |  TrainLoss: 0.14547 | TestLoss: 0.15418 | TestAcc: 0.94694 | TestF1: 0.95\n",
            "Epoch: 57 |  TrainLoss: 0.14566 | TestLoss: 0.15237 | TestAcc: 0.94825 | TestF1: 0.95\n",
            "Epoch: 58 |  TrainLoss: 0.13115 | TestLoss: 0.20870 | TestAcc: 0.92159 | TestF1: 0.92\n",
            "Epoch: 59 |  TrainLoss: 0.15048 | TestLoss: 0.14529 | TestAcc: 0.95269 | TestF1: 0.95\n",
            "Epoch: 60 |  TrainLoss: 0.12824 | TestLoss: 0.21773 | TestAcc: 0.92551 | TestF1: 0.93\n",
            "Epoch: 61 |  TrainLoss: 0.16617 | TestLoss: 0.32336 | TestAcc: 0.87951 | TestF1: 0.89\n",
            "Epoch: 62 |  TrainLoss: 0.19245 | TestLoss: 0.16853 | TestAcc: 0.94145 | TestF1: 0.94\n",
            "Epoch: 63 |  TrainLoss: 0.21483 | TestLoss: 0.20024 | TestAcc: 0.92499 | TestF1: 0.92\n",
            "Epoch: 64 |  TrainLoss: 0.15062 | TestLoss: 0.16552 | TestAcc: 0.94433 | TestF1: 0.94\n",
            "Epoch: 65 |  TrainLoss: 0.14750 | TestLoss: 0.13862 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 66 |  TrainLoss: 0.13015 | TestLoss: 0.18084 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 67 |  TrainLoss: 0.15106 | TestLoss: 0.13932 | TestAcc: 0.95531 | TestF1: 0.96\n",
            "Epoch: 68 |  TrainLoss: 0.11494 | TestLoss: 0.15115 | TestAcc: 0.94825 | TestF1: 0.95\n",
            "Epoch: 69 |  TrainLoss: 0.12858 | TestLoss: 0.13589 | TestAcc: 0.95583 | TestF1: 0.96\n",
            "Epoch: 70 |  TrainLoss: 0.12836 | TestLoss: 0.13347 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 71 |  TrainLoss: 0.13002 | TestLoss: 0.13268 | TestAcc: 0.96079 | TestF1: 0.96\n",
            "Epoch: 72 |  TrainLoss: 0.12575 | TestLoss: 0.13048 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 73 |  TrainLoss: 0.11490 | TestLoss: 0.16054 | TestAcc: 0.94668 | TestF1: 0.95\n",
            "Epoch: 74 |  TrainLoss: 0.12494 | TestLoss: 0.12982 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 75 |  TrainLoss: 0.11440 | TestLoss: 0.14843 | TestAcc: 0.94982 | TestF1: 0.95\n",
            "Epoch: 76 |  TrainLoss: 0.12357 | TestLoss: 0.12735 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 77 |  TrainLoss: 0.11186 | TestLoss: 0.15283 | TestAcc: 0.94877 | TestF1: 0.95\n",
            "Epoch: 78 |  TrainLoss: 0.12059 | TestLoss: 0.12692 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 79 |  TrainLoss: 0.10688 | TestLoss: 0.17382 | TestAcc: 0.93727 | TestF1: 0.93\n",
            "Epoch: 80 |  TrainLoss: 0.13891 | TestLoss: 0.12114 | TestAcc: 0.96315 | TestF1: 0.96\n",
            "Epoch: 81 |  TrainLoss: 0.13003 | TestLoss: 0.13634 | TestAcc: 0.95504 | TestF1: 0.96\n",
            "Epoch: 82 |  TrainLoss: 0.11106 | TestLoss: 0.12837 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 83 |  TrainLoss: 0.11358 | TestLoss: 0.12617 | TestAcc: 0.96132 | TestF1: 0.96\n",
            "Epoch: 84 |  TrainLoss: 0.10626 | TestLoss: 0.12295 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 85 |  TrainLoss: 0.09814 | TestLoss: 0.13152 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 86 |  TrainLoss: 0.10936 | TestLoss: 0.19892 | TestAcc: 0.92420 | TestF1: 0.92\n",
            "Epoch: 87 |  TrainLoss: 0.16875 | TestLoss: 0.19241 | TestAcc: 0.92995 | TestF1: 0.93\n",
            "Epoch: 88 |  TrainLoss: 0.16313 | TestLoss: 0.14927 | TestAcc: 0.94694 | TestF1: 0.95\n",
            "Epoch: 89 |  TrainLoss: 0.12954 | TestLoss: 0.17095 | TestAcc: 0.93701 | TestF1: 0.93\n",
            "Epoch: 90 |  TrainLoss: 0.12773 | TestLoss: 0.16353 | TestAcc: 0.94171 | TestF1: 0.94\n",
            "Epoch: 91 |  TrainLoss: 0.13645 | TestLoss: 0.11629 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 92 |  TrainLoss: 0.11304 | TestLoss: 0.13124 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 93 |  TrainLoss: 0.11991 | TestLoss: 0.11340 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 94 |  TrainLoss: 0.09743 | TestLoss: 0.11724 | TestAcc: 0.96472 | TestF1: 0.96\n",
            "Epoch: 95 |  TrainLoss: 0.10392 | TestLoss: 0.11930 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 96 |  TrainLoss: 0.09864 | TestLoss: 0.12916 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 97 |  TrainLoss: 0.10549 | TestLoss: 0.11921 | TestAcc: 0.96419 | TestF1: 0.96\n",
            "Epoch: 98 |  TrainLoss: 0.12375 | TestLoss: 0.21085 | TestAcc: 0.92237 | TestF1: 0.92\n",
            "Epoch: 99 |  TrainLoss: 0.13278 | TestLoss: 0.13874 | TestAcc: 0.95374 | TestF1: 0.95\n",
            "Epoch: 100 |  TrainLoss: 0.14485 | TestLoss: 0.26626 | TestAcc: 0.90957 | TestF1: 0.92\n",
            "Epoch: 101 |  TrainLoss: 0.17881 | TestLoss: 0.11899 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 102 |  TrainLoss: 0.12866 | TestLoss: 0.15814 | TestAcc: 0.94093 | TestF1: 0.94\n",
            "Epoch: 103 |  TrainLoss: 0.11398 | TestLoss: 0.11012 | TestAcc: 0.96811 | TestF1: 0.97\n",
            "Epoch: 104 |  TrainLoss: 0.10857 | TestLoss: 0.13002 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 105 |  TrainLoss: 0.09833 | TestLoss: 0.10624 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 106 |  TrainLoss: 0.08967 | TestLoss: 0.11101 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 107 |  TrainLoss: 0.08913 | TestLoss: 0.10749 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 108 |  TrainLoss: 0.08340 | TestLoss: 0.10964 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 109 |  TrainLoss: 0.08629 | TestLoss: 0.10726 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 110 |  TrainLoss: 0.08730 | TestLoss: 0.13073 | TestAcc: 0.95661 | TestF1: 0.96\n",
            "Epoch: 111 |  TrainLoss: 0.12381 | TestLoss: 0.12069 | TestAcc: 0.96236 | TestF1: 0.96\n",
            "Epoch: 112 |  TrainLoss: 0.11835 | TestLoss: 0.11049 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 113 |  TrainLoss: 0.10442 | TestLoss: 0.13763 | TestAcc: 0.95348 | TestF1: 0.95\n",
            "Epoch: 114 |  TrainLoss: 0.11341 | TestLoss: 0.14145 | TestAcc: 0.95165 | TestF1: 0.95\n",
            "Epoch: 115 |  TrainLoss: 0.13226 | TestLoss: 0.21183 | TestAcc: 0.92629 | TestF1: 0.93\n",
            "Epoch: 116 |  TrainLoss: 0.15217 | TestLoss: 0.10804 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 117 |  TrainLoss: 0.11279 | TestLoss: 0.16115 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 118 |  TrainLoss: 0.12666 | TestLoss: 0.24698 | TestAcc: 0.91976 | TestF1: 0.93\n",
            "Epoch: 119 |  TrainLoss: 0.13411 | TestLoss: 0.15275 | TestAcc: 0.94668 | TestF1: 0.94\n",
            "Epoch: 120 |  TrainLoss: 0.12677 | TestLoss: 0.10954 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 121 |  TrainLoss: 0.10070 | TestLoss: 0.16850 | TestAcc: 0.94590 | TestF1: 0.95\n",
            "Epoch: 122 |  TrainLoss: 0.11027 | TestLoss: 0.14182 | TestAcc: 0.95086 | TestF1: 0.95\n",
            "Epoch: 123 |  TrainLoss: 0.09109 | TestLoss: 0.12524 | TestAcc: 0.96079 | TestF1: 0.96\n",
            "Epoch: 124 |  TrainLoss: 0.09091 | TestLoss: 0.10088 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 125 |  TrainLoss: 0.08554 | TestLoss: 0.10770 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 126 |  TrainLoss: 0.08830 | TestLoss: 0.10433 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 127 |  TrainLoss: 0.07955 | TestLoss: 0.10607 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 128 |  TrainLoss: 0.08658 | TestLoss: 0.11449 | TestAcc: 0.96524 | TestF1: 0.97\n",
            "Epoch: 129 |  TrainLoss: 0.08518 | TestLoss: 0.10118 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 130 |  TrainLoss: 0.08003 | TestLoss: 0.10272 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 131 |  TrainLoss: 0.08231 | TestLoss: 0.10404 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 132 |  TrainLoss: 0.07577 | TestLoss: 0.11596 | TestAcc: 0.96393 | TestF1: 0.96\n",
            "Epoch: 133 |  TrainLoss: 0.07284 | TestLoss: 0.09957 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 134 |  TrainLoss: 0.06990 | TestLoss: 0.09927 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 135 |  TrainLoss: 0.07227 | TestLoss: 0.10259 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 136 |  TrainLoss: 0.07086 | TestLoss: 0.10549 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 137 |  TrainLoss: 0.07348 | TestLoss: 0.10336 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 138 |  TrainLoss: 0.06603 | TestLoss: 0.09895 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 139 |  TrainLoss: 0.06523 | TestLoss: 0.10174 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 140 |  TrainLoss: 0.07985 | TestLoss: 0.11833 | TestAcc: 0.96524 | TestF1: 0.96\n",
            "Epoch: 141 |  TrainLoss: 0.07920 | TestLoss: 0.10537 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 142 |  TrainLoss: 0.07574 | TestLoss: 0.12558 | TestAcc: 0.96106 | TestF1: 0.96\n",
            "Epoch: 143 |  TrainLoss: 0.07817 | TestLoss: 0.10537 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 144 |  TrainLoss: 0.07410 | TestLoss: 0.09947 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 145 |  TrainLoss: 0.07633 | TestLoss: 0.09896 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 146 |  TrainLoss: 0.07844 | TestLoss: 0.19941 | TestAcc: 0.92891 | TestF1: 0.92\n",
            "Epoch: 147 |  TrainLoss: 0.16428 | TestLoss: 0.30612 | TestAcc: 0.89571 | TestF1: 0.89\n",
            "Epoch: 148 |  TrainLoss: 0.17919 | TestLoss: 0.09752 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 149 |  TrainLoss: 0.13883 | TestLoss: 0.15641 | TestAcc: 0.94537 | TestF1: 0.95\n",
            "Epoch: 150 |  TrainLoss: 0.11578 | TestLoss: 0.14951 | TestAcc: 0.94773 | TestF1: 0.95\n",
            "Epoch: 151 |  TrainLoss: 0.10274 | TestLoss: 0.11325 | TestAcc: 0.96524 | TestF1: 0.97\n",
            "Epoch: 152 |  TrainLoss: 0.10226 | TestLoss: 0.09484 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 153 |  TrainLoss: 0.08288 | TestLoss: 0.11504 | TestAcc: 0.96524 | TestF1: 0.96\n",
            "Epoch: 154 |  TrainLoss: 0.08424 | TestLoss: 0.14161 | TestAcc: 0.95635 | TestF1: 0.96\n",
            "Epoch: 155 |  TrainLoss: 0.07405 | TestLoss: 0.10844 | TestAcc: 0.96681 | TestF1: 0.97\n",
            "Epoch: 156 |  TrainLoss: 0.07744 | TestLoss: 0.09610 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 157 |  TrainLoss: 0.09787 | TestLoss: 0.11401 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 158 |  TrainLoss: 0.08419 | TestLoss: 0.09860 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 159 |  TrainLoss: 0.06805 | TestLoss: 0.09701 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 160 |  TrainLoss: 0.06531 | TestLoss: 0.09776 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 161 |  TrainLoss: 0.06830 | TestLoss: 0.11174 | TestAcc: 0.96681 | TestF1: 0.97\n",
            "Epoch: 162 |  TrainLoss: 0.08562 | TestLoss: 0.10908 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 163 |  TrainLoss: 0.07356 | TestLoss: 0.09152 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 164 |  TrainLoss: 0.05660 | TestLoss: 0.10651 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 165 |  TrainLoss: 0.06426 | TestLoss: 0.10372 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 166 |  TrainLoss: 0.06321 | TestLoss: 0.09587 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 167 |  TrainLoss: 0.05757 | TestLoss: 0.11146 | TestAcc: 0.96811 | TestF1: 0.97\n",
            "Epoch: 168 |  TrainLoss: 0.06063 | TestLoss: 0.10327 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 169 |  TrainLoss: 0.06350 | TestLoss: 0.10484 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 170 |  TrainLoss: 0.05981 | TestLoss: 0.10174 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 171 |  TrainLoss: 0.05831 | TestLoss: 0.19800 | TestAcc: 0.93988 | TestF1: 0.94\n",
            "Epoch: 172 |  TrainLoss: 0.09816 | TestLoss: 0.09824 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 173 |  TrainLoss: 0.09670 | TestLoss: 0.17592 | TestAcc: 0.94093 | TestF1: 0.94\n",
            "Epoch: 174 |  TrainLoss: 0.09884 | TestLoss: 0.10235 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 175 |  TrainLoss: 0.09079 | TestLoss: 0.14914 | TestAcc: 0.95374 | TestF1: 0.96\n",
            "Epoch: 176 |  TrainLoss: 0.09390 | TestLoss: 0.11087 | TestAcc: 0.96811 | TestF1: 0.97\n",
            "Epoch: 177 |  TrainLoss: 0.06676 | TestLoss: 0.09997 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 178 |  TrainLoss: 0.05929 | TestLoss: 0.10941 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 179 |  TrainLoss: 0.06463 | TestLoss: 0.09515 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 180 |  TrainLoss: 0.06286 | TestLoss: 0.09066 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 181 |  TrainLoss: 0.06142 | TestLoss: 0.10843 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 182 |  TrainLoss: 0.06425 | TestLoss: 0.13361 | TestAcc: 0.96132 | TestF1: 0.96\n",
            "Epoch: 183 |  TrainLoss: 0.06301 | TestLoss: 0.08909 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 184 |  TrainLoss: 0.06328 | TestLoss: 0.11338 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 185 |  TrainLoss: 0.06900 | TestLoss: 0.10081 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 186 |  TrainLoss: 0.05338 | TestLoss: 0.09907 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 187 |  TrainLoss: 0.06062 | TestLoss: 0.09349 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 188 |  TrainLoss: 0.05379 | TestLoss: 0.11628 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 189 |  TrainLoss: 0.05358 | TestLoss: 0.10276 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 190 |  TrainLoss: 0.05529 | TestLoss: 0.11721 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 191 |  TrainLoss: 0.05224 | TestLoss: 0.09358 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 192 |  TrainLoss: 0.05076 | TestLoss: 0.09841 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 193 |  TrainLoss: 0.04952 | TestLoss: 0.10901 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 194 |  TrainLoss: 0.05056 | TestLoss: 0.09887 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 195 |  TrainLoss: 0.05808 | TestLoss: 0.13766 | TestAcc: 0.95740 | TestF1: 0.96\n",
            "Epoch: 196 |  TrainLoss: 0.09877 | TestLoss: 0.34859 | TestAcc: 0.88447 | TestF1: 0.90\n",
            "Epoch: 197 |  TrainLoss: 0.26745 | TestLoss: 0.49499 | TestAcc: 0.77339 | TestF1: 0.81\n",
            "Epoch: 198 |  TrainLoss: 0.23379 | TestLoss: 0.10787 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 199 |  TrainLoss: 0.09853 | TestLoss: 0.12253 | TestAcc: 0.96158 | TestF1: 0.96\n",
            "Epoch: 200 |  TrainLoss: 0.09104 | TestLoss: 0.14545 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 201 |  TrainLoss: 0.07751 | TestLoss: 0.09202 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 202 |  TrainLoss: 0.06803 | TestLoss: 0.09228 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 203 |  TrainLoss: 0.06864 | TestLoss: 0.13226 | TestAcc: 0.96158 | TestF1: 0.96\n",
            "Epoch: 204 |  TrainLoss: 0.07505 | TestLoss: 0.09789 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 205 |  TrainLoss: 0.06325 | TestLoss: 0.09085 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 206 |  TrainLoss: 0.07283 | TestLoss: 0.12946 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 207 |  TrainLoss: 0.07292 | TestLoss: 0.11488 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 208 |  TrainLoss: 0.09233 | TestLoss: 0.11705 | TestAcc: 0.96472 | TestF1: 0.96\n",
            "Epoch: 209 |  TrainLoss: 0.08580 | TestLoss: 0.09855 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 210 |  TrainLoss: 0.08359 | TestLoss: 0.14178 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 211 |  TrainLoss: 0.08168 | TestLoss: 0.09484 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 212 |  TrainLoss: 0.06780 | TestLoss: 0.09877 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 213 |  TrainLoss: 0.06965 | TestLoss: 0.12857 | TestAcc: 0.96602 | TestF1: 0.97\n",
            "Epoch: 214 |  TrainLoss: 0.06584 | TestLoss: 0.09353 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 215 |  TrainLoss: 0.06103 | TestLoss: 0.08834 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 216 |  TrainLoss: 0.05382 | TestLoss: 0.10061 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 217 |  TrainLoss: 0.05274 | TestLoss: 0.09109 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 218 |  TrainLoss: 0.04999 | TestLoss: 0.08905 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 219 |  TrainLoss: 0.05192 | TestLoss: 0.09453 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 220 |  TrainLoss: 0.04786 | TestLoss: 0.10300 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 221 |  TrainLoss: 0.04709 | TestLoss: 0.09371 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 222 |  TrainLoss: 0.04736 | TestLoss: 0.09399 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 223 |  TrainLoss: 0.05470 | TestLoss: 0.09236 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 224 |  TrainLoss: 0.05713 | TestLoss: 0.09326 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 225 |  TrainLoss: 0.05700 | TestLoss: 0.12630 | TestAcc: 0.96289 | TestF1: 0.96\n",
            "Epoch: 226 |  TrainLoss: 0.06771 | TestLoss: 0.10583 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 227 |  TrainLoss: 0.04750 | TestLoss: 0.09140 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 228 |  TrainLoss: 0.04658 | TestLoss: 0.09488 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 229 |  TrainLoss: 0.04058 | TestLoss: 0.10044 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 230 |  TrainLoss: 0.04858 | TestLoss: 0.09835 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 231 |  TrainLoss: 0.04575 | TestLoss: 0.10999 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 232 |  TrainLoss: 0.04604 | TestLoss: 0.09828 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 233 |  TrainLoss: 0.04870 | TestLoss: 0.09637 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 234 |  TrainLoss: 0.05218 | TestLoss: 0.10579 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 235 |  TrainLoss: 0.04648 | TestLoss: 0.09785 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 236 |  TrainLoss: 0.04269 | TestLoss: 0.09529 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 237 |  TrainLoss: 0.04249 | TestLoss: 0.09839 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 238 |  TrainLoss: 0.04120 | TestLoss: 0.09472 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 239 |  TrainLoss: 0.04350 | TestLoss: 0.09990 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 240 |  TrainLoss: 0.04152 | TestLoss: 0.10763 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 241 |  TrainLoss: 0.04920 | TestLoss: 0.33481 | TestAcc: 0.91375 | TestF1: 0.92\n",
            "Epoch: 242 |  TrainLoss: 0.32793 | TestLoss: 0.77752 | TestAcc: 0.68871 | TestF1: 0.76\n",
            "Epoch: 243 |  TrainLoss: 0.34184 | TestLoss: 0.22636 | TestAcc: 0.92473 | TestF1: 0.93\n",
            "Epoch: 244 |  TrainLoss: 0.15875 | TestLoss: 0.23872 | TestAcc: 0.91113 | TestF1: 0.90\n",
            "Epoch: 245 |  TrainLoss: 0.12624 | TestLoss: 0.14095 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 246 |  TrainLoss: 0.10458 | TestLoss: 0.11682 | TestAcc: 0.96289 | TestF1: 0.96\n",
            "Epoch: 247 |  TrainLoss: 0.08968 | TestLoss: 0.09918 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 248 |  TrainLoss: 0.07299 | TestLoss: 0.09741 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 249 |  TrainLoss: 0.06616 | TestLoss: 0.09771 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 250 |  TrainLoss: 0.06704 | TestLoss: 0.12157 | TestAcc: 0.96289 | TestF1: 0.96\n",
            "Epoch: 251 |  TrainLoss: 0.06675 | TestLoss: 0.09526 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 252 |  TrainLoss: 0.05828 | TestLoss: 0.09162 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 253 |  TrainLoss: 0.05376 | TestLoss: 0.10265 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 254 |  TrainLoss: 0.05311 | TestLoss: 0.09465 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 255 |  TrainLoss: 0.05382 | TestLoss: 0.09252 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 256 |  TrainLoss: 0.05629 | TestLoss: 0.11504 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 257 |  TrainLoss: 0.05948 | TestLoss: 0.09089 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 258 |  TrainLoss: 0.04805 | TestLoss: 0.10506 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 259 |  TrainLoss: 0.05090 | TestLoss: 0.09176 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 260 |  TrainLoss: 0.05338 | TestLoss: 0.09701 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 261 |  TrainLoss: 0.06803 | TestLoss: 0.11493 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 262 |  TrainLoss: 0.05094 | TestLoss: 0.09810 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 263 |  TrainLoss: 0.07524 | TestLoss: 0.10407 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 264 |  TrainLoss: 0.05924 | TestLoss: 0.09548 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 265 |  TrainLoss: 0.06565 | TestLoss: 0.11699 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 266 |  TrainLoss: 0.05601 | TestLoss: 0.09480 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 267 |  TrainLoss: 0.04563 | TestLoss: 0.09090 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 268 |  TrainLoss: 0.04394 | TestLoss: 0.09139 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 269 |  TrainLoss: 0.04319 | TestLoss: 0.09988 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 270 |  TrainLoss: 0.04287 | TestLoss: 0.10018 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 271 |  TrainLoss: 0.04456 | TestLoss: 0.09503 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 272 |  TrainLoss: 0.04726 | TestLoss: 0.10464 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 273 |  TrainLoss: 0.04914 | TestLoss: 0.11822 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 274 |  TrainLoss: 0.04357 | TestLoss: 0.09439 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 275 |  TrainLoss: 0.04274 | TestLoss: 0.09781 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 276 |  TrainLoss: 0.04266 | TestLoss: 0.11229 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 277 |  TrainLoss: 0.04196 | TestLoss: 0.09676 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 278 |  TrainLoss: 0.04303 | TestLoss: 0.09997 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 279 |  TrainLoss: 0.03746 | TestLoss: 0.10670 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 280 |  TrainLoss: 0.03787 | TestLoss: 0.09694 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 281 |  TrainLoss: 0.03848 | TestLoss: 0.10083 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 282 |  TrainLoss: 0.04047 | TestLoss: 0.11401 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 283 |  TrainLoss: 0.04100 | TestLoss: 0.10658 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 284 |  TrainLoss: 0.04647 | TestLoss: 0.09890 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 285 |  TrainLoss: 0.08218 | TestLoss: 0.12186 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 286 |  TrainLoss: 0.06579 | TestLoss: 0.12198 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 287 |  TrainLoss: 0.06121 | TestLoss: 0.10908 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 288 |  TrainLoss: 0.05050 | TestLoss: 0.11563 | TestAcc: 0.96550 | TestF1: 0.97\n",
            "Epoch: 289 |  TrainLoss: 0.05002 | TestLoss: 0.10665 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 290 |  TrainLoss: 0.04319 | TestLoss: 0.11337 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 291 |  TrainLoss: 0.04237 | TestLoss: 0.10579 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 292 |  TrainLoss: 0.03862 | TestLoss: 0.10758 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 293 |  TrainLoss: 0.04504 | TestLoss: 0.10181 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 294 |  TrainLoss: 0.04746 | TestLoss: 0.12797 | TestAcc: 0.96524 | TestF1: 0.96\n",
            "Epoch: 295 |  TrainLoss: 0.05585 | TestLoss: 0.13269 | TestAcc: 0.96289 | TestF1: 0.96\n",
            "Epoch: 296 |  TrainLoss: 0.05004 | TestLoss: 0.10127 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 297 |  TrainLoss: 0.04214 | TestLoss: 0.11482 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 298 |  TrainLoss: 0.03907 | TestLoss: 0.09525 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 299 |  TrainLoss: 0.03943 | TestLoss: 0.09719 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 300 |  TrainLoss: 0.04729 | TestLoss: 0.24771 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 301 |  TrainLoss: 0.09395 | TestLoss: 0.10578 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 302 |  TrainLoss: 0.05554 | TestLoss: 0.11386 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 303 |  TrainLoss: 0.05065 | TestLoss: 0.12080 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 304 |  TrainLoss: 0.04480 | TestLoss: 0.09733 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 305 |  TrainLoss: 0.04183 | TestLoss: 0.10673 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 306 |  TrainLoss: 0.03786 | TestLoss: 0.09902 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 307 |  TrainLoss: 0.03501 | TestLoss: 0.09408 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 308 |  TrainLoss: 0.03668 | TestLoss: 0.12044 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 309 |  TrainLoss: 0.03652 | TestLoss: 0.10185 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 310 |  TrainLoss: 0.03429 | TestLoss: 0.10188 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 311 |  TrainLoss: 0.03376 | TestLoss: 0.11360 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 312 |  TrainLoss: 0.03643 | TestLoss: 0.10594 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 313 |  TrainLoss: 0.04335 | TestLoss: 0.10755 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 314 |  TrainLoss: 0.03959 | TestLoss: 0.11123 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 315 |  TrainLoss: 0.03881 | TestLoss: 0.10317 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 316 |  TrainLoss: 0.03566 | TestLoss: 0.11487 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 317 |  TrainLoss: 0.03519 | TestLoss: 0.10464 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 318 |  TrainLoss: 0.03848 | TestLoss: 0.10335 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 319 |  TrainLoss: 0.04546 | TestLoss: 0.10210 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 320 |  TrainLoss: 0.04177 | TestLoss: 0.10758 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 321 |  TrainLoss: 0.03914 | TestLoss: 0.11304 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 322 |  TrainLoss: 0.04095 | TestLoss: 0.11084 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 323 |  TrainLoss: 0.03769 | TestLoss: 0.10375 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 324 |  TrainLoss: 0.03470 | TestLoss: 0.11087 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 325 |  TrainLoss: 0.03754 | TestLoss: 0.11724 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 326 |  TrainLoss: 0.03539 | TestLoss: 0.11098 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 327 |  TrainLoss: 0.03586 | TestLoss: 0.12268 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 328 |  TrainLoss: 0.03771 | TestLoss: 0.10794 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 329 |  TrainLoss: 0.03386 | TestLoss: 0.14210 | TestAcc: 0.96681 | TestF1: 0.97\n",
            "Epoch: 330 |  TrainLoss: 0.03349 | TestLoss: 0.11083 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 331 |  TrainLoss: 0.03915 | TestLoss: 0.10736 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 332 |  TrainLoss: 0.03298 | TestLoss: 0.11677 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 333 |  TrainLoss: 0.03280 | TestLoss: 0.13168 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 334 |  TrainLoss: 0.04017 | TestLoss: 0.11190 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 335 |  TrainLoss: 0.03153 | TestLoss: 0.11065 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 336 |  TrainLoss: 0.03127 | TestLoss: 0.10864 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 337 |  TrainLoss: 0.03174 | TestLoss: 0.13736 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 338 |  TrainLoss: 0.03093 | TestLoss: 0.12746 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 339 |  TrainLoss: 0.03687 | TestLoss: 0.10364 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 340 |  TrainLoss: 0.03748 | TestLoss: 0.14170 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 341 |  TrainLoss: 0.03136 | TestLoss: 0.12459 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 342 |  TrainLoss: 0.03061 | TestLoss: 0.16204 | TestAcc: 0.96079 | TestF1: 0.96\n",
            "Epoch: 343 |  TrainLoss: 0.03366 | TestLoss: 0.10685 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 344 |  TrainLoss: 0.03063 | TestLoss: 0.12306 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 345 |  TrainLoss: 0.02815 | TestLoss: 0.12181 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 346 |  TrainLoss: 0.02999 | TestLoss: 0.12329 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 347 |  TrainLoss: 0.02894 | TestLoss: 0.10671 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 348 |  TrainLoss: 0.03446 | TestLoss: 0.11986 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 349 |  TrainLoss: 0.07446 | TestLoss: 0.12952 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 350 |  TrainLoss: 0.05202 | TestLoss: 0.12813 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 351 |  TrainLoss: 0.03867 | TestLoss: 0.11642 | TestAcc: 0.96419 | TestF1: 0.96\n",
            "Epoch: 352 |  TrainLoss: 0.03970 | TestLoss: 0.12115 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 353 |  TrainLoss: 0.03714 | TestLoss: 0.12198 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 354 |  TrainLoss: 0.03011 | TestLoss: 0.10616 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 355 |  TrainLoss: 0.03134 | TestLoss: 0.13739 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 356 |  TrainLoss: 0.02814 | TestLoss: 0.11411 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 357 |  TrainLoss: 0.02734 | TestLoss: 0.13415 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 358 |  TrainLoss: 0.03023 | TestLoss: 0.11517 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 359 |  TrainLoss: 0.02754 | TestLoss: 0.11397 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 360 |  TrainLoss: 0.03030 | TestLoss: 0.13280 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 361 |  TrainLoss: 0.02684 | TestLoss: 0.11473 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 362 |  TrainLoss: 0.02823 | TestLoss: 0.11372 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 363 |  TrainLoss: 0.02796 | TestLoss: 0.14208 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 364 |  TrainLoss: 0.02683 | TestLoss: 0.12230 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 365 |  TrainLoss: 0.02535 | TestLoss: 0.19815 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 366 |  TrainLoss: 0.03546 | TestLoss: 0.11754 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 367 |  TrainLoss: 0.02978 | TestLoss: 0.11831 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 368 |  TrainLoss: 0.02945 | TestLoss: 0.11412 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 369 |  TrainLoss: 0.03352 | TestLoss: 0.30547 | TestAcc: 0.93466 | TestF1: 0.94\n",
            "Epoch: 370 |  TrainLoss: 0.09705 | TestLoss: 0.14334 | TestAcc: 0.96602 | TestF1: 0.97\n",
            "Epoch: 371 |  TrainLoss: 0.04771 | TestLoss: 0.14175 | TestAcc: 0.96419 | TestF1: 0.96\n",
            "Epoch: 372 |  TrainLoss: 0.04692 | TestLoss: 0.10206 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 373 |  TrainLoss: 0.04136 | TestLoss: 0.18652 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 374 |  TrainLoss: 0.06777 | TestLoss: 0.12921 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 375 |  TrainLoss: 0.03738 | TestLoss: 0.10415 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 376 |  TrainLoss: 0.03637 | TestLoss: 0.11834 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 377 |  TrainLoss: 0.04189 | TestLoss: 0.14809 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 378 |  TrainLoss: 0.06434 | TestLoss: 0.12431 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 379 |  TrainLoss: 0.04540 | TestLoss: 0.13798 | TestAcc: 0.96498 | TestF1: 0.96\n",
            "Epoch: 380 |  TrainLoss: 0.08392 | TestLoss: 0.26628 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 381 |  TrainLoss: 0.10344 | TestLoss: 0.11356 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 382 |  TrainLoss: 0.12959 | TestLoss: 0.15351 | TestAcc: 0.95243 | TestF1: 0.95\n",
            "Epoch: 383 |  TrainLoss: 0.14459 | TestLoss: 0.35312 | TestAcc: 0.86801 | TestF1: 0.85\n",
            "Epoch: 384 |  TrainLoss: 0.21814 | TestLoss: 0.18660 | TestAcc: 0.93074 | TestF1: 0.93\n",
            "Epoch: 385 |  TrainLoss: 0.15179 | TestLoss: 0.15126 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 386 |  TrainLoss: 0.08691 | TestLoss: 0.11626 | TestAcc: 0.96289 | TestF1: 0.96\n",
            "Epoch: 387 |  TrainLoss: 0.06711 | TestLoss: 0.15550 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 388 |  TrainLoss: 0.05886 | TestLoss: 0.09853 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 389 |  TrainLoss: 0.04496 | TestLoss: 0.09790 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 390 |  TrainLoss: 0.05691 | TestLoss: 0.11835 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 391 |  TrainLoss: 0.03709 | TestLoss: 0.10448 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 392 |  TrainLoss: 0.04241 | TestLoss: 0.10599 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 393 |  TrainLoss: 0.05238 | TestLoss: 0.12666 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 394 |  TrainLoss: 0.03207 | TestLoss: 0.10165 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 395 |  TrainLoss: 0.04120 | TestLoss: 0.10271 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 396 |  TrainLoss: 0.04390 | TestLoss: 0.11016 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 397 |  TrainLoss: 0.03211 | TestLoss: 0.10981 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 398 |  TrainLoss: 0.02950 | TestLoss: 0.10144 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 399 |  TrainLoss: 0.03125 | TestLoss: 0.12330 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 400 |  TrainLoss: 0.02804 | TestLoss: 0.11657 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 401 |  TrainLoss: 0.03281 | TestLoss: 0.15764 | TestAcc: 0.96602 | TestF1: 0.97\n",
            "Epoch: 402 |  TrainLoss: 0.03356 | TestLoss: 0.12416 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 403 |  TrainLoss: 0.05029 | TestLoss: 0.12367 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 404 |  TrainLoss: 0.03574 | TestLoss: 0.11275 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 405 |  TrainLoss: 0.03909 | TestLoss: 0.12262 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 406 |  TrainLoss: 0.04266 | TestLoss: 0.12458 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 407 |  TrainLoss: 0.03971 | TestLoss: 0.11827 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 408 |  TrainLoss: 0.03625 | TestLoss: 0.14660 | TestAcc: 0.96132 | TestF1: 0.96\n",
            "Epoch: 409 |  TrainLoss: 0.05594 | TestLoss: 0.12999 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 410 |  TrainLoss: 0.03176 | TestLoss: 0.11233 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 411 |  TrainLoss: 0.02680 | TestLoss: 0.14532 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 412 |  TrainLoss: 0.02865 | TestLoss: 0.11173 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 413 |  TrainLoss: 0.02502 | TestLoss: 0.11810 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 414 |  TrainLoss: 0.02813 | TestLoss: 0.11744 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 415 |  TrainLoss: 0.02399 | TestLoss: 0.13574 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 416 |  TrainLoss: 0.02401 | TestLoss: 0.11672 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 417 |  TrainLoss: 0.03023 | TestLoss: 0.11207 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 418 |  TrainLoss: 0.02969 | TestLoss: 0.19621 | TestAcc: 0.96236 | TestF1: 0.96\n",
            "Epoch: 419 |  TrainLoss: 0.03307 | TestLoss: 0.12673 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 420 |  TrainLoss: 0.02975 | TestLoss: 0.12535 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 421 |  TrainLoss: 0.02787 | TestLoss: 0.11840 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 422 |  TrainLoss: 0.02622 | TestLoss: 0.12468 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 423 |  TrainLoss: 0.02391 | TestLoss: 0.11424 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 424 |  TrainLoss: 0.02819 | TestLoss: 0.16066 | TestAcc: 0.96628 | TestF1: 0.97\n",
            "Epoch: 425 |  TrainLoss: 0.02675 | TestLoss: 0.11608 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 426 |  TrainLoss: 0.02347 | TestLoss: 0.20744 | TestAcc: 0.96184 | TestF1: 0.96\n",
            "Epoch: 427 |  TrainLoss: 0.04703 | TestLoss: 0.12831 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 428 |  TrainLoss: 0.03490 | TestLoss: 0.14153 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 429 |  TrainLoss: 0.04786 | TestLoss: 0.11879 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 430 |  TrainLoss: 0.10354 | TestLoss: 0.30434 | TestAcc: 0.91715 | TestF1: 0.92\n",
            "Epoch: 431 |  TrainLoss: 0.09548 | TestLoss: 0.12268 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 432 |  TrainLoss: 0.06304 | TestLoss: 0.15363 | TestAcc: 0.95348 | TestF1: 0.95\n",
            "Epoch: 433 |  TrainLoss: 0.07410 | TestLoss: 0.33717 | TestAcc: 0.92629 | TestF1: 0.93\n",
            "Epoch: 434 |  TrainLoss: 0.08504 | TestLoss: 0.13665 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 435 |  TrainLoss: 0.08291 | TestLoss: 0.11285 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 436 |  TrainLoss: 0.07840 | TestLoss: 0.18150 | TestAcc: 0.95687 | TestF1: 0.96\n",
            "Epoch: 437 |  TrainLoss: 0.06778 | TestLoss: 0.11161 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 438 |  TrainLoss: 0.04706 | TestLoss: 0.11059 | TestAcc: 0.96759 | TestF1: 0.97\n",
            "Epoch: 439 |  TrainLoss: 0.04830 | TestLoss: 0.13364 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 440 |  TrainLoss: 0.06013 | TestLoss: 0.13056 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 441 |  TrainLoss: 0.03601 | TestLoss: 0.10666 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 442 |  TrainLoss: 0.04192 | TestLoss: 0.10630 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 443 |  TrainLoss: 0.03591 | TestLoss: 0.13524 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 444 |  TrainLoss: 0.03415 | TestLoss: 0.14207 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 445 |  TrainLoss: 0.03017 | TestLoss: 0.10279 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 446 |  TrainLoss: 0.02935 | TestLoss: 0.14015 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 447 |  TrainLoss: 0.02751 | TestLoss: 0.12633 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 448 |  TrainLoss: 0.02899 | TestLoss: 0.12418 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 449 |  TrainLoss: 0.02665 | TestLoss: 0.12705 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 450 |  TrainLoss: 0.02541 | TestLoss: 0.13856 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 451 |  TrainLoss: 0.02618 | TestLoss: 0.12163 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 452 |  TrainLoss: 0.02133 | TestLoss: 0.12308 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 453 |  TrainLoss: 0.02634 | TestLoss: 0.12447 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 454 |  TrainLoss: 0.02153 | TestLoss: 0.14857 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 455 |  TrainLoss: 0.02440 | TestLoss: 0.12088 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 456 |  TrainLoss: 0.02261 | TestLoss: 0.13764 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 457 |  TrainLoss: 0.02356 | TestLoss: 0.12078 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 458 |  TrainLoss: 0.02347 | TestLoss: 0.14081 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 459 |  TrainLoss: 0.02055 | TestLoss: 0.11881 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 460 |  TrainLoss: 0.02673 | TestLoss: 0.12645 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 461 |  TrainLoss: 0.02166 | TestLoss: 0.13907 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 462 |  TrainLoss: 0.02177 | TestLoss: 0.13304 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 463 |  TrainLoss: 0.02111 | TestLoss: 0.15072 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 464 |  TrainLoss: 0.02365 | TestLoss: 0.12633 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 465 |  TrainLoss: 0.02281 | TestLoss: 0.13979 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 466 |  TrainLoss: 0.02107 | TestLoss: 0.12937 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 467 |  TrainLoss: 0.02129 | TestLoss: 0.13832 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 468 |  TrainLoss: 0.01969 | TestLoss: 0.13912 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 469 |  TrainLoss: 0.02235 | TestLoss: 0.14483 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 470 |  TrainLoss: 0.02242 | TestLoss: 0.13676 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 471 |  TrainLoss: 0.02677 | TestLoss: 0.12203 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 472 |  TrainLoss: 0.02229 | TestLoss: 0.13809 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 473 |  TrainLoss: 0.02396 | TestLoss: 0.15119 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 474 |  TrainLoss: 0.02693 | TestLoss: 0.12429 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 475 |  TrainLoss: 0.02072 | TestLoss: 0.15880 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 476 |  TrainLoss: 0.02250 | TestLoss: 0.13520 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 477 |  TrainLoss: 0.02067 | TestLoss: 0.19216 | TestAcc: 0.96550 | TestF1: 0.97\n",
            "Epoch: 478 |  TrainLoss: 0.02420 | TestLoss: 0.12208 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 479 |  TrainLoss: 0.02116 | TestLoss: 0.16085 | TestAcc: 0.96811 | TestF1: 0.97\n",
            "Epoch: 480 |  TrainLoss: 0.02019 | TestLoss: 0.12715 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 481 |  TrainLoss: 0.02390 | TestLoss: 0.12515 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 482 |  TrainLoss: 0.02481 | TestLoss: 0.14992 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 483 |  TrainLoss: 0.02190 | TestLoss: 0.12631 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 484 |  TrainLoss: 0.02281 | TestLoss: 0.20804 | TestAcc: 0.96393 | TestF1: 0.96\n",
            "Epoch: 485 |  TrainLoss: 0.02828 | TestLoss: 0.13032 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 486 |  TrainLoss: 0.02812 | TestLoss: 0.13943 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 487 |  TrainLoss: 0.02637 | TestLoss: 0.16126 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 488 |  TrainLoss: 0.02418 | TestLoss: 0.12878 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 489 |  TrainLoss: 0.02574 | TestLoss: 0.17111 | TestAcc: 0.96498 | TestF1: 0.97\n",
            "Epoch: 490 |  TrainLoss: 0.02461 | TestLoss: 0.12236 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 491 |  TrainLoss: 0.02143 | TestLoss: 0.13649 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 492 |  TrainLoss: 0.02871 | TestLoss: 0.15510 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 493 |  TrainLoss: 0.02056 | TestLoss: 0.12546 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 494 |  TrainLoss: 0.02940 | TestLoss: 0.14329 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 495 |  TrainLoss: 0.07116 | TestLoss: 0.13054 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 496 |  TrainLoss: 0.04987 | TestLoss: 0.14607 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 497 |  TrainLoss: 0.04170 | TestLoss: 0.11499 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 498 |  TrainLoss: 0.02547 | TestLoss: 0.12761 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 499 |  TrainLoss: 0.02428 | TestLoss: 0.12925 | TestAcc: 0.97412 | TestF1: 0.97\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wloss = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "\n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot of Test Accuracy over best Loss and Best Accuracy Epoch"
      ],
      "metadata": {
        "id": "5cLaQTwhNy-P"
      },
      "id": "5cLaQTwhNy-P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1d332b",
      "metadata": {
        "id": "cd1d332b",
        "outputId": "f0d5eeb1-77ad-4162-81d7-549d072503f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(test_accs, label='Test Accuracy')\n",
        "\n",
        "# Add legend and axis labels\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "\n",
        "# Add marker for best epoch\n",
        "best_epoch_loss = test_losses.index(min(test_losses))\n",
        "best_epoch_acc = test_accs.index(max(test_accs))\n",
        "plt.axvline(x=best_epoch_loss, color='r', linestyle='--', label='Best Loss Epoch')\n",
        "plt.axvline(x=best_epoch_acc, color='g', linestyle='--', label='Best Acc Epoch')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}