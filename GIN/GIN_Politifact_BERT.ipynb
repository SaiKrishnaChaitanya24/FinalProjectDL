{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xO_6zuup7X0O"
   },
   "source": [
    "#GIN Using Default values: Politifact and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kqFDhtgcVbD",
    "outputId": "833426ba-468e-4a25-d84a-f47a742283d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Requirement already satisfied: torch-scatter in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
      "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
      "Requirement already satisfied: torch-sparse in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
      "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
      "Requirement already satisfied: torch-geometric in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.4.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.8.0)\n",
      "Requirement already satisfied: jinja2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.11.3)\n",
      "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.7.3)\n",
      "Requirement already satisfied: pyparsing in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.0)\n",
      "Requirement already satisfied: requests in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.27.1)\n",
      "Requirement already satisfied: numpy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7ynggVb9cXM9"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import UPFD #importing the UPFD Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "QoQkvkXWkf_n",
    "outputId": "1b5b7ef0-48ae-4146-a694-bc94ca956ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politifact Dataset\n",
      "Train Samples:  93\n",
      "Validation Samples:  31\n",
      "Test Samples:  221\n"
     ]
    }
   ],
   "source": [
    "test_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\",split=\"test\")\n",
    "train_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"train\")\n",
    "val_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"val\")\n",
    "train_data_pol = train_data_pol + val_data_pol\n",
    "\n",
    "print(\"Politifact Dataset\")\n",
    "print(\"Train Samples: \", len(train_data_pol))\n",
    "print(\"Validation Samples: \", len(val_data_pol))\n",
    "print(\"Test Samples: \", len(test_data_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmFBQDsokhDD",
    "outputId": "9bbcebd0-353e-455d-9a73-74b72745c9ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  8, 16, 16, 16, 16, 16, 16,\n",
       "         24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
       "         24, 24, 24, 24, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 60],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pol[0].edge_index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FKotqDSA8AJ3"
   },
   "source": [
    "##Loading Dataset Using DataLoader for train data and test data of Politifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V0wtMCgkj5K"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data_pol, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data_pol, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjJmDS3gkpqa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
    "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import LeakyReLU\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2UcjAWIs8MEQ"
   },
   "source": [
    "##Defining Architecture of GIN Using 3 GIN Convolutional layers  with 3 unit MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgbNBfFRkqJV"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU\n",
    "from torch_geometric.nn import GINConv, global_max_pool\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(Linear(in_channels, hidden_channels[0]), ReLU()))\n",
    "        self.conv2 = GINConv(Sequential(Linear(hidden_channels[0], hidden_channels[1]), ReLU()))\n",
    "        self.conv3 = GINConv(Sequential(Linear(hidden_channels[1], hidden_channels[2]), ReLU()))\n",
    "\n",
    "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
    "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
    "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
    "\n",
    "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
    "\n",
    "        # dropouts\n",
    "        self.dp1 = torch.nn.Dropout(0.2)\n",
    "        self.dp2 = torch.nn.Dropout(0.2)\n",
    "        self.dp3 = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = global_max_pool(h, batch)\n",
    "\n",
    "        h = self.full1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dp1(h)\n",
    "        h = self.full2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dp2(h)\n",
    "        h = self.full3(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dp3(h)\n",
    "\n",
    "        h = self.softmax(h)\n",
    "\n",
    "        return torch.sigmoid(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZCVDyk8kw0z"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, f1_score "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sM6jdLvl8VX2"
   },
   "source": [
    "##Train and Test of model using Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkehUFOukzZ3",
    "outputId": "ea887981-5dd6-4b1d-fddb-73306afc62bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #using adam optimiser and the learning rate as lr= 0.0001\n",
    "lossff = torch.nn.BCELoss() #binary cross entropy loss\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsFCnQUek2G0"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOA0fdKsk-KW",
    "outputId": "7f1ac31d-de83-450f-d393-859189a752d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69646 | TestLoss: 0.69281 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 01 |  TrainLoss: 0.69417 | TestLoss: 0.69280 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 02 |  TrainLoss: 0.69488 | TestLoss: 0.69282 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 03 |  TrainLoss: 0.69531 | TestLoss: 0.69282 | TestAcc: 0.51131 | TestF1: 0.68\n",
      "Epoch: 04 |  TrainLoss: 0.69373 | TestLoss: 0.69292 | TestAcc: 0.63801 | TestF1: 0.73\n",
      "Epoch: 05 |  TrainLoss: 0.69507 | TestLoss: 0.69305 | TestAcc: 0.49774 | TestF1: 0.07\n",
      "Epoch: 06 |  TrainLoss: 0.69079 | TestLoss: 0.69313 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.69321 | TestLoss: 0.69318 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.69570 | TestLoss: 0.69316 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.69299 | TestLoss: 0.69318 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69009 | TestLoss: 0.69324 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.69161 | TestLoss: 0.69335 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.69125 | TestLoss: 0.69342 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.69293 | TestLoss: 0.69341 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.68947 | TestLoss: 0.69343 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.68809 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.69116 | TestLoss: 0.69349 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.69053 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.69009 | TestLoss: 0.69307 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.68912 | TestLoss: 0.69262 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 20 |  TrainLoss: 0.68997 | TestLoss: 0.69203 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 21 |  TrainLoss: 0.68695 | TestLoss: 0.69124 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 22 |  TrainLoss: 0.68524 | TestLoss: 0.69042 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 23 |  TrainLoss: 0.68694 | TestLoss: 0.68954 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 24 |  TrainLoss: 0.68625 | TestLoss: 0.68865 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 25 |  TrainLoss: 0.68472 | TestLoss: 0.68779 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 26 |  TrainLoss: 0.68288 | TestLoss: 0.68694 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 27 |  TrainLoss: 0.67958 | TestLoss: 0.68571 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 28 |  TrainLoss: 0.68023 | TestLoss: 0.68423 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 29 |  TrainLoss: 0.67929 | TestLoss: 0.68227 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 30 |  TrainLoss: 0.67533 | TestLoss: 0.67989 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 31 |  TrainLoss: 0.67194 | TestLoss: 0.67748 | TestAcc: 0.49774 | TestF1: 0.03\n",
      "Epoch: 32 |  TrainLoss: 0.66968 | TestLoss: 0.67462 | TestAcc: 0.51131 | TestF1: 0.08\n",
      "Epoch: 33 |  TrainLoss: 0.66597 | TestLoss: 0.67178 | TestAcc: 0.52489 | TestF1: 0.13\n",
      "Epoch: 34 |  TrainLoss: 0.65937 | TestLoss: 0.66814 | TestAcc: 0.57014 | TestF1: 0.27\n",
      "Epoch: 35 |  TrainLoss: 0.65477 | TestLoss: 0.66366 | TestAcc: 0.59276 | TestF1: 0.34\n",
      "Epoch: 36 |  TrainLoss: 0.65487 | TestLoss: 0.65812 | TestAcc: 0.65158 | TestF1: 0.49\n",
      "Epoch: 37 |  TrainLoss: 0.64457 | TestLoss: 0.65182 | TestAcc: 0.69683 | TestF1: 0.58\n",
      "Epoch: 38 |  TrainLoss: 0.63559 | TestLoss: 0.64447 | TestAcc: 0.72398 | TestF1: 0.65\n",
      "Epoch: 39 |  TrainLoss: 0.63441 | TestLoss: 0.63602 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 40 |  TrainLoss: 0.61956 | TestLoss: 0.62786 | TestAcc: 0.77376 | TestF1: 0.74\n",
      "Epoch: 41 |  TrainLoss: 0.60747 | TestLoss: 0.62100 | TestAcc: 0.71946 | TestF1: 0.64\n",
      "Epoch: 42 |  TrainLoss: 0.58873 | TestLoss: 0.61318 | TestAcc: 0.71493 | TestF1: 0.62\n",
      "Epoch: 43 |  TrainLoss: 0.58148 | TestLoss: 0.59693 | TestAcc: 0.78281 | TestF1: 0.76\n",
      "Epoch: 44 |  TrainLoss: 0.56192 | TestLoss: 0.58248 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 45 |  TrainLoss: 0.55185 | TestLoss: 0.57406 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 46 |  TrainLoss: 0.52854 | TestLoss: 0.56575 | TestAcc: 0.76018 | TestF1: 0.72\n",
      "Epoch: 47 |  TrainLoss: 0.51128 | TestLoss: 0.54401 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 48 |  TrainLoss: 0.48782 | TestLoss: 0.52479 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 49 |  TrainLoss: 0.47429 | TestLoss: 0.51123 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 50 |  TrainLoss: 0.44321 | TestLoss: 0.51240 | TestAcc: 0.78281 | TestF1: 0.75\n",
      "Epoch: 51 |  TrainLoss: 0.42704 | TestLoss: 0.48356 | TestAcc: 0.81900 | TestF1: 0.81\n",
      "Epoch: 52 |  TrainLoss: 0.38903 | TestLoss: 0.46170 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 53 |  TrainLoss: 0.38927 | TestLoss: 0.45412 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 54 |  TrainLoss: 0.33045 | TestLoss: 0.45331 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 55 |  TrainLoss: 0.32234 | TestLoss: 0.42755 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 56 |  TrainLoss: 0.29781 | TestLoss: 0.41982 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 57 |  TrainLoss: 0.27627 | TestLoss: 0.43902 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 58 |  TrainLoss: 0.24141 | TestLoss: 0.41593 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 59 |  TrainLoss: 0.23151 | TestLoss: 0.40611 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 60 |  TrainLoss: 0.21396 | TestLoss: 0.43835 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 61 |  TrainLoss: 0.20713 | TestLoss: 0.42139 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 62 |  TrainLoss: 0.15690 | TestLoss: 0.42826 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 63 |  TrainLoss: 0.13911 | TestLoss: 0.43982 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 64 |  TrainLoss: 0.14990 | TestLoss: 0.43963 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 65 |  TrainLoss: 0.12099 | TestLoss: 0.45292 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 66 |  TrainLoss: 0.10150 | TestLoss: 0.48277 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 67 |  TrainLoss: 0.10461 | TestLoss: 0.49279 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 68 |  TrainLoss: 0.08478 | TestLoss: 0.49273 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 69 |  TrainLoss: 0.07692 | TestLoss: 0.52097 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 70 |  TrainLoss: 0.06072 | TestLoss: 0.52591 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 71 |  TrainLoss: 0.04921 | TestLoss: 0.56251 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 72 |  TrainLoss: 0.06643 | TestLoss: 0.54007 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 73 |  TrainLoss: 0.05476 | TestLoss: 0.55652 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 74 |  TrainLoss: 0.03903 | TestLoss: 0.62418 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 75 |  TrainLoss: 0.03550 | TestLoss: 0.63513 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 76 |  TrainLoss: 0.03462 | TestLoss: 0.59614 | TestAcc: 0.87783 | TestF1: 0.88\n",
      "Epoch: 77 |  TrainLoss: 0.03751 | TestLoss: 0.62974 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 78 |  TrainLoss: 0.01990 | TestLoss: 0.68943 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 79 |  TrainLoss: 0.02130 | TestLoss: 0.68414 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 80 |  TrainLoss: 0.01102 | TestLoss: 0.67214 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 81 |  TrainLoss: 0.01692 | TestLoss: 0.68769 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 82 |  TrainLoss: 0.00938 | TestLoss: 0.69496 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 83 |  TrainLoss: 0.00699 | TestLoss: 0.70915 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 84 |  TrainLoss: 0.01390 | TestLoss: 0.72300 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 85 |  TrainLoss: 0.00487 | TestLoss: 0.74094 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 86 |  TrainLoss: 0.00500 | TestLoss: 0.75057 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 87 |  TrainLoss: 0.00359 | TestLoss: 0.75669 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 88 |  TrainLoss: 0.00535 | TestLoss: 0.75955 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 89 |  TrainLoss: 0.00355 | TestLoss: 0.76975 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 90 |  TrainLoss: 0.00405 | TestLoss: 0.78621 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 91 |  TrainLoss: 0.00203 | TestLoss: 0.80255 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 92 |  TrainLoss: 0.00478 | TestLoss: 0.80063 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 93 |  TrainLoss: 0.00206 | TestLoss: 0.79988 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 94 |  TrainLoss: 0.00208 | TestLoss: 0.80431 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 95 |  TrainLoss: 0.00285 | TestLoss: 0.81487 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 96 |  TrainLoss: 0.00237 | TestLoss: 0.82447 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 97 |  TrainLoss: 0.00199 | TestLoss: 0.83259 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 98 |  TrainLoss: 0.00166 | TestLoss: 0.84028 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 99 |  TrainLoss: 0.00132 | TestLoss: 0.84684 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 100 |  TrainLoss: 0.00222 | TestLoss: 0.84414 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 101 |  TrainLoss: 0.00183 | TestLoss: 0.84411 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 102 |  TrainLoss: 0.00136 | TestLoss: 0.84709 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 103 |  TrainLoss: 0.00108 | TestLoss: 0.85156 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 104 |  TrainLoss: 0.00094 | TestLoss: 0.85714 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 105 |  TrainLoss: 0.00077 | TestLoss: 0.86377 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 106 |  TrainLoss: 0.00084 | TestLoss: 0.86914 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 107 |  TrainLoss: 0.00121 | TestLoss: 0.87784 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 108 |  TrainLoss: 0.00142 | TestLoss: 0.89054 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 109 |  TrainLoss: 0.00108 | TestLoss: 0.90566 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 110 |  TrainLoss: 0.00094 | TestLoss: 0.91807 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 111 |  TrainLoss: 0.00131 | TestLoss: 0.92336 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 112 |  TrainLoss: 0.00097 | TestLoss: 0.92597 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 113 |  TrainLoss: 0.00117 | TestLoss: 0.92799 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 114 |  TrainLoss: 0.00102 | TestLoss: 0.92666 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 115 |  TrainLoss: 0.00100 | TestLoss: 0.92305 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 116 |  TrainLoss: 0.00094 | TestLoss: 0.91718 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 117 |  TrainLoss: 0.00101 | TestLoss: 0.91206 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 118 |  TrainLoss: 0.00057 | TestLoss: 0.90831 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 119 |  TrainLoss: 0.00058 | TestLoss: 0.90695 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 120 |  TrainLoss: 0.00069 | TestLoss: 0.90642 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 121 |  TrainLoss: 0.00059 | TestLoss: 0.90817 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 122 |  TrainLoss: 0.00138 | TestLoss: 0.91957 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 123 |  TrainLoss: 0.00085 | TestLoss: 0.93454 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 124 |  TrainLoss: 0.00087 | TestLoss: 0.94641 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 125 |  TrainLoss: 0.00056 | TestLoss: 0.95650 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 126 |  TrainLoss: 0.00064 | TestLoss: 0.96426 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 127 |  TrainLoss: 0.00077 | TestLoss: 0.96724 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 128 |  TrainLoss: 0.00056 | TestLoss: 0.96784 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 129 |  TrainLoss: 0.00035 | TestLoss: 0.96802 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 130 |  TrainLoss: 0.00044 | TestLoss: 0.96806 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 131 |  TrainLoss: 0.00032 | TestLoss: 0.96757 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 132 |  TrainLoss: 0.00050 | TestLoss: 0.96483 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 133 |  TrainLoss: 0.00073 | TestLoss: 0.96191 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 134 |  TrainLoss: 0.00068 | TestLoss: 0.95923 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 135 |  TrainLoss: 0.00050 | TestLoss: 0.95912 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 136 |  TrainLoss: 0.00059 | TestLoss: 0.96160 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 137 |  TrainLoss: 0.00043 | TestLoss: 0.96563 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 138 |  TrainLoss: 0.00049 | TestLoss: 0.96785 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 139 |  TrainLoss: 0.00066 | TestLoss: 0.97275 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 140 |  TrainLoss: 0.00051 | TestLoss: 0.97848 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 141 |  TrainLoss: 0.00044 | TestLoss: 0.98424 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 142 |  TrainLoss: 0.00034 | TestLoss: 0.98833 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 143 |  TrainLoss: 0.00048 | TestLoss: 0.98989 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 144 |  TrainLoss: 0.00045 | TestLoss: 0.98908 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 145 |  TrainLoss: 0.00025 | TestLoss: 0.98852 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 146 |  TrainLoss: 0.00036 | TestLoss: 0.98888 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 147 |  TrainLoss: 0.00038 | TestLoss: 0.99065 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 148 |  TrainLoss: 0.00038 | TestLoss: 0.99210 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 149 |  TrainLoss: 0.00034 | TestLoss: 0.99402 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 150 |  TrainLoss: 0.00045 | TestLoss: 0.99573 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 151 |  TrainLoss: 0.00035 | TestLoss: 0.99613 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 152 |  TrainLoss: 0.00032 | TestLoss: 0.99802 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 153 |  TrainLoss: 0.00047 | TestLoss: 1.00069 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 154 |  TrainLoss: 0.00024 | TestLoss: 1.00242 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 155 |  TrainLoss: 0.00039 | TestLoss: 1.00525 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 156 |  TrainLoss: 0.00052 | TestLoss: 1.00783 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 157 |  TrainLoss: 0.00022 | TestLoss: 1.01082 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 158 |  TrainLoss: 0.00031 | TestLoss: 1.01296 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 159 |  TrainLoss: 0.00026 | TestLoss: 1.01522 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 160 |  TrainLoss: 0.00032 | TestLoss: 1.01713 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 161 |  TrainLoss: 0.00076 | TestLoss: 1.02203 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 162 |  TrainLoss: 0.00015 | TestLoss: 1.02650 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 163 |  TrainLoss: 0.00021 | TestLoss: 1.03082 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 164 |  TrainLoss: 0.00034 | TestLoss: 1.03613 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 165 |  TrainLoss: 0.00019 | TestLoss: 1.04040 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 166 |  TrainLoss: 0.00026 | TestLoss: 1.04363 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 167 |  TrainLoss: 0.00035 | TestLoss: 1.04456 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 168 |  TrainLoss: 0.00025 | TestLoss: 1.04567 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 169 |  TrainLoss: 0.00041 | TestLoss: 1.04849 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 170 |  TrainLoss: 0.00029 | TestLoss: 1.04853 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 171 |  TrainLoss: 0.00020 | TestLoss: 1.04806 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 172 |  TrainLoss: 0.00023 | TestLoss: 1.04686 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 173 |  TrainLoss: 0.00024 | TestLoss: 1.04557 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 174 |  TrainLoss: 0.00017 | TestLoss: 1.04417 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 175 |  TrainLoss: 0.00042 | TestLoss: 1.04571 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 176 |  TrainLoss: 0.00021 | TestLoss: 1.04768 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 177 |  TrainLoss: 0.00032 | TestLoss: 1.05059 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 178 |  TrainLoss: 0.00017 | TestLoss: 1.05388 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 179 |  TrainLoss: 0.00028 | TestLoss: 1.05689 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 180 |  TrainLoss: 0.00026 | TestLoss: 1.05903 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 181 |  TrainLoss: 0.00030 | TestLoss: 1.05951 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 182 |  TrainLoss: 0.00020 | TestLoss: 1.05989 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 183 |  TrainLoss: 0.00019 | TestLoss: 1.05973 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 184 |  TrainLoss: 0.00021 | TestLoss: 1.05879 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 185 |  TrainLoss: 0.00025 | TestLoss: 1.05833 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 186 |  TrainLoss: 0.00015 | TestLoss: 1.05792 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 187 |  TrainLoss: 0.00023 | TestLoss: 1.05822 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 188 |  TrainLoss: 0.00015 | TestLoss: 1.05878 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 189 |  TrainLoss: 0.00023 | TestLoss: 1.06061 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 190 |  TrainLoss: 0.00040 | TestLoss: 1.06766 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 191 |  TrainLoss: 0.00019 | TestLoss: 1.07454 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 192 |  TrainLoss: 0.00018 | TestLoss: 1.08135 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 193 |  TrainLoss: 0.00022 | TestLoss: 1.08774 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 194 |  TrainLoss: 0.00028 | TestLoss: 1.09288 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 195 |  TrainLoss: 0.00012 | TestLoss: 1.09663 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 196 |  TrainLoss: 0.00019 | TestLoss: 1.10116 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 197 |  TrainLoss: 0.00020 | TestLoss: 1.10420 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 198 |  TrainLoss: 0.00018 | TestLoss: 1.10483 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 199 |  TrainLoss: 0.00029 | TestLoss: 1.10295 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 200 |  TrainLoss: 0.00014 | TestLoss: 1.10088 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 201 |  TrainLoss: 0.00021 | TestLoss: 1.09844 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 202 |  TrainLoss: 0.00008 | TestLoss: 1.09632 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 203 |  TrainLoss: 0.00016 | TestLoss: 1.09473 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 204 |  TrainLoss: 0.00013 | TestLoss: 1.09278 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 205 |  TrainLoss: 0.00032 | TestLoss: 1.09347 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 206 |  TrainLoss: 0.00018 | TestLoss: 1.09471 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 207 |  TrainLoss: 0.00014 | TestLoss: 1.09512 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 208 |  TrainLoss: 0.00014 | TestLoss: 1.09557 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 209 |  TrainLoss: 0.00067 | TestLoss: 1.08625 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 210 |  TrainLoss: 0.00010 | TestLoss: 1.07881 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 211 |  TrainLoss: 0.00023 | TestLoss: 1.07398 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 212 |  TrainLoss: 0.00012 | TestLoss: 1.07080 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 213 |  TrainLoss: 0.00032 | TestLoss: 1.07271 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 214 |  TrainLoss: 0.00030 | TestLoss: 1.07837 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 215 |  TrainLoss: 0.00012 | TestLoss: 1.08506 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 216 |  TrainLoss: 0.00045 | TestLoss: 1.09883 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 217 |  TrainLoss: 0.00008 | TestLoss: 1.11251 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 218 |  TrainLoss: 0.00015 | TestLoss: 1.12649 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 219 |  TrainLoss: 0.00013 | TestLoss: 1.13881 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 220 |  TrainLoss: 0.00014 | TestLoss: 1.14943 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 221 |  TrainLoss: 0.00014 | TestLoss: 1.15795 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 222 |  TrainLoss: 0.00013 | TestLoss: 1.16435 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 223 |  TrainLoss: 0.00017 | TestLoss: 1.16726 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 224 |  TrainLoss: 0.00017 | TestLoss: 1.16716 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 225 |  TrainLoss: 0.00018 | TestLoss: 1.16409 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 226 |  TrainLoss: 0.00010 | TestLoss: 1.16011 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 227 |  TrainLoss: 0.00013 | TestLoss: 1.15530 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 228 |  TrainLoss: 0.00009 | TestLoss: 1.15006 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 229 |  TrainLoss: 0.00011 | TestLoss: 1.14487 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 230 |  TrainLoss: 0.00015 | TestLoss: 1.13833 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 231 |  TrainLoss: 0.00011 | TestLoss: 1.13174 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 232 |  TrainLoss: 0.00010 | TestLoss: 1.12561 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 233 |  TrainLoss: 0.00014 | TestLoss: 1.12143 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 234 |  TrainLoss: 0.00014 | TestLoss: 1.11937 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 235 |  TrainLoss: 0.00021 | TestLoss: 1.12084 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 236 |  TrainLoss: 0.00009 | TestLoss: 1.12219 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 237 |  TrainLoss: 0.00007 | TestLoss: 1.12396 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 238 |  TrainLoss: 0.00009 | TestLoss: 1.12652 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 239 |  TrainLoss: 0.00011 | TestLoss: 1.12889 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 240 |  TrainLoss: 0.00017 | TestLoss: 1.13302 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 241 |  TrainLoss: 0.00012 | TestLoss: 1.13793 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 242 |  TrainLoss: 0.00009 | TestLoss: 1.14241 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 243 |  TrainLoss: 0.00008 | TestLoss: 1.14671 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 244 |  TrainLoss: 0.00007 | TestLoss: 1.15096 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 245 |  TrainLoss: 0.00007 | TestLoss: 1.15529 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 246 |  TrainLoss: 0.00010 | TestLoss: 1.15890 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 247 |  TrainLoss: 0.00009 | TestLoss: 1.16162 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 248 |  TrainLoss: 0.00010 | TestLoss: 1.16338 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 249 |  TrainLoss: 0.00011 | TestLoss: 1.16411 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 250 |  TrainLoss: 0.00007 | TestLoss: 1.16440 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 251 |  TrainLoss: 0.00006 | TestLoss: 1.16416 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 252 |  TrainLoss: 0.00015 | TestLoss: 1.16585 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 253 |  TrainLoss: 0.00006 | TestLoss: 1.16725 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 254 |  TrainLoss: 0.00011 | TestLoss: 1.16939 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 255 |  TrainLoss: 0.00006 | TestLoss: 1.17144 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 256 |  TrainLoss: 0.00009 | TestLoss: 1.17339 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 257 |  TrainLoss: 0.00014 | TestLoss: 1.17300 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 258 |  TrainLoss: 0.00013 | TestLoss: 1.17149 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 259 |  TrainLoss: 0.00008 | TestLoss: 1.16928 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 260 |  TrainLoss: 0.00011 | TestLoss: 1.16752 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 261 |  TrainLoss: 0.00010 | TestLoss: 1.16533 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 262 |  TrainLoss: 0.00011 | TestLoss: 1.16485 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 263 |  TrainLoss: 0.00005 | TestLoss: 1.16435 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 264 |  TrainLoss: 0.00006 | TestLoss: 1.16383 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 265 |  TrainLoss: 0.00009 | TestLoss: 1.16441 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 266 |  TrainLoss: 0.00012 | TestLoss: 1.16600 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 267 |  TrainLoss: 0.00004 | TestLoss: 1.16747 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 268 |  TrainLoss: 0.00006 | TestLoss: 1.16915 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 269 |  TrainLoss: 0.00008 | TestLoss: 1.17142 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 270 |  TrainLoss: 0.00005 | TestLoss: 1.17380 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 271 |  TrainLoss: 0.00005 | TestLoss: 1.17595 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 272 |  TrainLoss: 0.00004 | TestLoss: 1.17774 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 273 |  TrainLoss: 0.00003 | TestLoss: 1.17956 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 274 |  TrainLoss: 0.00011 | TestLoss: 1.18262 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 275 |  TrainLoss: 0.00007 | TestLoss: 1.18478 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 276 |  TrainLoss: 0.00006 | TestLoss: 1.18629 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 277 |  TrainLoss: 0.00012 | TestLoss: 1.18821 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 278 |  TrainLoss: 0.00009 | TestLoss: 1.18925 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 279 |  TrainLoss: 0.00007 | TestLoss: 1.18927 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 280 |  TrainLoss: 0.00004 | TestLoss: 1.18958 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 281 |  TrainLoss: 0.00008 | TestLoss: 1.18969 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 282 |  TrainLoss: 0.00013 | TestLoss: 1.18809 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 283 |  TrainLoss: 0.00005 | TestLoss: 1.18673 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 284 |  TrainLoss: 0.00005 | TestLoss: 1.18573 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 285 |  TrainLoss: 0.00013 | TestLoss: 1.18461 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 286 |  TrainLoss: 0.00003 | TestLoss: 1.18398 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 287 |  TrainLoss: 0.00025 | TestLoss: 1.18848 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 288 |  TrainLoss: 0.00008 | TestLoss: 1.19197 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 289 |  TrainLoss: 0.00011 | TestLoss: 1.19390 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 290 |  TrainLoss: 0.00005 | TestLoss: 1.19570 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 291 |  TrainLoss: 0.00005 | TestLoss: 1.19779 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 292 |  TrainLoss: 0.00003 | TestLoss: 1.19955 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 293 |  TrainLoss: 0.00006 | TestLoss: 1.20107 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 294 |  TrainLoss: 0.00023 | TestLoss: 1.20817 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 295 |  TrainLoss: 0.00012 | TestLoss: 1.21378 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 296 |  TrainLoss: 0.00004 | TestLoss: 1.21915 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 297 |  TrainLoss: 0.00007 | TestLoss: 1.22288 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 298 |  TrainLoss: 0.00006 | TestLoss: 1.22544 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 299 |  TrainLoss: 0.00010 | TestLoss: 1.22704 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 300 |  TrainLoss: 0.00004 | TestLoss: 1.22845 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 301 |  TrainLoss: 0.00005 | TestLoss: 1.22866 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 302 |  TrainLoss: 0.00008 | TestLoss: 1.22748 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 303 |  TrainLoss: 0.00005 | TestLoss: 1.22643 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 304 |  TrainLoss: 0.00010 | TestLoss: 1.22517 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 305 |  TrainLoss: 0.00008 | TestLoss: 1.22277 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 306 |  TrainLoss: 0.00006 | TestLoss: 1.21995 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 307 |  TrainLoss: 0.00007 | TestLoss: 1.21694 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 308 |  TrainLoss: 0.00003 | TestLoss: 1.21421 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 309 |  TrainLoss: 0.00005 | TestLoss: 1.21276 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 310 |  TrainLoss: 0.00004 | TestLoss: 1.21122 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 311 |  TrainLoss: 0.00002 | TestLoss: 1.20994 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 312 |  TrainLoss: 0.00005 | TestLoss: 1.20892 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 313 |  TrainLoss: 0.00005 | TestLoss: 1.20877 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 314 |  TrainLoss: 0.00008 | TestLoss: 1.20745 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 315 |  TrainLoss: 0.00004 | TestLoss: 1.20710 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 316 |  TrainLoss: 0.00005 | TestLoss: 1.20711 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 317 |  TrainLoss: 0.00003 | TestLoss: 1.20716 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 318 |  TrainLoss: 0.00005 | TestLoss: 1.20783 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 319 |  TrainLoss: 0.00003 | TestLoss: 1.20935 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 320 |  TrainLoss: 0.00004 | TestLoss: 1.21068 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 321 |  TrainLoss: 0.00005 | TestLoss: 1.21244 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 322 |  TrainLoss: 0.00007 | TestLoss: 1.21493 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 323 |  TrainLoss: 0.00005 | TestLoss: 1.21758 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 324 |  TrainLoss: 0.00005 | TestLoss: 1.22032 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 325 |  TrainLoss: 0.00003 | TestLoss: 1.22282 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 326 |  TrainLoss: 0.00004 | TestLoss: 1.22532 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 327 |  TrainLoss: 0.00005 | TestLoss: 1.22804 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 328 |  TrainLoss: 0.00008 | TestLoss: 1.23019 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 329 |  TrainLoss: 0.00004 | TestLoss: 1.23251 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 330 |  TrainLoss: 0.00005 | TestLoss: 1.23427 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 331 |  TrainLoss: 0.00004 | TestLoss: 1.23590 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 332 |  TrainLoss: 0.00004 | TestLoss: 1.23775 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 333 |  TrainLoss: 0.00004 | TestLoss: 1.23931 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 334 |  TrainLoss: 0.00005 | TestLoss: 1.24077 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 335 |  TrainLoss: 0.00004 | TestLoss: 1.24178 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 336 |  TrainLoss: 0.00003 | TestLoss: 1.24274 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 337 |  TrainLoss: 0.00005 | TestLoss: 1.24320 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 338 |  TrainLoss: 0.00003 | TestLoss: 1.24317 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 339 |  TrainLoss: 0.00005 | TestLoss: 1.24384 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 340 |  TrainLoss: 0.00009 | TestLoss: 1.24325 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 341 |  TrainLoss: 0.00004 | TestLoss: 1.24356 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 342 |  TrainLoss: 0.00002 | TestLoss: 1.24336 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 343 |  TrainLoss: 0.00002 | TestLoss: 1.24290 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 344 |  TrainLoss: 0.00005 | TestLoss: 1.24247 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 345 |  TrainLoss: 0.00005 | TestLoss: 1.24100 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 346 |  TrainLoss: 0.00007 | TestLoss: 1.24146 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 347 |  TrainLoss: 0.00005 | TestLoss: 1.24222 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 348 |  TrainLoss: 0.00003 | TestLoss: 1.24286 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 349 |  TrainLoss: 0.00004 | TestLoss: 1.24451 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 350 |  TrainLoss: 0.00003 | TestLoss: 1.24508 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 351 |  TrainLoss: 0.00004 | TestLoss: 1.24590 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 352 |  TrainLoss: 0.00002 | TestLoss: 1.24665 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 353 |  TrainLoss: 0.00003 | TestLoss: 1.24720 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 354 |  TrainLoss: 0.00003 | TestLoss: 1.24749 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 355 |  TrainLoss: 0.00009 | TestLoss: 1.24670 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 356 |  TrainLoss: 0.00002 | TestLoss: 1.24571 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 357 |  TrainLoss: 0.00002 | TestLoss: 1.24503 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 358 |  TrainLoss: 0.00005 | TestLoss: 1.24499 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 359 |  TrainLoss: 0.00002 | TestLoss: 1.24513 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 360 |  TrainLoss: 0.00005 | TestLoss: 1.24617 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 361 |  TrainLoss: 0.00008 | TestLoss: 1.24872 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 362 |  TrainLoss: 0.00004 | TestLoss: 1.25007 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 363 |  TrainLoss: 0.00004 | TestLoss: 1.25232 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 364 |  TrainLoss: 0.00004 | TestLoss: 1.25398 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 365 |  TrainLoss: 0.00002 | TestLoss: 1.25520 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 366 |  TrainLoss: 0.00003 | TestLoss: 1.25654 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 367 |  TrainLoss: 0.00003 | TestLoss: 1.25785 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 368 |  TrainLoss: 0.00002 | TestLoss: 1.25946 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 369 |  TrainLoss: 0.00006 | TestLoss: 1.26132 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 370 |  TrainLoss: 0.00002 | TestLoss: 1.26295 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 371 |  TrainLoss: 0.00003 | TestLoss: 1.26455 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 372 |  TrainLoss: 0.00005 | TestLoss: 1.26668 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 373 |  TrainLoss: 0.00003 | TestLoss: 1.26847 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 374 |  TrainLoss: 0.00006 | TestLoss: 1.26869 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 375 |  TrainLoss: 0.00003 | TestLoss: 1.26884 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 376 |  TrainLoss: 0.00004 | TestLoss: 1.26953 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 377 |  TrainLoss: 0.00002 | TestLoss: 1.27035 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 378 |  TrainLoss: 0.00004 | TestLoss: 1.27051 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 379 |  TrainLoss: 0.00002 | TestLoss: 1.27092 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 380 |  TrainLoss: 0.00002 | TestLoss: 1.27106 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 381 |  TrainLoss: 0.00004 | TestLoss: 1.27110 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 382 |  TrainLoss: 0.00002 | TestLoss: 1.27191 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 383 |  TrainLoss: 0.00004 | TestLoss: 1.27247 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 384 |  TrainLoss: 0.00002 | TestLoss: 1.27298 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 385 |  TrainLoss: 0.00002 | TestLoss: 1.27346 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 386 |  TrainLoss: 0.00002 | TestLoss: 1.27552 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 387 |  TrainLoss: 0.00005 | TestLoss: 1.27558 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 388 |  TrainLoss: 0.00008 | TestLoss: 1.27365 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 389 |  TrainLoss: 0.00003 | TestLoss: 1.27158 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 390 |  TrainLoss: 0.00003 | TestLoss: 1.27045 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 391 |  TrainLoss: 0.00002 | TestLoss: 1.26885 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 392 |  TrainLoss: 0.00002 | TestLoss: 1.26783 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 393 |  TrainLoss: 0.00004 | TestLoss: 1.26778 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 394 |  TrainLoss: 0.00010 | TestLoss: 1.27110 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 395 |  TrainLoss: 0.00002 | TestLoss: 1.27414 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 396 |  TrainLoss: 0.00003 | TestLoss: 1.27698 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 397 |  TrainLoss: 0.00003 | TestLoss: 1.27892 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 398 |  TrainLoss: 0.00002 | TestLoss: 1.28098 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 399 |  TrainLoss: 0.00002 | TestLoss: 1.28302 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 400 |  TrainLoss: 0.00002 | TestLoss: 1.28426 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 401 |  TrainLoss: 0.00002 | TestLoss: 1.28594 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 402 |  TrainLoss: 0.00001 | TestLoss: 1.28754 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 403 |  TrainLoss: 0.00003 | TestLoss: 1.28980 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 404 |  TrainLoss: 0.00002 | TestLoss: 1.29212 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 405 |  TrainLoss: 0.00002 | TestLoss: 1.29386 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 406 |  TrainLoss: 0.00002 | TestLoss: 1.29545 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 407 |  TrainLoss: 0.00003 | TestLoss: 1.29639 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 408 |  TrainLoss: 0.00001 | TestLoss: 1.29719 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 409 |  TrainLoss: 0.00002 | TestLoss: 1.29763 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 410 |  TrainLoss: 0.00003 | TestLoss: 1.29731 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 411 |  TrainLoss: 0.00001 | TestLoss: 1.29729 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 412 |  TrainLoss: 0.00002 | TestLoss: 1.29679 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 413 |  TrainLoss: 0.00003 | TestLoss: 1.29632 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 414 |  TrainLoss: 0.00002 | TestLoss: 1.29575 | TestAcc: 0.87330 | TestF1: 0.87\n",
      "Epoch: 415 |  TrainLoss: 0.00003 | TestLoss: 1.29540 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 416 |  TrainLoss: 0.00002 | TestLoss: 1.29538 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 417 |  TrainLoss: 0.00002 | TestLoss: 1.29487 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 418 |  TrainLoss: 0.00004 | TestLoss: 1.29425 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 419 |  TrainLoss: 0.00002 | TestLoss: 1.29533 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 420 |  TrainLoss: 0.00003 | TestLoss: 1.29468 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 421 |  TrainLoss: 0.00003 | TestLoss: 1.29349 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 422 |  TrainLoss: 0.00002 | TestLoss: 1.29245 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 1.29124 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 424 |  TrainLoss: 0.00003 | TestLoss: 1.29135 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 425 |  TrainLoss: 0.00002 | TestLoss: 1.29102 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 426 |  TrainLoss: 0.00002 | TestLoss: 1.29103 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 427 |  TrainLoss: 0.00004 | TestLoss: 1.29032 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 428 |  TrainLoss: 0.00004 | TestLoss: 1.28934 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 429 |  TrainLoss: 0.00012 | TestLoss: 1.29251 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 430 |  TrainLoss: 0.00001 | TestLoss: 1.29520 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 431 |  TrainLoss: 0.00002 | TestLoss: 1.29759 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 432 |  TrainLoss: 0.00003 | TestLoss: 1.30033 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 433 |  TrainLoss: 0.00001 | TestLoss: 1.30269 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 434 |  TrainLoss: 0.00002 | TestLoss: 1.30479 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 435 |  TrainLoss: 0.00003 | TestLoss: 1.30660 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 436 |  TrainLoss: 0.00008 | TestLoss: 1.30790 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 437 |  TrainLoss: 0.00002 | TestLoss: 1.30877 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 438 |  TrainLoss: 0.00001 | TestLoss: 1.30936 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 439 |  TrainLoss: 0.00003 | TestLoss: 1.30993 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 440 |  TrainLoss: 0.00002 | TestLoss: 1.30972 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 441 |  TrainLoss: 0.00003 | TestLoss: 1.30907 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 442 |  TrainLoss: 0.00001 | TestLoss: 1.30894 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 443 |  TrainLoss: 0.00002 | TestLoss: 1.30877 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 444 |  TrainLoss: 0.00002 | TestLoss: 1.30840 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 445 |  TrainLoss: 0.00002 | TestLoss: 1.30811 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 446 |  TrainLoss: 0.00001 | TestLoss: 1.30759 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 447 |  TrainLoss: 0.00002 | TestLoss: 1.30779 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 1.30803 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 449 |  TrainLoss: 0.00002 | TestLoss: 1.30868 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 450 |  TrainLoss: 0.00001 | TestLoss: 1.31005 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 451 |  TrainLoss: 0.00003 | TestLoss: 1.31118 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 452 |  TrainLoss: 0.00003 | TestLoss: 1.31236 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 453 |  TrainLoss: 0.00001 | TestLoss: 1.31333 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 454 |  TrainLoss: 0.00001 | TestLoss: 1.31413 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 455 |  TrainLoss: 0.00003 | TestLoss: 1.31525 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 456 |  TrainLoss: 0.00002 | TestLoss: 1.31631 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 457 |  TrainLoss: 0.00002 | TestLoss: 1.31733 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 458 |  TrainLoss: 0.00002 | TestLoss: 1.31869 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 459 |  TrainLoss: 0.00001 | TestLoss: 1.31982 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 460 |  TrainLoss: 0.00002 | TestLoss: 1.32419 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 461 |  TrainLoss: 0.00002 | TestLoss: 1.32477 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 462 |  TrainLoss: 0.00003 | TestLoss: 1.32496 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 463 |  TrainLoss: 0.00002 | TestLoss: 1.32532 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 464 |  TrainLoss: 0.00001 | TestLoss: 1.32547 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 465 |  TrainLoss: 0.00001 | TestLoss: 1.32525 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 466 |  TrainLoss: 0.00001 | TestLoss: 1.32531 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 467 |  TrainLoss: 0.00003 | TestLoss: 1.32430 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 468 |  TrainLoss: 0.00002 | TestLoss: 1.32397 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 469 |  TrainLoss: 0.00001 | TestLoss: 1.32292 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 470 |  TrainLoss: 0.00002 | TestLoss: 1.32234 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 471 |  TrainLoss: 0.00003 | TestLoss: 1.32152 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 472 |  TrainLoss: 0.00001 | TestLoss: 1.32108 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 473 |  TrainLoss: 0.00001 | TestLoss: 1.32055 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 474 |  TrainLoss: 0.00002 | TestLoss: 1.31981 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 475 |  TrainLoss: 0.00001 | TestLoss: 1.31970 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 476 |  TrainLoss: 0.00002 | TestLoss: 1.32013 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 477 |  TrainLoss: 0.00002 | TestLoss: 1.32040 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 478 |  TrainLoss: 0.00001 | TestLoss: 1.32182 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 479 |  TrainLoss: 0.00003 | TestLoss: 1.32315 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 480 |  TrainLoss: 0.00001 | TestLoss: 1.32415 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 1.32504 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 482 |  TrainLoss: 0.00007 | TestLoss: 1.32529 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 483 |  TrainLoss: 0.00001 | TestLoss: 1.32623 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 484 |  TrainLoss: 0.00002 | TestLoss: 1.32669 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 485 |  TrainLoss: 0.00001 | TestLoss: 1.32738 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 486 |  TrainLoss: 0.00002 | TestLoss: 1.32809 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 487 |  TrainLoss: 0.00002 | TestLoss: 1.32866 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 488 |  TrainLoss: 0.00003 | TestLoss: 1.32996 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 489 |  TrainLoss: 0.00002 | TestLoss: 1.33152 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 490 |  TrainLoss: 0.00002 | TestLoss: 1.33371 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 491 |  TrainLoss: 0.00001 | TestLoss: 1.33558 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 492 |  TrainLoss: 0.00002 | TestLoss: 1.33690 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 493 |  TrainLoss: 0.00002 | TestLoss: 1.33822 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 494 |  TrainLoss: 0.00001 | TestLoss: 1.33948 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 495 |  TrainLoss: 0.00002 | TestLoss: 1.34074 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 496 |  TrainLoss: 0.00001 | TestLoss: 1.34190 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 497 |  TrainLoss: 0.00001 | TestLoss: 1.34298 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 498 |  TrainLoss: 0.00002 | TestLoss: 1.34411 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 499 |  TrainLoss: 0.00001 | TestLoss: 1.34503 | TestAcc: 0.86878 | TestF1: 0.87\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wloss = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accs.append(test_acc)\n",
    "\n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EgTcpZrX81Re"
   },
   "source": [
    "##Plot of Test Accuracy over best Loss and Best Accuracy Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPqyRgZUxMgO",
    "outputId": "76f61823-b567-4fb7-b80b-6ac6afdc9085"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accs, label='Test Accuracy')\n",
    "\n",
    "# Add legend and axis labels\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "\n",
    "# Add marker for best epoch\n",
    "best_epoch_loss = test_losses.index(min(test_losses))\n",
    "best_epoch_acc = test_accs.index(max(test_accs))\n",
    "plt.axvline(x=best_epoch_loss, color='r', linestyle='--', label='Best Loss Epoch')\n",
    "plt.axvline(x=best_epoch_acc, color='g', linestyle='--', label='Best Acc Epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t_K0ZygB9J45"
   },
   "source": [
    "#GIN on BERT Embeddings using Optimal Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ujB6X_f7X0V"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = Net(in_channels=768, hidden_channels=[trial.suggest_int(name=\"layer_size1\", low=256, high=512, step=128),\n",
    "                                                  trial.suggest_int(name=\"layer_size2\", low=256, high=512, step=128),\n",
    "                                                  trial.suggest_int(name=\"layer_size3\", low=256, high=512, step=128),\n",
    "                                                  trial.suggest_int(name=\"layer_size4\", low=64, high=256, step=64),\n",
    "                                                  trial.suggest_int(name=\"layer_size5\", low=64, high=256, step=64),\n",
    "                                                 ],\n",
    "                out_channels=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),\n",
    "                                 betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3), 0.99))\n",
    "    lossff = torch.nn.BCELoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    weighted_loss = 0\n",
    "    exp_param = 0.8\n",
    "    wloss = []\n",
    "    best_val = 1\n",
    "\n",
    "    for i in range(1600):\n",
    "        print(\"Epoch:\", i)\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "        print(\"Train: \",total_loss / len(train_loader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "        print(\"Test\", total_loss / len(val_loader.dataset))\n",
    "\n",
    "        weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(total_loss/ len(val_loader.dataset))\n",
    "        print(weighted_loss/(1-exp_param**(i+1)))\n",
    "        wloss.append(weighted_loss/(1-exp_param**(i+1)))\n",
    "\n",
    "        if(best_val>weighted_loss/(1-exp_param**(i+1))):\n",
    "            best_val = weighted_loss/(1-exp_param**(i+1))\n",
    "\n",
    "        if(i-10>=0 and wloss[i-10]-weighted_loss<-0.01):\n",
    "            break\n",
    "\n",
    "    return best_val\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xd9WtQiy9NXK"
   },
   "source": [
    "##Training Our Model using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJoRtoIJ7X0V",
    "outputId": "1936351f-7de0-4998-f1de-7ccc8b4dad81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (3.1.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (1.4.32)\n",
      "Requirement already satisfied: numpy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (1.21.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (1.10.4)\n",
      "Requirement already satisfied: PyYAML in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (4.64.0)\n",
      "Requirement already satisfied: colorlog in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.1.1)\n",
      "Requirement already satisfied: Mako in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ8CQgk47X0V"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = Net(in_channels=768, \n",
    "                hidden_channels=[trial.suggest_int(name=f\"hidden_size_{i}\", low=256, high=512, step=128) for i in range(6)],\n",
    "                out_channels=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),\n",
    "                                 betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3), 0.99))\n",
    "    loss_func = torch.nn.BCELoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    weighted_loss = 0\n",
    "    exp_param = 0.8\n",
    "\n",
    "    wloss = []\n",
    "    best_val = 1\n",
    "\n",
    "    for i in range(1600):\n",
    "        print(\"Epoch:\", i)\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = loss_func(torch.reshape(out,(-1,)), data.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "        print(\"Train: \",total_loss / len(train_loader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = loss_func(torch.reshape(out,(-1,)), data.y.float())\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "        print(\"Val: \", total_loss / len(val_loader.dataset))\n",
    "\n",
    "        weighted_loss = exp_param * weighted_loss + (1-exp_param) * (total_loss / len(val_loader.dataset))\n",
    "        print(weighted_loss / (1-exp_param**(i+1)))\n",
    "        wloss.append(weighted_loss / (1-exp_param**(i+1)))\n",
    "\n",
    "        if best_val > weighted_loss / (1-exp_param**(i+1)):\n",
    "            best_val = weighted_loss / (1-exp_param**(i+1))\n",
    "\n",
    "        if i - 10 >= 0 and wloss[i-10] - weighted_loss < -0.01:\n",
    "            break\n",
    "\n",
    "    return best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4owIZMS7X0W"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeoxwkSO7X0W"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = Net(in_channels=768, \n",
    "                hidden_channels=[trial.suggest_int(name=f\"hidden_size_{i}\", low=256, high=512, step=128) for i in range(6)],\n",
    "                out_channels=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),\n",
    "                                 betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3), 0.99))\n",
    "    loss_func = torch.nn.BCELoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    weighted_loss = 0\n",
    "    exp_param = 0.8\n",
    "\n",
    "    wloss = []\n",
    "    best_val = 1\n",
    "\n",
    "    for i in range(1600):\n",
    "        print(\"Epoch:\", i)\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = loss_func(torch.reshape(out,(-1,)), data.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "        print(\"Train: \",total_loss / len(train_loader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = loss_func(torch.reshape(out,(-1,)), data.y.float())\n",
    "            total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "        print(\"Test: \", total_loss / len(test_loader.dataset))\n",
    "\n",
    "        weighted_loss = exp_param * weighted_loss + (1-exp_param) * (total_loss / len(test_loader.dataset))\n",
    "        print(weighted_loss / (1-exp_param**(i+1)))\n",
    "        wloss.append(weighted_loss / (1-exp_param**(i+1)))\n",
    "\n",
    "        if best_val > weighted_loss / (1-exp_param**(i+1)):\n",
    "            best_val = weighted_loss / (1-exp_param**(i+1))\n",
    "\n",
    "        if i - 10 >= 0 and wloss[i-10] - weighted_loss < -0.01:\n",
    "            break\n",
    "\n",
    "    return best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbNqcmEQ7X0W",
    "outputId": "0b6e1c4f-d2fd-420a-af8b-ffaac3821cad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-15 17:42:33,798]\u001b[0m A new study created in memory with name: no-name-c01bad10-77e8-41f9-aaa7-eafce3b06b3c\u001b[0m\n",
      "/var/folders/q8/cv7jrv9n5tx6ydvwt_sft3640000gn/T/ipykernel_92563/1571534392.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),\n",
      "/var/folders/q8/cv7jrv9n5tx6ydvwt_sft3640000gn/T/ipykernel_92563/1571534392.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3), 0.99))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train:  0.6936161518096924\n",
      "Test:  0.6930882334709167\n",
      "0.6930882334709166\n",
      "Epoch: 1\n",
      "Train:  2.3396491574984726\n",
      "Test:  0.6930922865867615\n",
      "0.6930904852019416\n",
      "Epoch: 2\n",
      "Train:  2.338781647143825\n",
      "Test:  0.6930826902389526\n",
      "0.693087290544979\n",
      "Epoch: 3\n",
      "Train:  2.3379109668475326\n",
      "Test:  0.6930773854255676\n",
      "0.6930839351522244\n",
      "Epoch: 4\n",
      "Train:  2.3395435681907077\n",
      "Test:  0.6930771470069885\n",
      "0.6930819158324851\n",
      "Epoch: 5\n",
      "Train:  2.3390733170252975\n",
      "Test:  0.6930758357048035\n",
      "0.6930802677798349\n",
      "Epoch: 6\n",
      "Train:  2.337769091129303\n",
      "Test:  0.6930808424949646\n",
      "0.6930804132249021\n",
      "Epoch: 7\n",
      "Train:  2.3388548858704103\n",
      "Test:  0.6930841207504272\n",
      "0.6930813042130058\n",
      "Epoch: 8\n",
      "Train:  2.3385479636089777\n",
      "Test:  0.6930885314941406\n",
      "0.6930829737508026\n",
      "Epoch: 9\n",
      "Train:  2.340692052918096\n",
      "Test:  0.6930946707725525\n",
      "0.6930855945626857\n",
      "Epoch: 10\n",
      "Train:  2.337180729194354\n",
      "Test:  0.6930986642837524\n",
      "0.6930884541429755\n",
      "Epoch: 11\n",
      "Train:  2.3385548078885643\n",
      "Test:  0.6931004524230957\n",
      "0.6930910308703568\n",
      "Epoch: 12\n",
      "Train:  2.338471676713677\n",
      "Test:  0.6931054592132568\n",
      "0.693094084408999\n",
      "Epoch: 13\n",
      "Train:  2.3400874759561274\n",
      "Test:  0.6931094527244568\n",
      "0.6930972994720592\n",
      "Epoch: 14\n",
      "Train:  2.339400306824715\n",
      "Test:  0.6931151151657104\n",
      "0.6931009925493968\n",
      "Epoch: 15\n",
      "Train:  2.3390254980774334\n",
      "Test:  0.6931222081184387\n",
      "0.6931053585553475\n",
      "Epoch: 16\n",
      "Train:  2.3391570923148945\n",
      "Test:  0.6931286454200745\n",
      "0.6931101232189796\n",
      "Epoch: 17\n",
      "Train:  2.337532963804019\n",
      "Test:  0.6931308507919312\n",
      "0.6931143447824976\n",
      "Epoch: 18\n",
      "Train:  2.3380504468435883\n",
      "Test:  0.6931300759315491\n",
      "0.6931175370172591\n",
      "Epoch: 19\n",
      "Train:  2.3382159157465865\n",
      "Test:  0.6931298971176147\n",
      "0.6931200378702016\n",
      "Epoch: 20\n",
      "Train:  2.338740667348267\n",
      "Test:  0.6931297183036804\n",
      "0.6931219919803826\n",
      "Epoch: 21\n",
      "Train:  2.3409640917214016\n",
      "Test:  0.6931313872337341\n",
      "0.6931238849990654\n",
      "Epoch: 22\n",
      "Train:  2.3396462650709253\n",
      "Test:  0.6931318640708923\n",
      "0.6931254902893923\n",
      "Epoch: 23\n",
      "Train:  2.3365059353971995\n",
      "Test:  0.6931339502334595\n",
      "0.6931271903063086\n",
      "Epoch: 24\n",
      "Train:  2.3366312942197247\n",
      "Test:  0.6931360363960266\n",
      "0.6931289662335155\n",
      "Epoch: 25\n",
      "Train:  2.3379256789402296\n",
      "Test:  0.6931381821632385\n",
      "0.6931308150070352\n",
      "Epoch: 26\n",
      "Train:  2.3368185553499448\n",
      "Test:  0.6931434273719788\n",
      "0.6931333435937714\n",
      "Epoch: 27\n",
      "Train:  2.338208721530053\n",
      "Test:  0.6931506991386414\n",
      "0.6931368214298588\n",
      "Epoch: 28\n",
      "Train:  2.3368810825450446\n",
      "Test:  0.6931582093238831\n",
      "0.6931411056381549\n",
      "Epoch: 29\n",
      "Train:  2.3371800844387343\n",
      "Test:  0.6931634545326233\n",
      "0.6931455809572252\n",
      "Epoch: 30\n",
      "Train:  2.336614026177314\n",
      "Test:  0.6931672096252441\n",
      "0.6931499109790749\n",
      "Epoch: 31\n",
      "Train:  2.3373996576955243\n",
      "Test:  0.6931671500205994\n",
      "0.693153361521181\n",
      "Epoch: 32\n",
      "Train:  2.3377938007795684\n",
      "Test:  0.6931616067886353\n",
      "0.6931550116205465\n",
      "Epoch: 33\n",
      "Train:  2.3389932846510284\n",
      "Test:  0.6931558847427368\n",
      "0.6931551863335746\n",
      "Epoch: 34\n",
      "Train:  2.33739080608532\n",
      "Test:  0.6931508779525757\n",
      "0.6931543243076955\n",
      "Epoch: 35\n",
      "Train:  2.3386819253685656\n",
      "Test:  0.6931421756744385\n",
      "0.6931518937922967\n",
      "Epoch: 36\n",
      "Train:  2.337674073634609\n",
      "Test:  0.6931319832801819\n",
      "0.6931479106557923\n",
      "Epoch: 37\n",
      "Train:  2.3385254452305455\n",
      "Test:  0.6931156516075134\n",
      "0.6931414575058698\n",
      "Epoch: 38\n",
      "Train:  2.337208413949577\n",
      "Test:  0.69309401512146\n",
      "0.6931319674521821\n",
      "Epoch: 39\n",
      "Train:  2.337959753569736\n",
      "Test:  0.6930636763572693\n",
      "0.6931183074174695\n",
      "Epoch: 40\n",
      "Train:  2.337534560311225\n",
      "Test:  0.6930322647094727\n",
      "0.6931010970457494\n",
      "Epoch: 41\n",
      "Train:  2.3380916650577257\n",
      "Test:  0.692999005317688\n",
      "0.6930806769629886\n",
      "Epoch: 42\n",
      "Train:  2.336405151633806\n",
      "Test:  0.6929612159729004\n",
      "0.6930567831388416\n",
      "Epoch: 43\n",
      "Train:  2.3364417129947292\n",
      "Test:  0.6929273009300232\n",
      "0.6930308852870647\n",
      "Epoch: 44\n",
      "Train:  2.3362134188734074\n",
      "Test:  0.6928877830505371\n",
      "0.6930022635931086\n",
      "Epoch: 45\n",
      "Train:  2.335106172228372\n",
      "Test:  0.6928463578224182\n",
      "0.6929710813524279\n",
      "Epoch: 46\n",
      "Train:  2.3354541704218876\n",
      "Test:  0.6927972435951233\n",
      "0.692936312831762\n",
      "Epoch: 47\n",
      "Train:  2.335424480899688\n",
      "Test:  0.6927438974380493\n",
      "0.692897828894799\n",
      "Epoch: 48\n",
      "Train:  2.33609999059349\n",
      "Test:  0.692681074142456\n",
      "0.6928544771709099\n",
      "Epoch: 49\n",
      "Train:  2.334786085672276\n",
      "Test:  0.6926191449165344\n",
      "0.6928074100482703\n",
      "Epoch: 50\n",
      "Train:  2.334834840989882\n",
      "Test:  0.6925496459007263\n",
      "0.6927558566301256\n",
      "Epoch: 51\n",
      "Train:  2.335190035963571\n",
      "Test:  0.6924808025360107\n",
      "0.692700845308808\n",
      "Epoch: 52\n",
      "Train:  2.3344983432882573\n",
      "Test:  0.6923986673355103\n",
      "0.6926404092725116\n",
      "Epoch: 53\n",
      "Train:  2.3336053106092636\n",
      "Test:  0.6923240423202515\n",
      "0.6925771355121607\n",
      "Epoch: 54\n",
      "Train:  2.334151397469223\n",
      "Test:  0.6922439932823181\n",
      "0.6925105067545825\n",
      "Epoch: 55\n",
      "Train:  2.3325184429845502\n",
      "Test:  0.6921582221984863\n",
      "0.6924400495797517\n",
      "Epoch: 56\n",
      "Train:  2.33207179205392\n",
      "Test:  0.6920708417892456\n",
      "0.6923662078006305\n",
      "Epoch: 57\n",
      "Train:  2.333300439260339\n",
      "Test:  0.6919817924499512\n",
      "0.6922893245463959\n",
      "Epoch: 58\n",
      "Train:  2.333098006504838\n",
      "Test:  0.6918941140174866\n",
      "0.6922102822891991\n",
      "Epoch: 59\n",
      "Train:  2.333025610575112\n",
      "Test:  0.6917935609817505\n",
      "0.6921269378999846\n",
      "Epoch: 60\n",
      "Train:  2.331012470106925\n",
      "Test:  0.6916925311088562\n",
      "0.6920400564352425\n",
      "Epoch: 61\n",
      "Train:  2.330934155371881\n",
      "Test:  0.6915850639343262\n",
      "0.691949057845808\n",
      "Epoch: 62\n",
      "Train:  2.3307801638880083\n",
      "Test:  0.6914758682250977\n",
      "0.6918544198474094\n",
      "Epoch: 63\n",
      "Train:  2.3298349258720235\n",
      "Test:  0.691360592842102\n",
      "0.6917556543843518\n",
      "Epoch: 64\n",
      "Train:  2.331804868354592\n",
      "Test:  0.6912383437156677\n",
      "0.6916521921986596\n",
      "Epoch: 65\n",
      "Train:  2.331480566532381\n",
      "Test:  0.6911163926124573\n",
      "0.6915450322383693\n",
      "Epoch: 66\n",
      "Train:  2.3316409626314716\n",
      "Test:  0.6909874081611633\n",
      "0.6914335073870854\n",
      "Epoch: 67\n",
      "Train:  2.3295066151567685\n",
      "Test:  0.6908473968505859\n",
      "0.6913162852496465\n",
      "Epoch: 68\n",
      "Train:  2.327671803453917\n",
      "Test:  0.6907036304473877\n",
      "0.6911937542639915\n",
      "Epoch: 69\n",
      "Train:  2.3275886703562993\n",
      "Test:  0.6905485391616821\n",
      "0.6910647112222956\n",
      "Epoch: 70\n",
      "Train:  2.327299036646402\n",
      "Test:  0.6903849244117737\n",
      "0.6909287538422939\n",
      "Epoch: 71\n",
      "Train:  2.328429315679817\n",
      "Test:  0.6902139782905579\n",
      "0.6907857987168917\n",
      "Epoch: 72\n",
      "Train:  2.3263327011498074\n",
      "Test:  0.6900345683097839\n",
      "0.6906355526228118\n",
      "Epoch: 73\n",
      "Train:  2.325180019101789\n",
      "Test:  0.6898477077484131\n",
      "0.6904779836373119\n",
      "Epoch: 74\n",
      "Train:  2.3249768960860466\n",
      "Test:  0.6896454691886902\n",
      "0.6903114807386098\n",
      "Epoch: 75\n",
      "Train:  2.3250065234399613\n",
      "Test:  0.6894232034683228\n",
      "0.6901338252768889\n",
      "Epoch: 76\n",
      "Train:  2.322648540619881\n",
      "Test:  0.6891880631446838\n",
      "0.6899446728439206\n",
      "Epoch: 77\n",
      "Train:  2.3217012632277703\n",
      "Test:  0.6889626383781433\n",
      "0.6897482659453429\n",
      "Epoch: 78\n",
      "Train:  2.321878984410276\n",
      "Test:  0.6887547969818115\n",
      "0.6895495721482483\n",
      "Epoch: 79\n",
      "Train:  2.3211589417149945\n",
      "Test:  0.6885470151901245\n",
      "0.6893490607530808\n",
      "Epoch: 80\n",
      "Train:  2.319428026676178\n",
      "Test:  0.6883394122123718\n",
      "0.6891471310420847\n",
      "Epoch: 81\n",
      "Train:  2.3196259256332152\n",
      "Test:  0.6881163120269775\n",
      "0.6889409672367319\n",
      "Epoch: 82\n",
      "Train:  2.318977745630408\n",
      "Test:  0.6878802180290222\n",
      "0.6887288173932709\n",
      "Epoch: 83\n",
      "Train:  2.3179697894280955\n",
      "Test:  0.6876259446144104\n",
      "0.6885082428359024\n",
      "Epoch: 84\n",
      "Train:  2.3165640446447555\n",
      "Test:  0.687360942363739\n",
      "0.6882787827401412\n",
      "Epoch: 85\n",
      "Train:  2.316005356850163\n",
      "Test:  0.6870815753936768\n",
      "0.6880393412697393\n",
      "Epoch: 86\n",
      "Train:  2.3166078450859233\n",
      "Test:  0.686790406703949\n",
      "0.6877895543556557\n",
      "Epoch: 87\n",
      "Train:  2.31405548831468\n",
      "Test:  0.6864930987358093\n",
      "0.6875302632309178\n",
      "Epoch: 88\n",
      "Train:  2.3129023050749176\n",
      "Test:  0.6861820220947266\n",
      "0.6872606150030399\n",
      "Epoch: 89\n",
      "Train:  2.31019296761482\n",
      "Test:  0.6858527660369873\n",
      "0.6869790452092952\n",
      "Epoch: 90\n",
      "Train:  2.3087353405132087\n",
      "Test:  0.6855144500732422\n",
      "0.68668612618164\n",
      "Epoch: 91\n",
      "Train:  2.31141617093035\n",
      "Test:  0.685157299041748\n",
      "0.6863803607532903\n",
      "Epoch: 92\n",
      "Train:  2.3107820935146783\n",
      "Test:  0.6847993731498718\n",
      "0.6860641632322994\n",
      "Epoch: 93\n",
      "Train:  2.309053800439322\n",
      "Test:  0.6844357252120972\n",
      "0.6857384756280058\n",
      "Epoch: 94\n",
      "Train:  2.3070380264712917\n",
      "Test:  0.684058666229248\n",
      "0.6854025137480455\n",
      "Epoch: 95\n",
      "Train:  2.305578681089545\n",
      "Test:  0.683678388595581\n",
      "0.685057688717381\n",
      "Epoch: 96\n",
      "Train:  2.3037967361429685\n",
      "Test:  0.6832878589630127\n",
      "0.6847037227663667\n",
      "Epoch: 97\n",
      "Train:  2.3021076712557065\n",
      "Test:  0.6828890442848206\n",
      "0.6843407870699418\n",
      "Epoch: 98\n",
      "Train:  2.299065443136359\n",
      "Test:  0.682477593421936\n",
      "0.6839681483402458\n",
      "Epoch: 99\n",
      "Train:  2.2995966454987884\n",
      "Test:  0.6820409297943115\n",
      "0.6835827046309805\n",
      "Epoch: 100\n",
      "Train:  2.2996820531865603\n",
      "Test:  0.6815836429595947\n",
      "0.6831828922966381\n",
      "Epoch: 101\n",
      "Train:  2.29448623811045\n",
      "Test:  0.6811004281044006\n",
      "0.6827663994581363\n",
      "Epoch: 102\n",
      "Train:  2.2911917092979595\n",
      "Test:  0.6805981993675232\n",
      "0.6823327594399685\n",
      "Epoch: 103\n",
      "Train:  2.2925979571957744\n",
      "Test:  0.6800808310508728\n",
      "0.6818823737621118\n",
      "Epoch: 104\n",
      "Train:  2.292193797967767\n",
      "Test:  0.6795578598976135\n",
      "0.681417470989181\n",
      "Epoch: 105\n",
      "Train:  2.285094047746351\n",
      "Test:  0.6790090799331665\n",
      "0.6809357927779525\n",
      "Epoch: 106\n",
      "Train:  2.284578646382978\n",
      "Test:  0.6784701347351074\n",
      "0.6804426611693624\n",
      "Epoch: 107\n",
      "Train:  2.2830253551083226\n",
      "Test:  0.6779216527938843\n",
      "0.6799384594942495\n",
      "Epoch: 108\n",
      "Train:  2.2799195288329996\n",
      "Test:  0.6773446798324585\n",
      "0.6794197035618772\n",
      "Epoch: 109\n",
      "Train:  2.2793119184432493\n",
      "Test:  0.6767364144325256\n",
      "0.6788830457359951\n",
      "Epoch: 110\n",
      "Train:  2.2767028090774373\n",
      "Test:  0.6760997772216797\n",
      "0.6783263920331224\n",
      "Epoch: 111\n",
      "Train:  2.2751997151682453\n",
      "Test:  0.6754324436187744\n",
      "0.6777476023502446\n",
      "Epoch: 112\n",
      "Train:  2.2734824130612035\n",
      "Test:  0.6747480630874634\n",
      "0.6771476944976816\n",
      "Epoch: 113\n",
      "Train:  2.2732906540234885\n",
      "Test:  0.6740796566009521\n",
      "0.6765340869183303\n",
      "Epoch: 114\n",
      "Train:  2.268836796924632\n",
      "Test:  0.6734046339988708\n",
      "0.6759081963344339\n",
      "Epoch: 115\n",
      "Train:  2.26510190451017\n",
      "Test:  0.6727259755134583\n",
      "0.675271752170235\n",
      "Epoch: 116\n",
      "Train:  2.2646225011476906\n",
      "Test:  0.672011137008667\n",
      "0.6746196291379185\n",
      "Epoch: 117\n",
      "Train:  2.261345825528586\n",
      "Test:  0.6712396740913391\n",
      "0.6739436381286001\n",
      "Epoch: 118\n",
      "Train:  2.2578689065030826\n",
      "Test:  0.670426607131958\n",
      "0.6732402319292696\n",
      "Epoch: 119\n",
      "Train:  2.2519146582131744\n",
      "Test:  0.6695743799209595\n",
      "0.6725070615276059\n",
      "Epoch: 120\n",
      "Train:  2.2544694940249124\n",
      "Test:  0.6687136292457581\n",
      "0.6717483750712349\n",
      "Epoch: 121\n",
      "Train:  2.245615058047797\n",
      "Test:  0.6677987575531006\n",
      "0.6709584515676067\n",
      "Epoch: 122\n",
      "Train:  2.2457987294402173\n",
      "Test:  0.666823148727417\n",
      "0.6701313909995678\n",
      "Epoch: 123\n",
      "Train:  2.2439087257590344\n",
      "Test:  0.6657870411872864\n",
      "0.6692625210371107\n",
      "Epoch: 124\n",
      "Train:  2.2402315966544615\n",
      "Test:  0.66472327709198\n",
      "0.6683546722480839\n",
      "Epoch: 125\n",
      "Train:  2.234101818453881\n",
      "Test:  0.6636377573013306\n",
      "0.6674112892587326\n",
      "Epoch: 126\n",
      "Train:  2.23380236600035\n",
      "Test:  0.6625438928604126\n",
      "0.666437809979068\n",
      "Epoch: 127\n",
      "Train:  2.2288993077893413\n",
      "Test:  0.6614193320274353\n",
      "0.6654341143887411\n",
      "Epoch: 128\n",
      "Train:  2.221657851049977\n",
      "Test:  0.6602604389190674\n",
      "0.664399379294806\n",
      "Epoch: 129\n",
      "Train:  2.2207461826262938\n",
      "Test:  0.6590557098388672\n",
      "0.663330645403618\n",
      "Epoch: 130\n",
      "Train:  2.2138632965344254\n",
      "Test:  0.6578503847122192\n",
      "0.6622345932653381\n",
      "Epoch: 131\n",
      "Train:  2.210690992493783\n",
      "Test:  0.6565927863121033\n",
      "0.6611062318746909\n",
      "Epoch: 132\n",
      "Train:  2.2039870504410035\n",
      "Test:  0.6552924513816833\n",
      "0.6599434757760893\n",
      "Epoch: 133\n",
      "Train:  2.1999141862315517\n",
      "Test:  0.6539717316627502\n",
      "0.6587491269534214\n",
      "Epoch: 134\n",
      "Train:  2.1925906212099138\n",
      "Test:  0.6525797843933105\n",
      "0.6575152584413991\n",
      "Epoch: 135\n",
      "Train:  2.1944489293200995\n",
      "Test:  0.6511582732200623\n",
      "0.6562438613971316\n",
      "Epoch: 136\n",
      "Train:  2.1841648970880816\n",
      "Test:  0.6496411561965942\n",
      "0.6549233203570242\n",
      "Epoch: 137\n",
      "Train:  2.179665098908127\n",
      "Test:  0.6480077505111694\n",
      "0.6535402063878532\n",
      "Epoch: 138\n",
      "Train:  2.174829234999995\n",
      "Test:  0.6463299989700317\n",
      "0.6520981649042888\n",
      "Epoch: 139\n",
      "Train:  2.1690424385891167\n",
      "Test:  0.6446290016174316\n",
      "0.6506043322469174\n",
      "Epoch: 140\n",
      "Train:  2.1596747873931803\n",
      "Test:  0.6429340243339539\n",
      "0.6490702706643247\n",
      "Epoch: 141\n",
      "Train:  2.153866607014851\n",
      "Test:  0.6411930918693542\n",
      "0.6474948349053306\n",
      "Epoch: 142\n",
      "Train:  2.150954547107861\n",
      "Test:  0.6394361257553101\n",
      "0.6458830930753264\n",
      "Epoch: 143\n",
      "Train:  2.145383092664903\n",
      "Test:  0.637640118598938\n",
      "0.6442344981800486\n",
      "Epoch: 144\n",
      "Train:  2.140965196394151\n",
      "Test:  0.6358402371406555\n",
      "0.64255564597217\n",
      "Epoch: 145\n",
      "Train:  2.1326819113505784\n",
      "Test:  0.6339884400367737\n",
      "0.6408422047850907\n",
      "Epoch: 146\n",
      "Train:  2.127904881713211\n",
      "Test:  0.6321064233779907\n",
      "0.6390950485036707\n",
      "Epoch: 147\n",
      "Train:  2.115873322691969\n",
      "Test:  0.630224883556366\n",
      "0.6373210155142097\n",
      "Epoch: 148\n",
      "Train:  2.107334715704764\n",
      "Test:  0.6282038688659668\n",
      "0.6354975861845612\n",
      "Epoch: 149\n",
      "Train:  2.098992944404643\n",
      "Test:  0.6261243224143982\n",
      "0.6336229334305286\n",
      "Epoch: 150\n",
      "Train:  2.1004290426931074\n",
      "Test:  0.6239665746688843\n",
      "0.6316916616781997\n",
      "Epoch: 151\n",
      "Train:  2.0869861072109592\n",
      "Test:  0.6217017769813538\n",
      "0.6296936847388306\n",
      "Epoch: 152\n",
      "Train:  2.081700191702894\n",
      "Test:  0.6194429993629456\n",
      "0.6276435476636535\n",
      "Epoch: 153\n",
      "Train:  2.072160650965988\n",
      "Test:  0.6171330809593201\n",
      "0.6255414543227868\n",
      "Epoch: 154\n",
      "Train:  2.062782883003194\n",
      "Test:  0.614793062210083\n",
      "0.6233917759002461\n",
      "Epoch: 155\n",
      "Train:  2.058568304584872\n",
      "Test:  0.6125543117523193\n",
      "0.6212242830706607\n",
      "Epoch: 156\n",
      "Train:  2.044151223474933\n",
      "Test:  0.6102849841117859\n",
      "0.6190364232788858\n",
      "Epoch: 157\n",
      "Train:  2.0385472812960224\n",
      "Test:  0.6079843044281006\n",
      "0.6168259995087286\n",
      "Epoch: 158\n",
      "Train:  2.027704510637509\n",
      "Test:  0.605708122253418\n",
      "0.6146024240576665\n",
      "Epoch: 159\n",
      "Train:  2.0201405126561403\n",
      "Test:  0.6033493876457214\n",
      "0.6123518167752775\n",
      "Epoch: 160\n",
      "Train:  2.0134615103403726\n",
      "Test:  0.6008307933807373\n",
      "0.6100476120963694\n",
      "Epoch: 161\n",
      "Train:  2.004992857415189\n",
      "Test:  0.5981584191322327\n",
      "0.607669773503542\n",
      "Epoch: 162\n",
      "Train:  1.994741908324662\n",
      "Test:  0.5954017043113708\n",
      "0.6052161596651078\n",
      "Epoch: 163\n",
      "Train:  1.9778879252813195\n",
      "Test:  0.5925091505050659\n",
      "0.6026747578330994\n",
      "Epoch: 164\n",
      "Train:  1.9722446408323062\n",
      "Test:  0.5895996689796448\n",
      "0.6000597400624085\n",
      "Epoch: 165\n",
      "Train:  1.9641055381426247\n",
      "Test:  0.5866878628730774\n",
      "0.5973853646245423\n",
      "Epoch: 166\n",
      "Train:  1.9533073126628835\n",
      "Test:  0.583760142326355\n",
      "0.5946603201649049\n",
      "Epoch: 167\n",
      "Train:  1.9385183485605384\n",
      "Test:  0.5809311270713806\n",
      "0.5919144815462\n",
      "Epoch: 168\n",
      "Train:  1.9250291964059234\n",
      "Test:  0.578178882598877\n",
      "0.5891673617567353\n",
      "Epoch: 169\n",
      "Train:  1.91853904403666\n",
      "Test:  0.5753856897354126\n",
      "0.5864110273524707\n",
      "Epoch: 170\n",
      "Train:  1.9135973280475986\n",
      "Test:  0.5724706053733826\n",
      "0.5836229429566531\n",
      "Epoch: 171\n",
      "Train:  1.9055131584085443\n",
      "Test:  0.5695526599884033\n",
      "0.5808088863630031\n",
      "Epoch: 172\n",
      "Train:  1.8858004167515745\n",
      "Test:  0.5663822293281555\n",
      "0.5779235549560336\n",
      "Epoch: 173\n",
      "Train:  1.874568748858667\n",
      "Test:  0.5630074143409729\n",
      "0.5749403268330215\n",
      "Epoch: 174\n",
      "Train:  1.863618803280656\n",
      "Test:  0.5594858527183533\n",
      "0.5718494320100879\n",
      "Epoch: 175\n",
      "Train:  1.8466582919961663\n",
      "Test:  0.5558727383613586\n",
      "0.568654093280342\n",
      "Epoch: 176\n",
      "Train:  1.8359424958946884\n",
      "Test:  0.5523477792739868\n",
      "0.565392830479071\n",
      "Epoch: 177\n",
      "Train:  1.8252803209007427\n",
      "Test:  0.5487380623817444\n",
      "0.5620618768596056\n",
      "Epoch: 178\n",
      "Train:  1.8177482146088795\n",
      "Test:  0.5452507734298706\n",
      "0.5586996561736586\n",
      "Epoch: 179\n",
      "Train:  1.79551566640536\n",
      "Test:  0.541875422000885\n",
      "0.5553348093391038\n",
      "Epoch: 180\n",
      "Train:  1.786160151804647\n",
      "Test:  0.5385581254959106\n",
      "0.5519794725704652\n",
      "Epoch: 181\n",
      "Train:  1.778296753603925\n",
      "Test:  0.5351814031600952\n",
      "0.5486198586883911\n",
      "Epoch: 182\n",
      "Train:  1.7472742558807455\n",
      "Test:  0.5317985415458679\n",
      "0.5452555952598864\n",
      "Epoch: 183\n",
      "Train:  1.7463478561370605\n",
      "Test:  0.5283781290054321\n",
      "0.5418801020089955\n",
      "Epoch: 184\n",
      "Train:  1.7381029539210822\n",
      "Test:  0.5247054696083069\n",
      "0.5384451755288578\n",
      "Epoch: 185\n",
      "Train:  1.7234512862338816\n",
      "Test:  0.5210891962051392\n",
      "0.5349739796641141\n",
      "Epoch: 186\n",
      "Train:  1.710902173672953\n",
      "Test:  0.5175645351409912\n",
      "0.5314920907594896\n",
      "Epoch: 187\n",
      "Train:  1.6919648701785712\n",
      "Test:  0.5136898756027222\n",
      "0.527931647728136\n",
      "Epoch: 188\n",
      "Train:  1.6774798612440787\n",
      "Test:  0.5101153254508972\n",
      "0.5243683832726883\n",
      "Epoch: 189\n",
      "Train:  1.6597102723454917\n",
      "Test:  0.5063930749893188\n",
      "0.5207733216160144\n",
      "Epoch: 190\n",
      "Train:  1.6545485213238706\n",
      "Test:  0.5025109052658081\n",
      "0.5171208383459731\n",
      "Epoch: 191\n",
      "Train:  1.6321054030490179\n",
      "Test:  0.4986664056777954\n",
      "0.5134299518123375\n",
      "Epoch: 192\n",
      "Train:  1.6198917143447424\n",
      "Test:  0.4948742687702179\n",
      "0.5097188152039136\n",
      "Epoch: 193\n",
      "Train:  1.6091079183163182\n",
      "Test:  0.49130943417549133\n",
      "0.5060369389982292\n",
      "Epoch: 194\n",
      "Train:  1.59795041238108\n",
      "Test:  0.4878430962562561\n",
      "0.5023981704498346\n",
      "Epoch: 195\n",
      "Train:  1.581219628293027\n",
      "Test:  0.4846341907978058\n",
      "0.4988453745194288\n",
      "Epoch: 196\n",
      "Train:  1.5640912174537618\n",
      "Test:  0.4810246229171753\n",
      "0.4952812241989781\n",
      "Epoch: 197\n",
      "Train:  1.5425466218943238\n",
      "Test:  0.4771389365196228\n",
      "0.49165276666310703\n",
      "Epoch: 198\n",
      "Train:  1.5332168709206324\n",
      "Test:  0.47324326634407043\n",
      "0.4879708665992997\n",
      "Epoch: 199\n",
      "Train:  1.5213407476743062\n",
      "Test:  0.46938687562942505\n",
      "0.4842540684053248\n",
      "Epoch: 200\n",
      "Train:  1.4962691997969022\n",
      "Test:  0.465940922498703\n",
      "0.48059143922400044\n",
      "Epoch: 201\n",
      "Train:  1.484265076537286\n",
      "Test:  0.46269217133522034\n",
      "0.4770115856462444\n",
      "Epoch: 202\n",
      "Train:  1.483910987133621\n",
      "Test:  0.4592658579349518\n",
      "0.4734624401039859\n",
      "Epoch: 203\n",
      "Train:  1.4668416316791246\n",
      "Test:  0.4557175934314728\n",
      "0.4699134707694833\n",
      "Epoch: 204\n",
      "Train:  1.4481117802281533\n",
      "Test:  0.4522937536239624\n",
      "0.46638952734037914\n",
      "Epoch: 205\n",
      "Train:  1.4333750595969539\n",
      "Test:  0.4489316940307617\n",
      "0.4628979606784557\n",
      "Epoch: 206\n",
      "Train:  1.4158464920136236\n",
      "Test:  0.44554245471954346\n",
      "0.4594268594866732\n",
      "Epoch: 207\n",
      "Train:  1.4060862208566358\n",
      "Test:  0.44240763783454895\n",
      "0.4560230151562483\n",
      "Epoch: 208\n",
      "Train:  1.4046546810416765\n",
      "Test:  0.4396163821220398\n",
      "0.4527416885494066\n",
      "Epoch: 209\n",
      "Train:  1.3815338303965907\n",
      "Test:  0.4363860487937927\n",
      "0.44947056059828383\n",
      "Epoch: 210\n",
      "Train:  1.3652834379544823\n",
      "Test:  0.43287983536720276\n",
      "0.44615241555206764\n",
      "Epoch: 211\n",
      "Train:  1.349417083686398\n",
      "Test:  0.4301191568374634\n",
      "0.4429457638091468\n",
      "Epoch: 212\n",
      "Train:  1.334079530610833\n",
      "Test:  0.42728960514068604\n",
      "0.4398145320754547\n",
      "Epoch: 213\n",
      "Train:  1.3343492531648247\n",
      "Test:  0.42446210980415344\n",
      "0.43674404762119445\n",
      "Epoch: 214\n",
      "Train:  1.305894185138005\n",
      "Test:  0.42156094312667847\n",
      "0.4337074267222913\n",
      "Epoch: 215\n",
      "Train:  1.295725127061208\n",
      "Test:  0.41832229495048523\n",
      "0.4306304003679301\n",
      "Epoch: 216\n",
      "Train:  1.2938609414844102\n",
      "Test:  0.4156755208969116\n",
      "0.4276394244737264\n",
      "Epoch: 217\n",
      "Train:  1.2787704313955\n",
      "Test:  0.41315752267837524\n",
      "0.42474304411465613\n",
      "Epoch: 218\n",
      "Train:  1.2637102655185166\n",
      "Test:  0.4109489917755127\n",
      "0.42198423364682747\n",
      "Epoch: 219\n",
      "Train:  1.2452101652981133\n",
      "Test:  0.40850716829299927\n",
      "0.4192888205760618\n",
      "Epoch: 220\n",
      "Train:  1.2458718848484818\n",
      "Test:  0.4060649573802948\n",
      "0.4166440479369084\n",
      "Epoch: 221\n",
      "Train:  1.2297572544825974\n",
      "Test:  0.4037013053894043\n",
      "0.4140554994274076\n",
      "Epoch: 222\n",
      "Train:  1.2183968706797528\n",
      "Test:  0.4013367295265198\n",
      "0.41151174544723\n",
      "Epoch: 223\n",
      "Train:  1.2093815223504139\n",
      "Test:  0.3991306722164154\n",
      "0.4090355308010671\n",
      "Epoch: 224\n",
      "Train:  1.2048461119974814\n",
      "Test:  0.3978106379508972\n",
      "0.4067905522310331\n",
      "Epoch: 225\n",
      "Train:  1.201193586792997\n",
      "Test:  0.3968847692012787\n",
      "0.4048093956250822\n",
      "Epoch: 226\n",
      "Train:  1.186699960981646\n",
      "Test:  0.3953866958618164\n",
      "0.4029248556724291\n",
      "Epoch: 227\n",
      "Train:  1.1729911552962435\n",
      "Test:  0.39316388964653015\n",
      "0.4009726624672493\n",
      "Epoch: 228\n",
      "Train:  1.1643453041712444\n",
      "Test:  0.39109256863594055\n",
      "0.39899664370098753\n",
      "Epoch: 229\n",
      "Train:  1.1531352599461873\n",
      "Test:  0.3888973295688629\n",
      "0.3969767808745626\n",
      "Epoch: 230\n",
      "Train:  1.1459608424094416\n",
      "Test:  0.38714706897735596\n",
      "0.3950108384951213\n",
      "Epoch: 231\n",
      "Train:  1.1248275890786161\n",
      "Test:  0.3862597346305847\n",
      "0.393260617722214\n",
      "Epoch: 232\n",
      "Train:  1.130833464284097\n",
      "Test:  0.38587334752082825\n",
      "0.39178316368193683\n",
      "Epoch: 233\n",
      "Train:  1.1211046063771812\n",
      "Test:  0.38517528772354126\n",
      "0.39046158849025775\n",
      "Epoch: 234\n",
      "Train:  1.1160875312102738\n",
      "Test:  0.3845491111278534\n",
      "0.38927909301777686\n",
      "Epoch: 235\n",
      "Train:  1.1100737692848328\n",
      "Test:  0.38366180658340454\n",
      "0.3881556357309024\n",
      "Epoch: 236\n",
      "Train:  1.1036052380197792\n",
      "Test:  0.38277506828308105\n",
      "0.3870795222413381\n",
      "Epoch: 237\n",
      "Train:  1.099177037836403\n",
      "Test:  0.3809284269809723\n",
      "0.3858493031892649\n",
      "Epoch: 238\n",
      "Train:  1.0841437507701177\n",
      "Test:  0.3799054026603699\n",
      "0.3846605230834859\n",
      "Epoch: 239\n",
      "Train:  1.08696748316288\n",
      "Test:  0.379023939371109\n",
      "0.38353320634101057\n",
      "Epoch: 240\n",
      "Train:  1.0866680404832285\n",
      "Test:  0.3792012333869934\n",
      "0.3826668117502071\n",
      "Epoch: 241\n",
      "Train:  1.0691333783249701\n",
      "Test:  0.3817470073699951\n",
      "0.3824828508741647\n",
      "Epoch: 242\n",
      "Train:  1.0765222998396042\n",
      "Test:  0.38368770480155945\n",
      "0.3827238216596437\n",
      "Epoch: 243\n",
      "Train:  1.0802619742449893\n",
      "Test:  0.3836953938007355\n",
      "0.38291813608786207\n",
      "Epoch: 244\n",
      "Train:  1.0725617573786808\n",
      "Test:  0.3810485899448395\n",
      "0.38254422685925754\n",
      "Epoch: 245\n",
      "Train:  1.0457379574416785\n",
      "Test:  0.3787030577659607\n",
      "0.38177599304059817\n",
      "Epoch: 246\n",
      "Train:  1.054594986861752\n",
      "Test:  0.3774856925010681\n",
      "0.38091793293269216\n",
      "Epoch: 247\n",
      "Train:  1.0516524872472208\n",
      "Test:  0.37740567326545715\n",
      "0.3802154809992452\n",
      "Epoch: 248\n",
      "Train:  1.0434491004674666\n",
      "Test:  0.3775915801525116\n",
      "0.3796907008298985\n",
      "Epoch: 249\n",
      "Train:  1.0352231717558318\n",
      "Test:  0.3787066638469696\n",
      "0.37949389343331275\n",
      "Epoch: 250\n",
      "Train:  1.0424302015253293\n",
      "Test:  0.3805985152721405\n",
      "0.3797148178010783\n",
      "Epoch: 251\n",
      "Train:  1.0420874099257171\n",
      "Test:  0.3821430802345276\n",
      "0.38020047028776816\n",
      "Epoch: 252\n",
      "Train:  1.0334493195818317\n",
      "Test:  0.38198322057724\n",
      "0.3805570203456625\n",
      "Epoch: 253\n",
      "Train:  1.043546280553264\n",
      "Test:  0.3814071714878082\n",
      "0.3807270505740917\n",
      "Epoch: 254\n",
      "Train:  1.0210814283740135\n",
      "Test:  0.3808532655239105\n",
      "0.3807522935640555\n",
      "Epoch: 255\n",
      "Train:  1.0426682866709207\n",
      "Test:  0.38042736053466797\n",
      "0.380687306958178\n",
      "Epoch: 256\n",
      "Train:  1.0332546767688566\n",
      "Test:  0.38062649965286255\n",
      "0.3806751454971149\n",
      "Epoch: 257\n",
      "Train:  1.019133046910327\n",
      "Test:  0.3816036581993103\n",
      "0.380860848037554\n",
      "Epoch: 258\n",
      "Train:  1.0163727280914143\n",
      "Test:  0.38332778215408325\n",
      "0.38135423486085984\n",
      "Epoch: 259\n",
      "Train:  1.0221076562840452\n",
      "Test:  0.38637927174568176\n",
      "0.38235924223782425\n",
      "Epoch: 260\n",
      "Train:  1.0354653700224814\n",
      "Test:  0.38990432024002075\n",
      "0.38386825783826356\n",
      "Epoch: 261\n",
      "Train:  1.0419248551610978\n",
      "Test:  0.38958650827407837\n",
      "0.3850119079254265\n",
      "Epoch: 262\n",
      "Train:  1.0406368410074582\n",
      "Test:  0.3880780041217804\n",
      "0.3856251271646973\n",
      "Epoch: 263\n",
      "Train:  1.0261111619972414\n",
      "Test:  0.3864865303039551\n",
      "0.3857974077925489\n",
      "Epoch: 264\n",
      "Train:  1.0166222545248207\n",
      "Test:  0.3865428864955902\n",
      "0.3859465035331572\n",
      "Epoch: 265\n",
      "Train:  1.0204977162422673\n",
      "Test:  0.3874689042568207\n",
      "0.38625098367788985\n",
      "Epoch: 266\n",
      "Train:  1.009953960455874\n",
      "Test:  0.38844624161720276\n",
      "0.38669003526575246\n",
      "Epoch: 267\n",
      "Train:  1.0200067331553788\n",
      "Test:  0.3900938034057617\n",
      "0.3873707888937543\n",
      "Epoch: 268\n",
      "Train:  1.0124065130147883\n",
      "Test:  0.3935513496398926\n",
      "0.388606901042982\n",
      "Epoch: 269\n",
      "Train:  1.0276570002077727\n",
      "Test:  0.39744552969932556\n",
      "0.3903746267742507\n",
      "Epoch: 270\n",
      "Train:  1.0391450877631865\n",
      "Test:  0.3976692855358124\n",
      "0.3918335585265631\n",
      "Epoch: 271\n",
      "Train:  1.0382840343380486\n",
      "Test:  0.39650923013687134\n",
      "0.3927686928486247\n",
      "Epoch: 272\n",
      "Train:  1.0249469156066577\n",
      "Test:  0.39588412642478943\n",
      "0.3933917795638576\n",
      "Epoch: 273\n",
      "Train:  1.0182604794540713\n",
      "Test:  0.3962424397468567\n",
      "0.39396191160045746\n",
      "Epoch: 274\n",
      "Train:  1.0140353381954215\n",
      "Test:  0.39748910069465637\n",
      "0.39466734941929726\n",
      "Epoch: 275\n",
      "Train:  1.0302907943885813\n",
      "Test:  0.3991212546825409\n",
      "0.395558130471946\n",
      "Epoch: 276\n",
      "Train:  1.0298163074479307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-15 17:45:32,425]\u001b[0m Trial 0 finished with value: 0.37949389343331275 and parameters: {'hidden_size_0': 256, 'hidden_size_1': 256, 'hidden_size_2': 256, 'hidden_size_3': 384, 'hidden_size_4': 384, 'hidden_size_5': 512, 'learning_rate': 1.7023104360630722e-05, 'b1': 0.9217182628689928}. Best is trial 0 with value: 0.37949389343331275.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  0.40157508850097656\n",
      "0.3967615220777521\n",
      "Epoch: 0\n",
      "Train:  0.694062352180481\n",
      "Test:  0.6933193206787109\n",
      "0.6933193206787109\n",
      "Epoch: 1\n",
      "Train:  2.338384054040396\n",
      "Test:  0.6938725113868713\n",
      "0.6936266488499113\n",
      "Epoch: 2\n",
      "Train:  2.3393227586182217\n",
      "Test:  0.6949629783630371\n",
      "0.6941743248798808\n",
      "Epoch: 3\n",
      "Train:  2.342774059182854\n",
      "Test:  0.695854663848877\n",
      "0.6947435453978335\n",
      "Epoch: 4\n",
      "Train:  2.3443005405446535\n",
      "Test:  0.6960716247558594\n",
      "0.6951386189812537\n",
      "Epoch: 5\n",
      "Train:  2.347384657270165\n",
      "Test:  0.6948592662811279\n",
      "0.69506289886781\n",
      "Epoch: 6\n",
      "Train:  2.3397627229331643\n",
      "Test:  0.6932578086853027\n",
      "0.6946060786980415\n",
      "Epoch: 7\n",
      "Train:  2.3324588524397982\n",
      "Test:  0.6920681595802307\n",
      "0.693996169008452\n",
      "Epoch: 8\n",
      "Train:  2.3310415462781022\n",
      "Test:  0.6901361346244812\n",
      "0.6931044819160255\n",
      "Epoch: 9\n",
      "Train:  2.3256977027462375\n",
      "Test:  0.6883214116096497\n",
      "0.692032796510547\n",
      "Epoch: 10\n",
      "Train:  2.319520684339667\n",
      "Test:  0.6858917474746704\n",
      "0.6906891700710588\n",
      "Epoch: 11\n",
      "Train:  2.3050016536507556\n",
      "Test:  0.6818535327911377\n",
      "0.6887916457356514\n",
      "Epoch: 12\n",
      "Train:  2.2955965585606073\n",
      "Test:  0.6756949424743652\n",
      "0.6860199283062683\n",
      "Epoch: 13\n",
      "Train:  2.266411896674864\n",
      "Test:  0.6677391529083252\n",
      "0.6821955764534443\n",
      "Epoch: 14\n",
      "Train:  2.239520372882966\n",
      "Test:  0.6569532752037048\n",
      "0.676963011698168\n",
      "Epoch: 15\n",
      "Train:  2.201562818019621\n",
      "Test:  0.641298234462738\n",
      "0.6696234664173843\n",
      "Epoch: 16\n",
      "Train:  2.1525956341015395\n",
      "Test:  0.6234431862831116\n",
      "0.6601746417748334\n",
      "Epoch: 17\n",
      "Train:  2.0718088765298166\n",
      "Test:  0.5985723733901978\n",
      "0.6476281709729944\n",
      "Epoch: 18\n",
      "Train:  1.9905820828612133\n",
      "Test:  0.5765365958213806\n",
      "0.6332019522197874\n",
      "Epoch: 19\n",
      "Train:  1.905633600809241\n",
      "Test:  0.5417086482048035\n",
      "0.6146898615424189\n",
      "Epoch: 20\n",
      "Train:  1.7737191418806713\n",
      "Test:  0.5158280730247498\n",
      "0.5947334383230075\n",
      "Epoch: 21\n",
      "Train:  1.6490773295843473\n",
      "Test:  0.48033857345581055\n",
      "0.571684393416555\n",
      "Epoch: 22\n",
      "Train:  1.5220102158284956\n",
      "Test:  0.5081925988197327\n",
      "0.5589106315158656\n",
      "Epoch: 23\n",
      "Train:  1.5588074105401193\n",
      "Test:  0.458058625459671\n",
      "0.5386445263287527\n",
      "Epoch: 24\n",
      "Train:  1.4081084978195928\n",
      "Test:  0.45199504494667053\n",
      "0.52124891127676\n",
      "Epoch: 25\n",
      "Train:  1.343490328519575\n",
      "Test:  0.5115674734115601\n",
      "0.5193067538932031\n",
      "Epoch: 26\n",
      "Train:  1.4758696620182326\n",
      "Test:  0.4548446834087372\n",
      "0.5063830922999417\n",
      "Epoch: 27\n",
      "Train:  1.274776269191055\n",
      "Test:  0.4696652591228485\n",
      "0.4990252936120282\n",
      "Epoch: 28\n",
      "Train:  1.2933160230036704\n",
      "Test:  0.616142749786377\n",
      "0.5224850871190175\n",
      "Epoch: 29\n",
      "Train:  1.662238018647317\n",
      "Test:  0.5152526497840881\n",
      "0.5210368067677967\n",
      "Epoch: 30\n",
      "Train:  1.3844580401976903\n",
      "Test:  0.5314455628395081\n",
      "0.5231206216924827\n",
      "Epoch: 31\n",
      "Train:  1.4215678116006236\n",
      "Test:  0.6217517256736755\n",
      "0.5428624836031593\n",
      "Epoch: 32\n",
      "Train:  1.5878767305484383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-15 17:45:59,689]\u001b[0m Trial 1 finished with value: 0.4990252936120282 and parameters: {'hidden_size_0': 512, 'hidden_size_1': 256, 'hidden_size_2': 512, 'hidden_size_3': 384, 'hidden_size_4': 384, 'hidden_size_5': 384, 'learning_rate': 0.00033541101797643905, 'b1': 0.9171546381469242}. Best is trial 0 with value: 0.37949389343331275.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  0.699033796787262\n",
      "0.574116555861709\n",
      "Epoch: 0\n",
      "Train:  0.6916696429252625\n",
      "Test:  0.6937035322189331\n",
      "0.6937035322189331\n",
      "Epoch: 1\n",
      "Train:  2.3396504688006576\n",
      "Test:  0.6937196254730225\n",
      "0.6937124729156495\n",
      "Epoch: 2\n",
      "Train:  2.340093010215349\n",
      "Test:  0.693738579750061\n",
      "0.6937231724379492\n",
      "Epoch: 3\n",
      "Train:  2.3433145624335094\n",
      "Test:  0.6937538385391235\n",
      "0.6937335606836045\n",
      "Epoch: 4\n",
      "Train:  2.3404746600376662\n",
      "Test:  0.6937564015388489\n",
      "0.6937403553216471\n",
      "Epoch: 5\n",
      "Train:  2.341153448627841\n",
      "Test:  0.6937583684921265\n",
      "0.6937452378923599\n",
      "Epoch: 6\n",
      "Train:  2.339565069444718\n",
      "Test:  0.6937609314918518\n",
      "0.6937492095237242\n",
      "Epoch: 7\n",
      "Train:  2.34300039404182\n",
      "Test:  0.6937574148178101\n",
      "0.6937511814101937\n",
      "Epoch: 8\n",
      "Train:  2.3404891189708503\n",
      "Test:  0.6937578320503235\n",
      "0.6937527177410576\n",
      "Epoch: 9\n",
      "Train:  2.3379314022679485\n",
      "Test:  0.6937631368637085\n",
      "0.6937550522293606\n",
      "Epoch: 10\n",
      "Train:  2.3424881472382495\n",
      "Test:  0.6937649250030518\n",
      "0.6937572123358319\n",
      "Epoch: 11\n",
      "Train:  2.338790113567024\n",
      "Test:  0.6937656402587891\n",
      "0.6937590223002297\n",
      "Epoch: 12\n",
      "Train:  2.3385417717759327\n",
      "Test:  0.6937680244445801\n",
      "0.693760927466736\n",
      "Epoch: 13\n",
      "Train:  2.3382219960612636\n",
      "Test:  0.6937700510025024\n",
      "0.6937628361172257\n",
      "Epoch: 14\n",
      "Train:  2.339606599782103\n",
      "Test:  0.6937732100486755\n",
      "0.6937649865656947\n",
      "Epoch: 15\n",
      "Train:  2.340933753598121\n",
      "Test:  0.6937779784202576\n",
      "0.6937676601925118\n",
      "Epoch: 16\n",
      "Train:  2.338642140229543\n",
      "Test:  0.6937806606292725\n",
      "0.6937703201773987\n",
      "Epoch: 17\n",
      "Train:  2.340147868920398\n",
      "Test:  0.6937832832336426\n",
      "0.6937729603497647\n",
      "Epoch: 18\n",
      "Train:  2.340223123309433\n",
      "Test:  0.6937839984893799\n",
      "0.6937752002581697\n",
      "Epoch: 19\n",
      "Train:  2.34082039261377\n",
      "Test:  0.6937870383262634\n",
      "0.6937775954868955\n",
      "Epoch: 20\n",
      "Train:  2.340045961000586\n",
      "Test:  0.6937888264656067\n",
      "0.6937798625930008\n",
      "Epoch: 21\n",
      "Train:  2.3408144609902495\n",
      "Test:  0.6937911510467529\n",
      "0.6937821370664028\n",
      "Epoch: 22\n",
      "Train:  2.342137784086248\n",
      "Test:  0.6937921047210693\n",
      "0.6937841424349426\n",
      "Epoch: 23\n",
      "Train:  2.338628025465114\n",
      "Test:  0.6937956213951111\n",
      "0.6937864491199884\n",
      "Epoch: 24\n",
      "Train:  2.3397921163548707\n",
      "Test:  0.6938028335571289\n",
      "0.6937897384340939\n",
      "Epoch: 25\n",
      "Train:  2.341434315968585\n",
      "Test:  0.693810224533081\n",
      "0.6937938480745174\n",
      "Epoch: 26\n",
      "Train:  2.341590698688261\n",
      "Test:  0.6938135027885437\n",
      "0.693797788544795\n",
      "Epoch: 27\n",
      "Train:  2.3417872568612457\n",
      "Test:  0.6938148140907288\n",
      "0.6938012002531856\n",
      "Epoch: 28\n",
      "Train:  2.3407922931896743\n",
      "Test:  0.6938163638114929\n",
      "0.6938042376650143\n",
      "Epoch: 29\n",
      "Train:  2.3387495099857287\n",
      "Test:  0.6938178539276123\n",
      "0.6938069642929358\n",
      "Epoch: 30\n",
      "Train:  2.342346436233931\n",
      "Test:  0.6938199400901794\n",
      "0.6938095620250537\n",
      "Epoch: 31\n",
      "Train:  2.340137021515959\n",
      "Test:  0.6938216090202332\n",
      "0.6938119733345258\n",
      "Epoch: 32\n",
      "Train:  2.3411231528046312\n",
      "Test:  0.6938281059265137\n",
      "0.6938152018992694\n",
      "Epoch: 33\n",
      "Train:  2.340230753344874\n",
      "Test:  0.6938336491584778\n",
      "0.6938188932228344\n",
      "Epoch: 34\n",
      "Train:  2.3409994148438975\n",
      "Test:  0.6938395500183105\n",
      "0.693823026258488\n",
      "Epoch: 35\n",
      "Train:  2.339911705063235\n",
      "Test:  0.6938466429710388\n",
      "0.6938277511343082\n",
      "Epoch: 36\n",
      "Train:  2.3422736455035467\n",
      "Test:  0.6938562393188477\n",
      "0.6938334502507912\n",
      "Epoch: 37\n",
      "Train:  2.3393000050257613\n",
      "Test:  0.6938667297363281\n",
      "0.6938401075305617\n",
      "Epoch: 38\n",
      "Train:  2.3400165861652744\n",
      "Test:  0.6938768029212952\n",
      "0.6938474478283244\n",
      "Epoch: 39\n",
      "Train:  2.341059822549102\n",
      "Test:  0.693884551525116\n",
      "0.6938548695541993\n",
      "Epoch: 40\n",
      "Train:  2.3412584207391225\n",
      "Test:  0.6938921213150024\n",
      "0.6938623206987015\n",
      "Epoch: 41\n",
      "Train:  2.3411960621033945\n",
      "Test:  0.6939009428024292\n",
      "0.693870045776624\n",
      "Epoch: 42\n",
      "Train:  2.339581592749524\n",
      "Test:  0.6939060688018799\n",
      "0.6938772508720286\n",
      "Epoch: 43\n",
      "Train:  2.3413945494159574\n",
      "Test:  0.6939101815223694\n",
      "0.6938838373606993\n",
      "Epoch: 44\n",
      "Train:  2.3399200477907733\n",
      "Test:  0.6939152479171753\n",
      "0.693890119745631\n",
      "Epoch: 45\n",
      "Train:  2.3420186074831153\n",
      "Test:  0.6939167976379395\n",
      "0.693895455510017\n",
      "Epoch: 46\n",
      "Train:  2.3404487275308177\n",
      "Test:  0.6939156651496887\n",
      "0.693899497550627\n",
      "Epoch: 47\n",
      "Train:  2.33941529322696\n",
      "Test:  0.6939172744750977\n",
      "0.6939030530148106\n",
      "Epoch: 48\n",
      "Train:  2.3408342510141353\n",
      "Test:  0.6939204931259155\n",
      "0.6939065410992611\n",
      "Epoch: 49\n",
      "Train:  2.341782699349106\n",
      "Test:  0.6939181685447693\n",
      "0.6939088666215537\n",
      "Epoch: 50\n",
      "Train:  2.3400389846935066\n",
      "Test:  0.6939142346382141\n",
      "0.6939099402371444\n",
      "Epoch: 51\n",
      "Train:  2.3366968426653134\n",
      "Test:  0.6939070820808411\n",
      "0.6939093686006621\n",
      "Epoch: 52\n",
      "Train:  2.339296847261408\n",
      "Test:  0.6938974857330322\n",
      "0.6939069920097691\n",
      "Epoch: 53\n",
      "Train:  2.3404825868145114\n",
      "Test:  0.6938874125480652\n",
      "0.6939030760945358\n",
      "Epoch: 54\n",
      "Train:  2.3395988524601026\n",
      "Test:  0.6938763856887817\n",
      "0.6938977379884197\n",
      "Epoch: 55\n",
      "Train:  2.3408359692942713\n",
      "Test:  0.6938663125038147\n",
      "0.6938914528679834\n",
      "Epoch: 56\n",
      "Train:  2.3378114745181096\n",
      "Test:  0.6938549876213074\n",
      "0.6938841597968188\n",
      "Epoch: 57\n",
      "Train:  2.339591000669746\n",
      "Test:  0.693844199180603\n",
      "0.6938761676544383\n",
      "Epoch: 58\n",
      "Train:  2.3420734674699846\n",
      "Test:  0.6938358545303345\n",
      "0.6938681050141725\n",
      "Epoch: 59\n",
      "Train:  2.3393894292974986\n",
      "Test:  0.6938281059265137\n",
      "0.6938601051843811\n",
      "Epoch: 60\n",
      "Train:  2.3406082891648814\n",
      "Test:  0.6938180923461914\n",
      "0.6938517026064416\n",
      "Epoch: 61\n",
      "Train:  2.340611255938007\n",
      "Test:  0.6938053965568542\n",
      "0.6938424413874408\n",
      "Epoch: 62\n",
      "Train:  2.339653936124617\n",
      "Test:  0.6937984228134155\n",
      "0.693833637665728\n",
      "Epoch: 63\n",
      "Train:  2.340324128827741\n",
      "Test:  0.6937950253486633\n",
      "0.6938259151974675\n",
      "Epoch: 64\n",
      "Train:  2.3392917494620047\n",
      "Test:  0.6937904357910156\n",
      "0.6938188193126138\n",
      "Epoch: 65\n",
      "Train:  2.3414739959983417\n",
      "Test:  0.6937856078147888\n",
      "0.6938121770103803\n",
      "Epoch: 66\n",
      "Train:  2.340638727270147\n",
      "Test:  0.6937779188156128\n",
      "0.6938053253692247\n",
      "Epoch: 67\n",
      "Train:  2.3390023682707097\n",
      "Test:  0.6937769055366516\n",
      "0.6937996414012487\n",
      "Epoch: 68\n",
      "Train:  2.3389606213056915\n",
      "Test:  0.6937785148620605\n",
      "0.6937954160925419\n",
      "Epoch: 69\n",
      "Train:  2.3376409244793717\n",
      "Test:  0.6937745213508606\n",
      "0.693791237143518\n",
      "Epoch: 70\n",
      "Train:  2.3380264941082207\n",
      "Test:  0.6937679052352905\n",
      "0.6937865707612582\n",
      "Epoch: 71\n",
      "Train:  2.3365000326146363\n",
      "Test:  0.6937550902366638\n",
      "0.6937802746556762\n",
      "Epoch: 72\n",
      "Train:  2.338448275161046\n",
      "Test:  0.6937335729598999\n",
      "0.693770934315734\n",
      "Epoch: 73\n",
      "Train:  2.339578208743885\n",
      "Test:  0.6937134265899658\n",
      "0.6937594327698052\n",
      "Epoch: 74\n",
      "Train:  2.342328950922976\n",
      "Test:  0.6936894655227661\n",
      "0.6937454393196428\n",
      "Epoch: 75\n",
      "Train:  2.3386066235521787\n",
      "Test:  0.6936689019203186\n",
      "0.6937301318391177\n",
      "Epoch: 76\n",
      "Train:  2.33850476882791\n",
      "Test:  0.6936486959457397\n",
      "0.6937138446598801\n",
      "Epoch: 77\n",
      "Train:  2.3391153838044856\n",
      "Test:  0.6936312913894653\n",
      "0.6936973340053413\n",
      "Epoch: 78\n",
      "Train:  2.3371811598859806\n",
      "Test:  0.6936121582984924\n",
      "0.6936802988635954\n",
      "Epoch: 79\n",
      "Train:  2.338823160176636\n",
      "Test:  0.6935899257659912\n",
      "0.6936622242437552\n",
      "Epoch: 80\n",
      "Train:  2.3390861730421744\n",
      "Test:  0.6935654282569885\n",
      "0.6936428650461283\n",
      "Epoch: 81\n",
      "Train:  2.3380479389621365\n",
      "Test:  0.6935402750968933\n",
      "0.6936223470560493\n",
      "Epoch: 82\n",
      "Train:  2.3371541782092025\n",
      "Test:  0.6935105919837952\n",
      "0.6935999960413963\n",
      "Epoch: 83\n",
      "Train:  2.338756445274558\n",
      "Test:  0.6934788227081299\n",
      "0.6935757613745676\n",
      "Epoch: 84\n",
      "Train:  2.335145322225427\n",
      "Test:  0.6934466361999512\n",
      "0.6935499363394947\n",
      "Epoch: 85\n",
      "Train:  2.3361795089578115\n",
      "Test:  0.6934137940406799\n",
      "0.6935227078796057\n",
      "Epoch: 86\n",
      "Train:  2.3348129910807454\n",
      "Test:  0.6933733820915222\n",
      "0.6934928427218783\n",
      "Epoch: 87\n",
      "Train:  2.3380442684696567\n",
      "Test:  0.6933314800262451\n",
      "0.693460570182656\n",
      "Epoch: 88\n",
      "Train:  2.336887546764907\n",
      "Test:  0.6932908296585083\n",
      "0.6934266220777461\n",
      "Epoch: 89\n",
      "Train:  2.3377262635897567\n",
      "Test:  0.6932507157325745\n",
      "0.6933914408086449\n",
      "Epoch: 90\n",
      "Train:  2.33596355876615\n",
      "Test:  0.6932069659233093\n",
      "0.6933545458315218\n",
      "Epoch: 91\n",
      "Train:  2.3368889663809087\n",
      "Test:  0.6931688785552979\n",
      "0.6933174123762319\n",
      "Epoch: 92\n",
      "Train:  2.337592927999394\n",
      "Test:  0.6931318044662476\n",
      "0.693280290794199\n",
      "Epoch: 93\n",
      "Train:  2.3346656791625486\n",
      "Test:  0.6930884718894958\n",
      "0.6932419270132285\n",
      "Epoch: 94\n",
      "Train:  2.334537016448154\n",
      "Test:  0.6930531859397888\n",
      "0.6932041787985171\n",
      "Epoch: 95\n",
      "Train:  2.3377540701179096\n",
      "Test:  0.6930201053619385\n",
      "0.6931673641111831\n",
      "Epoch: 96\n",
      "Train:  2.335052199261163\n",
      "Test:  0.6929852962493896\n",
      "0.6931309505388099\n",
      "Epoch: 97\n",
      "Train:  2.3376543124516806\n",
      "Test:  0.6929508447647095\n",
      "0.6930949293839782\n",
      "Epoch: 98\n",
      "Train:  2.3371986631424195\n",
      "Test:  0.6929199695587158\n",
      "0.6930599374189168\n",
      "Epoch: 99\n",
      "Train:  2.3364017522463234\n",
      "Test:  0.6928890347480774\n",
      "0.6930257568847419\n",
      "Epoch: 100\n",
      "Train:  2.3344885431310183\n",
      "Test:  0.692862331867218\n",
      "0.6929930718812317\n",
      "Epoch: 101\n",
      "Train:  2.3360021671941205\n",
      "Test:  0.692840039730072\n",
      "0.6929624654509958\n",
      "Epoch: 102\n",
      "Train:  2.3362763633010206\n",
      "Test:  0.6928215026855469\n",
      "0.692934272897903\n",
      "Epoch: 103\n",
      "Train:  2.3359045469632713\n",
      "Test:  0.692804217338562\n",
      "0.6929082617860326\n",
      "Epoch: 104\n",
      "Train:  2.3359237909317017\n",
      "Test:  0.6927903294563293\n",
      "0.6928846753200903\n",
      "Epoch: 105\n",
      "Train:  2.3343489951984857\n",
      "Test:  0.6927762031555176\n",
      "0.6928629808871747\n",
      "Epoch: 106\n",
      "Train:  2.3334603380131465\n",
      "Test:  0.6927556395530701\n",
      "0.6928415126203528\n",
      "Epoch: 107\n",
      "Train:  2.336638944123381\n",
      "Test:  0.6927266120910645\n",
      "0.6928185325144943\n",
      "Epoch: 108\n",
      "Train:  2.3349186182022095\n",
      "Test:  0.6927005052566528\n",
      "0.6927949270629254\n",
      "Epoch: 109\n",
      "Train:  2.3334907985502675\n",
      "Test:  0.6926762461662292\n",
      "0.6927711908835856\n",
      "Epoch: 110\n",
      "Train:  2.3348042362479755\n",
      "Test:  0.6926426887512207\n",
      "0.6927454904571121\n",
      "Epoch: 111\n",
      "Train:  2.333841270656996\n",
      "Test:  0.6925994753837585\n",
      "0.692716287442441\n",
      "Epoch: 112\n",
      "Train:  2.3350664531030962\n",
      "Test:  0.6925415992736816\n",
      "0.6926813498086887\n",
      "Epoch: 113\n",
      "Train:  2.3338569896195525\n",
      "Test:  0.6924822330474854\n",
      "0.6926415264564476\n",
      "Epoch: 114\n",
      "Train:  2.335417687252004\n",
      "Test:  0.6924095749855042\n",
      "0.6925951361622585\n",
      "Epoch: 115\n",
      "Train:  2.3318479195717843\n",
      "Test:  0.6923325657844543\n",
      "0.6925426220866974\n",
      "Epoch: 116\n",
      "Train:  2.334452270820577\n",
      "Test:  0.6922544836997986\n",
      "0.6924849944093174\n",
      "Epoch: 117\n",
      "Train:  2.331748782306589\n",
      "Test:  0.6921691298484802\n",
      "0.6924218214971498\n",
      "Epoch: 118\n",
      "Train:  2.331475797519889\n",
      "Test:  0.6920793652534485\n",
      "0.6923533302484093\n",
      "Epoch: 119\n",
      "Train:  2.333383874867552\n",
      "Test:  0.6919968724250793\n",
      "0.6922820386837432\n",
      "Epoch: 120\n",
      "Train:  2.3299857031914497\n",
      "Test:  0.6919267177581787\n",
      "0.69221097449863\n",
      "Epoch: 121\n",
      "Train:  2.330684152982568\n",
      "Test:  0.6918528079986572\n",
      "0.6921393411986353\n",
      "Epoch: 122\n",
      "Train:  2.3330767032920674\n",
      "Test:  0.6917782425880432\n",
      "0.6920671214765168\n",
      "Epoch: 123\n",
      "Train:  2.3324803703574726\n",
      "Test:  0.6917104721069336\n",
      "0.6919957916026\n",
      "Epoch: 124\n",
      "Train:  2.331690078140587\n",
      "Test:  0.6916438341140747\n",
      "0.6919254001048949\n",
      "Epoch: 125\n",
      "Train:  2.3293480475743613\n",
      "Test:  0.6915794014930725\n",
      "0.6918562003825304\n",
      "Epoch: 126\n",
      "Train:  2.331169754587194\n",
      "Test:  0.6915024518966675\n",
      "0.6917854506853578\n",
      "Epoch: 127\n",
      "Train:  2.331180968592244\n",
      "Test:  0.691416323184967\n",
      "0.6917116251852796\n",
      "Epoch: 128\n",
      "Train:  2.3322413461182707\n",
      "Test:  0.6913340091705322\n",
      "0.6916361019823302\n",
      "Epoch: 129\n",
      "Train:  2.3293498216136808\n",
      "Test:  0.6912484765052795\n",
      "0.69155857688692\n",
      "Epoch: 130\n",
      "Train:  2.3303046252137873\n",
      "Test:  0.6911670565605164\n",
      "0.6914802728216392\n",
      "Epoch: 131\n",
      "Train:  2.327784833728626\n",
      "Test:  0.6910921931266785\n",
      "0.691402656882647\n",
      "Epoch: 132\n",
      "Train:  2.3309438984881163\n",
      "Test:  0.6910114288330078\n",
      "0.6913244112727192\n",
      "Epoch: 133\n",
      "Train:  2.330321688805857\n",
      "Test:  0.690923273563385\n",
      "0.6912441837308523\n",
      "Epoch: 134\n",
      "Train:  2.327784699778403\n",
      "Test:  0.6908369660377502\n",
      "0.6911627401922318\n",
      "Epoch: 135\n",
      "Train:  2.3271835305357493\n",
      "Test:  0.6907386183738708\n",
      "0.6910779158285596\n",
      "Epoch: 136\n",
      "Train:  2.3274815556823567\n",
      "Test:  0.6906347274780273\n",
      "0.6909892781584531\n",
      "Epoch: 137\n",
      "Train:  2.3260991470788115\n",
      "Test:  0.6905178427696228\n",
      "0.6908949910806871\n",
      "Epoch: 138\n",
      "Train:  2.3271526585343065\n",
      "Test:  0.6904008984565735\n",
      "0.6907961725558645\n",
      "Epoch: 139\n",
      "Train:  2.3273623246018604\n",
      "Test:  0.6902822256088257\n",
      "0.6906933831664567\n",
      "Epoch: 140\n",
      "Train:  2.327547557892338\n",
      "Test:  0.6901569366455078\n",
      "0.6905860938622669\n",
      "Epoch: 141\n",
      "Train:  2.324276510105338\n",
      "Test:  0.6900221705436707\n",
      "0.6904733091985477\n",
      "Epoch: 142\n",
      "Train:  2.3232802831998436\n",
      "Test:  0.689873218536377\n",
      "0.6903532910661134\n",
      "Epoch: 143\n",
      "Train:  2.323963204378723\n",
      "Test:  0.6897161602973938\n",
      "0.6902258649123695\n",
      "Epoch: 144\n",
      "Train:  2.3223087185172626\n",
      "Test:  0.6895604133605957\n",
      "0.6900927746020147\n",
      "Epoch: 145\n",
      "Train:  2.3265785337776266\n",
      "Test:  0.6894115805625916\n",
      "0.68995653579413\n",
      "Epoch: 146\n",
      "Train:  2.3233441633562886\n",
      "Test:  0.6892597079277039\n",
      "0.6898171702208447\n",
      "Epoch: 147\n",
      "Train:  2.323456641807351\n",
      "Test:  0.6891192197799683\n",
      "0.6896775801326693\n",
      "Epoch: 148\n",
      "Train:  2.3240367712513095\n",
      "Test:  0.6889774203300476\n",
      "0.6895375481721451\n",
      "Epoch: 149\n",
      "Train:  2.3217487085250115\n",
      "Test:  0.6888294219970703\n",
      "0.6893959229371301\n",
      "Epoch: 150\n",
      "Train:  2.3194561671185236\n",
      "Test:  0.6886705756187439\n",
      "0.6892508534734528\n",
      "Epoch: 151\n",
      "Train:  2.322165677624364\n",
      "Test:  0.6885115504264832\n",
      "0.6891029928640591\n",
      "Epoch: 152\n",
      "Train:  2.316698972896863\n",
      "Test:  0.6883540153503418\n",
      "0.6889531973613156\n",
      "Epoch: 153\n",
      "Train:  2.318135166040031\n",
      "Test:  0.6881922483444214\n",
      "0.6888010075579368\n",
      "Epoch: 154\n",
      "Train:  2.3180296421051025\n",
      "Test:  0.688025176525116\n",
      "0.6886458413513725\n",
      "Epoch: 155\n",
      "Train:  2.318882114143782\n",
      "Test:  0.6878654360771179\n",
      "0.6884897602965215\n",
      "Epoch: 156\n",
      "Train:  2.316555291734716\n",
      "Test:  0.6877079010009766\n",
      "0.6883333884374124\n",
      "Epoch: 157\n",
      "Train:  2.3189943330262297\n",
      "Test:  0.687551736831665\n",
      "0.688177058116263\n",
      "Epoch: 158\n",
      "Train:  2.3188871029884583\n",
      "Test:  0.6874101161956787\n",
      "0.6880236697321461\n",
      "Epoch: 159\n",
      "Train:  2.3179715641083254\n",
      "Test:  0.6872730255126953\n",
      "0.6878735408882559\n",
      "Epoch: 160\n",
      "Train:  2.317568422645651\n",
      "Test:  0.6871289014816284\n",
      "0.6877246130069303\n",
      "Epoch: 161\n",
      "Train:  2.3178548229637967\n",
      "Test:  0.6869986653327942\n",
      "0.6875794234721031\n",
      "Epoch: 162\n",
      "Train:  2.316600424628104\n",
      "Test:  0.6868644952774048\n",
      "0.6874364378331634\n",
      "Epoch: 163\n",
      "Train:  2.3125005101644867\n",
      "Test:  0.686726987361908\n",
      "0.6872945477389124\n",
      "Epoch: 164\n",
      "Train:  2.313817282517751\n",
      "Test:  0.6865767240524292\n",
      "0.6871509830016158\n",
      "Epoch: 165\n",
      "Train:  2.3142525297339245\n",
      "Test:  0.6864096522331238\n",
      "0.6870027168479175\n",
      "Epoch: 166\n",
      "Train:  2.313252131785116\n",
      "Test:  0.6862348914146423\n",
      "0.6868491517612625\n",
      "Epoch: 167\n",
      "Train:  2.3109255574082814\n",
      "Test:  0.6860522031784058\n",
      "0.686689762044691\n",
      "Epoch: 168\n",
      "Train:  2.3104905332288435\n",
      "Test:  0.6858695149421692\n",
      "0.6865257126241866\n",
      "Epoch: 169\n",
      "Train:  2.3093185572214026\n",
      "Test:  0.6856558322906494\n",
      "0.6863517365574792\n",
      "Epoch: 170\n",
      "Train:  2.3082897099115516\n",
      "Test:  0.6854246854782104\n",
      "0.6861663263416254\n",
      "Epoch: 171\n",
      "Train:  2.311620330297819\n",
      "Test:  0.6851854920387268\n",
      "0.6859701594810458\n",
      "Epoch: 172\n",
      "Train:  2.3076254321682836\n",
      "Test:  0.6849410533905029\n",
      "0.6857643382629371\n",
      "Epoch: 173\n",
      "Train:  2.305785413711302\n",
      "Test:  0.6847049593925476\n",
      "0.6855524624888593\n",
      "Epoch: 174\n",
      "Train:  2.30725319603438\n",
      "Test:  0.6844637989997864\n",
      "0.6853347297910446\n",
      "Epoch: 175\n",
      "Train:  2.3081621258489546\n",
      "Test:  0.68422931432724\n",
      "0.6851136466982837\n",
      "Epoch: 176\n",
      "Train:  2.303529978439372\n",
      "Test:  0.6839779019355774\n",
      "0.6848864977457424\n",
      "Epoch: 177\n",
      "Train:  2.303640639269224\n",
      "Test:  0.6837324500083923\n",
      "0.6846556881982724\n",
      "Epoch: 178\n",
      "Train:  2.3034737590820558\n",
      "Test:  0.683493435382843\n",
      "0.6844232376351864\n",
      "Epoch: 179\n",
      "Train:  2.3021634618441262\n",
      "Test:  0.6832491755485535\n",
      "0.6841884252178598\n",
      "Epoch: 180\n",
      "Train:  2.300576413190493\n",
      "Test:  0.6829954385757446\n",
      "0.6839498278894367\n",
      "Epoch: 181\n",
      "Train:  2.2970340437786554\n",
      "Test:  0.6827461123466492\n",
      "0.6837090847808791\n",
      "Epoch: 182\n",
      "Train:  2.3011360802958087\n",
      "Test:  0.6824926733970642\n",
      "0.683465802504116\n",
      "Epoch: 183\n",
      "Train:  2.2980564742959957\n",
      "Test:  0.6822320818901062\n",
      "0.6832190583813141\n",
      "Epoch: 184\n",
      "Train:  2.296946494169133\n",
      "Test:  0.6819643974304199\n",
      "0.6829681261911352\n",
      "Epoch: 185\n",
      "Train:  2.2949407286541437\n",
      "Test:  0.681658923625946\n",
      "0.6827062856780973\n",
      "Epoch: 186\n",
      "Train:  2.2964168517820296\n",
      "Test:  0.681342601776123\n",
      "0.6824335488977025\n",
      "Epoch: 187\n",
      "Train:  2.294928031582986\n",
      "Test:  0.6810314059257507\n",
      "0.6821531203033121\n",
      "Epoch: 188\n",
      "Train:  2.291744911542503\n",
      "Test:  0.6807035803794861\n",
      "0.6818632123185469\n",
      "Epoch: 189\n",
      "Train:  2.2898799482212273\n",
      "Test:  0.6803801655769348\n",
      "0.6815666029702244\n",
      "Epoch: 190\n",
      "Train:  2.2878957070330137\n",
      "Test:  0.6800554990768433\n",
      "0.6812643821915482\n",
      "Epoch: 191\n",
      "Train:  2.2860797355251927\n",
      "Test:  0.6797276139259338\n",
      "0.6809570285384252\n",
      "Epoch: 192\n",
      "Train:  2.2883721737451452\n",
      "Test:  0.6794053912162781\n",
      "0.6806467010739957\n",
      "Epoch: 193\n",
      "Train:  2.285859211157727\n",
      "Test:  0.679060161113739\n",
      "0.6803293930819443\n",
      "Epoch: 194\n",
      "Train:  2.2835816708944177\n",
      "Test:  0.6787030696868896\n",
      "0.6800041284029333\n",
      "Epoch: 195\n",
      "Train:  2.287823991108966\n",
      "Test:  0.6783438920974731\n",
      "0.6796720811418413\n",
      "Epoch: 196\n",
      "Train:  2.283042634687116\n",
      "Test:  0.6779929995536804\n",
      "0.6793362648242092\n",
      "Epoch: 197\n",
      "Train:  2.2841806341243047\n",
      "Test:  0.6776298880577087\n",
      "0.6789949894709091\n",
      "Epoch: 198\n",
      "Train:  2.282225501152777\n",
      "Test:  0.6772449016571045\n",
      "0.6786449719081481\n",
      "Epoch: 199\n",
      "Train:  2.2807493440566526\n",
      "Test:  0.6768333315849304\n",
      "0.6782826438435046\n",
      "Epoch: 200\n",
      "Train:  2.2760993082036256\n",
      "Test:  0.6764170527458191\n",
      "0.6779095256239674\n",
      "Epoch: 201\n",
      "Train:  2.273103972916962\n",
      "Test:  0.6759909987449646\n",
      "0.6775258202481669\n",
      "Epoch: 202\n",
      "Train:  2.2786522648667775\n",
      "Test:  0.6755553483963013\n",
      "0.6771317258777938\n",
      "Epoch: 203\n",
      "Train:  2.2786591373464113\n",
      "Test:  0.6751148104667664\n",
      "0.6767283427955884\n",
      "Epoch: 204\n",
      "Train:  2.2751626359519137\n",
      "Test:  0.6746796369552612\n",
      "0.6763186016275229\n",
      "Epoch: 205\n",
      "Train:  2.2705547982646572\n",
      "Test:  0.6742305755615234\n",
      "0.675900996414323\n",
      "Epoch: 206\n",
      "Train:  2.267265135883003\n",
      "Test:  0.6737626194953918\n",
      "0.6754733210305367\n",
      "Epoch: 207\n",
      "Train:  2.2678025505876027\n",
      "Test:  0.6732843518257141\n",
      "0.6750355271895723\n",
      "Epoch: 208\n",
      "Train:  2.26964190313893\n",
      "Test:  0.672797441482544\n",
      "0.6745879100481667\n",
      "Epoch: 209\n",
      "Train:  2.265093034313571\n",
      "Test:  0.6723229885101318\n",
      "0.6741349257405597\n",
      "Epoch: 210\n",
      "Train:  2.264030957093803\n",
      "Test:  0.6718424558639526\n",
      "0.6736764317652383\n",
      "Epoch: 211\n",
      "Train:  2.2600410766499017\n",
      "Test:  0.671360433101654\n",
      "0.6732132320325214\n",
      "Epoch: 212\n",
      "Train:  2.2592251191857042\n",
      "Test:  0.670868456363678\n",
      "0.6727442768987527\n",
      "Epoch: 213\n",
      "Train:  2.254714934415715\n",
      "Test:  0.6703687310218811\n",
      "0.6722691677233784\n",
      "Epoch: 214\n",
      "Train:  2.2574788453758403\n",
      "Test:  0.6698681712150574\n",
      "0.6717889684217142\n",
      "Epoch: 215\n",
      "Train:  2.25456694761912\n",
      "Test:  0.6693429350852966\n",
      "0.6712997617544306\n",
      "Epoch: 216\n",
      "Train:  2.2540185047734167\n",
      "Test:  0.6687960624694824\n",
      "0.670799021897441\n",
      "Epoch: 217\n",
      "Train:  2.250410161351645\n",
      "Test:  0.668241560459137\n",
      "0.6702875296097802\n",
      "Epoch: 218\n",
      "Train:  2.2485338590478383\n",
      "Test:  0.6676961779594421\n",
      "0.6697692592797126\n",
      "Epoch: 219\n",
      "Train:  2.2464124374492194\n",
      "Test:  0.6671373844146729\n",
      "0.6692428843067046\n",
      "Epoch: 220\n",
      "Train:  2.2456621207216734\n",
      "Test:  0.666580080986023\n",
      "0.6687103236425683\n",
      "Epoch: 221\n",
      "Train:  2.2434807804323014\n",
      "Test:  0.6660279035568237\n",
      "0.6681738396254193\n",
      "Epoch: 222\n",
      "Train:  2.240234863373541\n",
      "Test:  0.6654662489891052\n",
      "0.6676323214981564\n",
      "Epoch: 223\n",
      "Train:  2.239830190135587\n",
      "Test:  0.6649109125137329\n",
      "0.6670880397012717\n",
      "Epoch: 224\n",
      "Train:  2.2377237382755486\n",
      "Test:  0.6643560528755188\n",
      "0.6665416423361211\n",
      "Epoch: 225\n",
      "Train:  2.23702783097503\n",
      "Test:  0.66379714012146\n",
      "0.6659927418931889\n",
      "Epoch: 226\n",
      "Train:  2.2347281062474815\n",
      "Test:  0.6631953716278076\n",
      "0.6654332678401127\n",
      "Epoch: 227\n",
      "Train:  2.2305560451681896\n",
      "Test:  0.6625919342041016\n",
      "0.6648650011129105\n",
      "Epoch: 228\n",
      "Train:  2.228546050927972\n",
      "Test:  0.6619485020637512\n",
      "0.6642817013030786\n",
      "Epoch: 229\n",
      "Train:  2.224800338027298\n",
      "Test:  0.6612871885299683\n",
      "0.6636827987484566\n",
      "Epoch: 230\n",
      "Train:  2.223293977399026\n",
      "Test:  0.6605929136276245\n",
      "0.6630648217242902\n",
      "Epoch: 231\n",
      "Train:  2.222068526411569\n",
      "Test:  0.6598998308181763\n",
      "0.6624318235430673\n",
      "Epoch: 232\n",
      "Train:  2.2149271074161736\n",
      "Test:  0.6591991186141968\n",
      "0.6617852825572932\n",
      "Epoch: 233\n",
      "Train:  2.2155121090591594\n",
      "Test:  0.6584933996200562\n",
      "0.6611269059698458\n",
      "Epoch: 234\n",
      "Train:  2.2147707516147244\n",
      "Test:  0.6577712297439575\n",
      "0.6604557707246682\n",
      "Epoch: 235\n",
      "Train:  2.209122807107946\n",
      "Test:  0.6570368409156799\n",
      "0.6597719847628705\n",
      "Epoch: 236\n",
      "Train:  2.2073037367995068\n",
      "Test:  0.6563093066215515\n",
      "0.6590794491346067\n",
      "Epoch: 237\n",
      "Train:  2.199662143825203\n",
      "Test:  0.6555559039115906\n",
      "0.6583747400900035\n",
      "Epoch: 238\n",
      "Train:  2.2000815227467525\n",
      "Test:  0.6548163890838623\n",
      "0.6576630698887752\n",
      "Epoch: 239\n",
      "Train:  2.1995531692299792\n",
      "Test:  0.6540692448616028\n",
      "0.6569443048833407\n",
      "Epoch: 240\n",
      "Train:  2.1994665354810734\n",
      "Test:  0.6533198952674866\n",
      "0.6562194229601698\n",
      "Epoch: 241\n",
      "Train:  2.1920979471616846\n",
      "Test:  0.6525557637214661\n",
      "0.655486691112429\n",
      "Epoch: 242\n",
      "Train:  2.193132878631674\n",
      "Test:  0.6517696976661682\n",
      "0.6547432924231767\n",
      "Epoch: 243\n",
      "Train:  2.1861303326904133\n",
      "Test:  0.650975227355957\n",
      "0.6539896794097328\n",
      "Epoch: 244\n",
      "Train:  2.1858656111583916\n",
      "Test:  0.6501814723014832\n",
      "0.6532280379880829\n",
      "Epoch: 245\n",
      "Train:  2.183823629092145\n",
      "Test:  0.6493651866912842\n",
      "0.6524554677287232\n",
      "Epoch: 246\n",
      "Train:  2.181241792376323\n",
      "Test:  0.6485693454742432\n",
      "0.6516782432778272\n",
      "Epoch: 247\n",
      "Train:  2.1781381817274195\n",
      "Test:  0.6477023363113403\n",
      "0.6508830618845298\n",
      "Epoch: 248\n",
      "Train:  2.1748202116258684\n",
      "Test:  0.6468507051467896\n",
      "0.6500765905369817\n",
      "Epoch: 249\n",
      "Train:  2.171760871846189\n",
      "Test:  0.6459619402885437\n",
      "0.6492536604872942\n",
      "Epoch: 250\n",
      "Train:  2.167905782499621\n",
      "Test:  0.6450451016426086\n",
      "0.6484119487183571\n",
      "Epoch: 251\n",
      "Train:  2.1589629445024716\n",
      "Test:  0.6440659761428833\n",
      "0.6475427542032622\n",
      "Epoch: 252\n",
      "Train:  2.1627100379236284\n",
      "Test:  0.6430684924125671\n",
      "0.6466479018451232\n",
      "Epoch: 253\n",
      "Train:  2.1579273541768393\n",
      "Test:  0.6420570015907288\n",
      "0.6457297217942443\n",
      "Epoch: 254\n",
      "Train:  2.156037078749749\n",
      "Test:  0.6410669088363647\n",
      "0.6447971592026683\n",
      "Epoch: 255\n",
      "Train:  2.145232041035929\n",
      "Test:  0.6400643587112427\n",
      "0.6438505991043832\n",
      "Epoch: 256\n",
      "Train:  2.1467375883492092\n",
      "Test:  0.6390588283538818\n",
      "0.642892244954283\n",
      "Epoch: 257\n",
      "Train:  2.144745963235055\n",
      "Test:  0.6380613446235657\n",
      "0.6419260648881395\n",
      "Epoch: 258\n",
      "Train:  2.1397188408400423\n",
      "Test:  0.6370841860771179\n",
      "0.6409576891259352\n",
      "Epoch: 259\n",
      "Train:  2.13378664178233\n",
      "Test:  0.6361021995544434\n",
      "0.6399865912116368\n",
      "Epoch: 260\n",
      "Train:  2.1312421630787592\n",
      "Test:  0.6351290345191956\n",
      "0.6390150798731487\n",
      "Epoch: 261\n",
      "Train:  2.13060280969066\n",
      "Test:  0.6341757774353027\n",
      "0.6380472193855794\n",
      "Epoch: 262\n",
      "Train:  2.125342647875509\n",
      "Test:  0.6331730484962463\n",
      "0.6370723852077128\n",
      "Epoch: 263\n",
      "Train:  2.1187998915231354\n",
      "Test:  0.6320947408676147\n",
      "0.6360768563396932\n",
      "Epoch: 264\n",
      "Train:  2.115904986858368\n",
      "Test:  0.6309202313423157\n",
      "0.6350455313402178\n",
      "Epoch: 265\n",
      "Train:  2.110162683712539\n",
      "Test:  0.6297013163566589\n",
      "0.6339766883435061\n",
      "Epoch: 266\n",
      "Train:  2.1087176742092257\n",
      "Test:  0.6284056901931763\n",
      "0.6328624887134401\n",
      "Epoch: 267\n",
      "Train:  2.105048317422149\n",
      "Test:  0.6270872950553894\n",
      "0.6317074499818299\n",
      "Epoch: 268\n",
      "Train:  2.0940927241438176\n",
      "Test:  0.625705897808075\n",
      "0.6305071395470789\n",
      "Epoch: 269\n",
      "Train:  2.0947091464073426\n",
      "Test:  0.6243531703948975\n",
      "0.6292763457166426\n",
      "Epoch: 270\n",
      "Train:  2.0874380873095606\n",
      "Test:  0.6229848861694336\n",
      "0.6280180538072009\n",
      "Epoch: 271\n",
      "Train:  2.0808754159558203\n",
      "Test:  0.621634304523468\n",
      "0.6267413039504544\n",
      "Epoch: 272\n",
      "Train:  2.076269611235588\n",
      "Test:  0.6202770471572876\n",
      "0.6254484525918209\n",
      "Epoch: 273\n",
      "Train:  2.071953595325511\n",
      "Test:  0.6189954876899719\n",
      "0.6241578596114511\n",
      "Epoch: 274\n",
      "Train:  2.0678097746705495\n",
      "Test:  0.6178098917007446\n",
      "0.6228882660293098\n",
      "Epoch: 275\n",
      "Train:  2.0619994626250318\n",
      "Test:  0.6166124939918518\n",
      "0.6216331116218182\n",
      "Epoch: 276\n",
      "Train:  2.0576181411743164\n",
      "Test:  0.6154959201812744\n",
      "0.6204056733337094\n",
      "Epoch: 277\n",
      "Train:  2.052090939655099\n",
      "Test:  0.6143644452095032\n",
      "0.6191974277088682\n",
      "Epoch: 278\n",
      "Train:  2.051408636954523\n",
      "Test:  0.61324143409729\n",
      "0.6180062289865526\n",
      "Epoch: 279\n",
      "Train:  2.052117056103163\n",
      "Test:  0.6119523644447327\n",
      "0.6167954560781885\n",
      "Epoch: 280\n",
      "Train:  2.045220846770912\n",
      "Test:  0.6104939579963684\n",
      "0.6155351564618244\n",
      "Epoch: 281\n",
      "Train:  2.034941466264827\n",
      "Test:  0.6088772416114807\n",
      "0.6142035734917557\n",
      "Epoch: 282\n",
      "Train:  2.0373326283629223\n",
      "Test:  0.6072434782981873\n",
      "0.612811554453042\n",
      "Epoch: 283\n",
      "Train:  2.027519880443491\n",
      "Test:  0.6056479215621948\n",
      "0.6113788278748726\n",
      "Epoch: 284\n",
      "Train:  2.0196280851159045\n",
      "Test:  0.6040449142456055\n",
      "0.6099120451490192\n",
      "Epoch: 285\n",
      "Train:  2.014327241528419\n",
      "Test:  0.6024680733680725\n",
      "0.6084232507928298\n",
      "Epoch: 286\n",
      "Train:  2.005874993980572\n",
      "Test:  0.6009644269943237\n",
      "0.6069314860331285\n",
      "Epoch: 287\n",
      "Train:  2.003769338771861\n",
      "Test:  0.5994830131530762\n",
      "0.6054417914571181\n",
      "Epoch: 288\n",
      "Train:  2.000905475308818\n",
      "Test:  0.5980250835418701\n",
      "0.6039584498740684\n",
      "Epoch: 289\n",
      "Train:  1.9940788137015475\n",
      "Test:  0.5966274738311768\n",
      "0.6024922546654901\n",
      "Epoch: 290\n",
      "Train:  1.9840524472216123\n",
      "Test:  0.5951379537582397\n",
      "0.60102139448404\n",
      "Epoch: 291\n",
      "Train:  1.9839471354279468\n",
      "Test:  0.5936423540115356\n",
      "0.5995455863895391\n",
      "Epoch: 292\n",
      "Train:  1.9754498838096537\n",
      "Test:  0.5920361280441284\n",
      "0.598043694720457\n",
      "Epoch: 293\n",
      "Train:  1.9692721770655723\n",
      "Test:  0.5902590751647949\n",
      "0.5964867708093246\n",
      "Epoch: 294\n",
      "Train:  1.9632874938749498\n",
      "Test:  0.5885103940963745\n",
      "0.5948914954667346\n",
      "Epoch: 295\n",
      "Train:  1.9538657171751863\n",
      "Test:  0.5866754055023193\n",
      "0.5932482774738516\n",
      "Epoch: 296\n",
      "Train:  1.9494857813722344\n",
      "Test:  0.5847821235656738\n",
      "0.591555046692216\n",
      "Epoch: 297\n",
      "Train:  1.941945663062475\n",
      "Test:  0.5829183459281921\n",
      "0.5898277065394113\n",
      "Epoch: 298\n",
      "Train:  1.9339640108487939\n",
      "Test:  0.5810558199882507\n",
      "0.5880733292291792\n",
      "Epoch: 299\n",
      "Train:  1.9233102170369958\n",
      "Test:  0.5792701840400696\n",
      "0.5863127001913573\n",
      "Epoch: 300\n",
      "Train:  1.9271384137932972\n",
      "Test:  0.5774570107460022\n",
      "0.5845415623022863\n",
      "Epoch: 301\n",
      "Train:  1.9122546340829583\n",
      "Test:  0.5756285786628723\n",
      "0.5827589655744034\n",
      "Epoch: 302\n",
      "Train:  1.9076273627178644\n",
      "Test:  0.5737484097480774\n",
      "0.5809568544091382\n",
      "Epoch: 303\n",
      "Train:  1.902778083919197\n",
      "Test:  0.5719229578971863\n",
      "0.5791500751067479\n",
      "Epoch: 304\n",
      "Train:  1.8944256901741028\n",
      "Test:  0.5701947808265686\n",
      "0.5773590162507121\n",
      "Epoch: 305\n",
      "Train:  1.885074870560759\n",
      "Test:  0.5684837102890015\n",
      "0.5755839550583699\n",
      "Epoch: 306\n",
      "Train:  1.882038031214027\n",
      "Test:  0.5668694972991943\n",
      "0.5738410635065349\n",
      "Epoch: 307\n",
      "Train:  1.8736905109497808\n",
      "Test:  0.5652697086334229\n",
      "0.5721267925319125\n",
      "Epoch: 308\n",
      "Train:  1.876822313313843\n",
      "Test:  0.5636362433433533\n",
      "0.5704286826942007\n",
      "Epoch: 309\n",
      "Train:  1.8630754729752899\n",
      "Test:  0.5618686079978943\n",
      "0.5687166677549393\n",
      "Epoch: 310\n",
      "Train:  1.8538701399680106\n",
      "Test:  0.5598415732383728\n",
      "0.566941648851626\n",
      "Epoch: 311\n",
      "Train:  1.8521996178934652\n",
      "Test:  0.5576546788215637\n",
      "0.5650842548456135\n",
      "Epoch: 312\n",
      "Train:  1.8437926769256592\n",
      "Test:  0.5554248094558716\n",
      "0.5631523657676651\n",
      "Epoch: 313\n",
      "Train:  1.8305765146850257\n",
      "Test:  0.5532065629959106\n",
      "0.5611632052133142\n",
      "Epoch: 314\n",
      "Train:  1.8284227591688915\n",
      "Test:  0.5509347915649414\n",
      "0.5591175224836397\n",
      "Epoch: 315\n",
      "Train:  1.8245361799834876\n",
      "Test:  0.5487736463546753\n",
      "0.5570487472578468\n",
      "Epoch: 316\n",
      "Train:  1.8089680735782911\n",
      "Test:  0.5466488599777222\n",
      "0.5549687698018219\n",
      "Epoch: 317\n",
      "Train:  1.8115040179221862\n",
      "Test:  0.5445852279663086\n",
      "0.5528920614347191\n",
      "Epoch: 318\n",
      "Train:  1.7937268262268395\n",
      "Test:  0.542608916759491\n",
      "0.5508354324996735\n",
      "Epoch: 319\n",
      "Train:  1.7847269754255972\n",
      "Test:  0.540668249130249\n",
      "0.5488019958257886\n",
      "Epoch: 320\n",
      "Train:  1.7795511234191157\n",
      "Test:  0.5386962890625\n",
      "0.5467808544731309\n",
      "Epoch: 321\n",
      "Train:  1.7619419761242405\n",
      "Test:  0.5367879867553711\n",
      "0.5447822809295789\n",
      "Epoch: 322\n",
      "Train:  1.7641007054236628\n",
      "Test:  0.5346095561981201\n",
      "0.5427477359832872\n",
      "Epoch: 323\n",
      "Train:  1.753354517041996\n",
      "Test:  0.5323805212974548\n",
      "0.5406742930461207\n",
      "Epoch: 324\n",
      "Train:  1.7511559615853012\n",
      "Test:  0.5301579236984253\n",
      "0.5385710191765816\n",
      "Epoch: 325\n",
      "Train:  1.7384930743325142\n",
      "Test:  0.5279138088226318\n",
      "0.5364395771057916\n",
      "Epoch: 326\n",
      "Train:  1.7287588994349203\n",
      "Test:  0.5257151126861572\n",
      "0.5342946842218648\n",
      "Epoch: 327\n",
      "Train:  1.7107004157958492\n",
      "Test:  0.5236448049545288\n",
      "0.5321647083683976\n",
      "Epoch: 328\n",
      "Train:  1.703206536269957\n",
      "Test:  0.5216163992881775\n",
      "0.5300550465523536\n",
      "Epoch: 329\n",
      "Train:  1.710960244299263\n",
      "Test:  0.5194758176803589\n",
      "0.5279392007779546\n",
      "Epoch: 330\n",
      "Train:  1.7024632657086978\n",
      "Test:  0.5173619389533997\n",
      "0.5258237484130437\n",
      "Epoch: 331\n",
      "Train:  1.6881817518382944\n",
      "Test:  0.5151340365409851\n",
      "0.523685806038632\n",
      "Epoch: 332\n",
      "Train:  1.6858431642414422\n",
      "Test:  0.5128602385520935\n",
      "0.5215206925413244\n",
      "Epoch: 333\n",
      "Train:  1.667291083002603\n",
      "Test:  0.5105926990509033\n",
      "0.5193350938432402\n",
      "Epoch: 334\n",
      "Train:  1.6667337641921094\n",
      "Test:  0.5083657503128052\n",
      "0.5171412251371532\n",
      "Epoch: 335\n",
      "Train:  1.6506660526157708\n",
      "Test:  0.5060602426528931\n",
      "0.5149250286403012\n",
      "Epoch: 336\n",
      "Train:  1.6483274948212407\n",
      "Test:  0.5037816762924194\n",
      "0.5126963581707248\n",
      "Epoch: 337\n",
      "Train:  1.638842793562079\n",
      "Test:  0.5016512870788574\n",
      "0.5104873439523513\n",
      "Epoch: 338\n",
      "Train:  1.62525109930705\n",
      "Test:  0.49976739287376404\n",
      "0.5083433537366339\n",
      "Epoch: 339\n",
      "Train:  1.6215168135140532\n",
      "Test:  0.49801889061927795\n",
      "0.5062784611131627\n",
      "Epoch: 340\n",
      "Train:  1.6199449464198081\n",
      "Test:  0.49633166193962097\n",
      "0.5042891012784543\n",
      "Epoch: 341\n",
      "Train:  1.6096902252525411\n",
      "Test:  0.49412867426872253\n",
      "0.502257015876508\n",
      "Epoch: 342\n",
      "Train:  1.5906959727246275\n",
      "Test:  0.49176812171936035\n",
      "0.5001592370450785\n",
      "Epoch: 343\n",
      "Train:  1.5875575769973058\n",
      "Test:  0.48920488357543945\n",
      "0.4979683663511507\n",
      "Epoch: 344\n",
      "Train:  1.574737374821017\n",
      "Test:  0.48648738861083984\n",
      "0.49567217080308856\n",
      "Epoch: 345\n",
      "Train:  1.5665449977561992\n",
      "Test:  0.48399049043655396\n",
      "0.49333583472978165\n",
      "Epoch: 346\n",
      "Train:  1.5596009117941703\n",
      "Test:  0.48153749108314514\n",
      "0.49097616600045435\n",
      "Epoch: 347\n",
      "Train:  1.557342864172433\n",
      "Test:  0.4792768657207489\n",
      "0.4886363059445133\n",
      "Epoch: 348\n",
      "Train:  1.5441921064930577\n",
      "Test:  0.47710418701171875\n",
      "0.48632988215795436\n",
      "Epoch: 349\n",
      "Train:  1.534727317671622\n",
      "Test:  0.47505566477775574\n",
      "0.48407503868191465\n",
      "Epoch: 350\n",
      "Train:  1.532800515172302\n",
      "Test:  0.47329315543174744\n",
      "0.4819186620318812\n",
      "Epoch: 351\n",
      "Train:  1.525783767302831\n",
      "Test:  0.4717487096786499\n",
      "0.47988467156123493\n",
      "Epoch: 352\n",
      "Train:  1.5209460373847716\n",
      "Test:  0.47037947177886963\n",
      "0.47798363160476187\n",
      "Epoch: 353\n",
      "Train:  1.509865600896138\n",
      "Test:  0.4688548147678375\n",
      "0.47615786823737705\n",
      "Epoch: 354\n",
      "Train:  1.4992136432919452\n",
      "Test:  0.46722862124443054\n",
      "0.47437201883878777\n",
      "Epoch: 355\n",
      "Train:  1.4854512708161467\n",
      "Test:  0.46526166796684265\n",
      "0.47254994866439876\n",
      "Epoch: 356\n",
      "Train:  1.4662092014025616\n",
      "Test:  0.46318334341049194\n",
      "0.4706766276136174\n",
      "Epoch: 357\n",
      "Train:  1.4813149225327276\n",
      "Test:  0.4607657194137573\n",
      "0.4686944459736454\n",
      "Epoch: 358\n",
      "Train:  1.4611791388321949\n",
      "Test:  0.45832470059394836\n",
      "0.466620496897706\n",
      "Epoch: 359\n",
      "Train:  1.475104982814481\n",
      "Test:  0.455798476934433\n",
      "0.4644560929050514\n",
      "Epoch: 360\n",
      "Train:  1.4504404561493986\n",
      "Test:  0.45347118377685547\n",
      "0.46225911107941225\n",
      "Epoch: 361\n",
      "Train:  1.4472419789401434\n",
      "Test:  0.4512460231781006\n",
      "0.4600564934991499\n",
      "Epoch: 362\n",
      "Train:  1.4154519502834608\n",
      "Test:  0.44931352138519287\n",
      "0.45790789907635854\n",
      "Epoch: 363\n",
      "Train:  1.4163693356257614\n",
      "Test:  0.447581022977829\n",
      "0.4558425238566526\n",
      "Epoch: 364\n",
      "Train:  1.4086183472987144\n",
      "Test:  0.4459814727306366\n",
      "0.4538703136314494\n",
      "Epoch: 365\n",
      "Train:  1.400019121426408\n",
      "Test:  0.4445131719112396\n",
      "0.4519988852874075\n",
      "Epoch: 366\n",
      "Train:  1.4099990334562076\n",
      "Test:  0.4431253969669342\n",
      "0.45022418762331284\n",
      "Epoch: 367\n",
      "Train:  1.3948902442891111\n",
      "Test:  0.4414840638637543\n",
      "0.44847616287140113\n",
      "Epoch: 368\n",
      "Train:  1.3765735052606112\n",
      "Test:  0.4395691156387329\n",
      "0.44669475342486753\n",
      "Epoch: 369\n",
      "Train:  1.3751335765725823\n",
      "Test:  0.43732088804244995\n",
      "0.444819980348384\n",
      "Epoch: 370\n",
      "Train:  1.3637151551503006\n",
      "Test:  0.4353112280368805\n",
      "0.4429182298860833\n",
      "Epoch: 371\n",
      "Train:  1.3470623621376612\n",
      "Test:  0.43334007263183594\n",
      "0.4410025984352339\n",
      "Epoch: 372\n",
      "Train:  1.353412498709976\n",
      "Test:  0.4315992593765259\n",
      "0.4391219306234923\n",
      "Epoch: 373\n",
      "Train:  1.3478655113327889\n",
      "Test:  0.4299188256263733\n",
      "0.4372813096240685\n",
      "Epoch: 374\n",
      "Train:  1.3403675886251594\n",
      "Test:  0.4284093379974365\n",
      "0.4355069152987421\n",
      "Epoch: 375\n",
      "Train:  1.3396393144002525\n",
      "Test:  0.42692744731903076\n",
      "0.4337910217027998\n",
      "Epoch: 376\n",
      "Train:  1.3205384000014233\n",
      "Test:  0.4256814420223236\n",
      "0.4321691057667046\n",
      "Epoch: 377\n",
      "Train:  1.3083190732104804\n",
      "Test:  0.424154669046402\n",
      "0.43056621842264403\n",
      "Epoch: 378\n",
      "Train:  1.3086279114728332\n",
      "Test:  0.422565221786499\n",
      "0.42896601909541504\n",
      "Epoch: 379\n",
      "Train:  1.308619341542644\n",
      "Test:  0.42083263397216797\n",
      "0.4273393420707656\n",
      "Epoch: 380\n",
      "Train:  1.3031875660983465\n",
      "Test:  0.4189387261867523\n",
      "0.42565921889396297\n",
      "Epoch: 381\n",
      "Train:  1.2996357166638939\n",
      "Test:  0.4172743856906891\n",
      "0.4239822522533082\n",
      "Epoch: 382\n",
      "Train:  1.290921865932403\n",
      "Test:  0.4157026410102844\n",
      "0.42232633000470343\n",
      "Epoch: 383\n",
      "Train:  1.272210984140314\n",
      "Test:  0.41481325030326843\n",
      "0.42082371406441643\n",
      "Epoch: 384\n",
      "Train:  1.2708097911009224\n",
      "Test:  0.4142569601535797\n",
      "0.4195103632822491\n",
      "Epoch: 385\n",
      "Train:  1.256897267154468\n",
      "Test:  0.41382896900177\n",
      "0.4183740844261533\n",
      "Epoch: 386\n",
      "Train:  1.2610420645565115\n",
      "Test:  0.41275179386138916\n",
      "0.4172496263132005\n",
      "Epoch: 387\n",
      "Train:  1.2579976804153894\n",
      "Test:  0.4115011394023895\n",
      "0.4160999289310383\n",
      "Epoch: 388\n",
      "Train:  1.2462137444044954\n",
      "Test:  0.4099472463130951\n",
      "0.4148693924074497\n",
      "Epoch: 389\n",
      "Train:  1.2411648262572545\n",
      "Test:  0.4080963730812073\n",
      "0.41351478854220125\n",
      "Epoch: 390\n",
      "Train:  1.2334932957285194\n",
      "Test:  0.40620294213294983\n",
      "0.41205241926035097\n",
      "Epoch: 391\n",
      "Train:  1.232062925254145\n",
      "Test:  0.40484490990638733\n",
      "0.41061091738955824\n",
      "Epoch: 392\n",
      "Train:  1.2246675311878163\n",
      "Test:  0.4035361707210541\n",
      "0.40919596805585745\n",
      "Epoch: 393\n",
      "Train:  1.2168459184067224\n",
      "Test:  0.40230756998062134\n",
      "0.40781828844081025\n",
      "Epoch: 394\n",
      "Train:  1.2109053384873174\n",
      "Test:  0.4009765088558197\n",
      "0.4064499325238121\n",
      "Epoch: 395\n",
      "Train:  1.2087067420123725\n",
      "Test:  0.39995619654655457\n",
      "0.40515118532836064\n",
      "Epoch: 396\n",
      "Train:  1.1891812880833943\n",
      "Test:  0.39938369393348694\n",
      "0.40399768704938593\n",
      "Epoch: 397\n",
      "Train:  1.1991087525121626\n",
      "Test:  0.39880847930908203\n",
      "0.40295984550132513\n",
      "Epoch: 398\n",
      "Train:  1.1872644805780022\n",
      "Test:  0.39806145429611206\n",
      "0.4019801672602825\n",
      "Epoch: 399\n",
      "Train:  1.1871412199351095\n",
      "Test:  0.3973858952522278\n",
      "0.4010613128586716\n",
      "Epoch: 400\n",
      "Train:  1.1706057450463694\n",
      "Test:  0.3966051936149597\n",
      "0.40017008900992923\n",
      "Epoch: 401\n",
      "Train:  1.1771980423119761\n",
      "Test:  0.39589452743530273\n",
      "0.39931497669500393\n",
      "Epoch: 402\n",
      "Train:  1.1626825439994053\n",
      "Test:  0.3954627811908722\n",
      "0.3985445375941776\n",
      "Epoch: 403\n",
      "Train:  1.1617750902009267\n",
      "Test:  0.3951302468776703\n",
      "0.39786167945087614\n",
      "Epoch: 404\n",
      "Train:  1.1558363442459414\n",
      "Test:  0.39424556493759155\n",
      "0.3971384565482192\n",
      "Epoch: 405\n",
      "Train:  1.1654516381602134\n",
      "Test:  0.3933614194393158\n",
      "0.3963830491264385\n",
      "Epoch: 406\n",
      "Train:  1.1641909850220526\n",
      "Test:  0.39248892664909363\n",
      "0.39560422463096956\n",
      "Epoch: 407\n",
      "Train:  1.1525124279401635\n",
      "Test:  0.39155787229537964\n",
      "0.3947949541638516\n",
      "Epoch: 408\n",
      "Train:  1.1509742297792946\n",
      "Test:  0.39096465706825256\n",
      "0.3940288947447318\n",
      "Epoch: 409\n",
      "Train:  1.1466683863952596\n",
      "Test:  0.3902107775211334\n",
      "0.3932652713000121\n",
      "Epoch: 410\n",
      "Train:  1.129098978574558\n",
      "Test:  0.38952764868736267\n",
      "0.3925177467774822\n",
      "Epoch: 411\n",
      "Train:  1.1400705676886342\n",
      "Test:  0.38914716243743896\n",
      "0.3918436299094735\n",
      "Epoch: 412\n",
      "Train:  1.1358718590069843\n",
      "Test:  0.38854914903640747\n",
      "0.3911847337348603\n",
      "Epoch: 413\n",
      "Train:  1.13207312921683\n",
      "Test:  0.3885394036769867\n",
      "0.3906556677232856\n",
      "Epoch: 414\n",
      "Train:  1.127040781641519\n",
      "Test:  0.3881133496761322\n",
      "0.3901472041138549\n",
      "Epoch: 415\n",
      "Train:  1.1041657493640018\n",
      "Test:  0.38731780648231506\n",
      "0.3895813245875469\n",
      "Epoch: 416\n",
      "Train:  1.1174675948837751\n",
      "Test:  0.3864400088787079\n",
      "0.3889530614457791\n",
      "Epoch: 417\n",
      "Train:  1.1186754580146523\n",
      "Test:  0.38566869497299194\n",
      "0.3882961881512217\n",
      "Epoch: 418\n",
      "Train:  1.0998789049604887\n",
      "Test:  0.38501113653182983\n",
      "0.38763917782734336\n",
      "Epoch: 419\n",
      "Train:  1.10784667041353\n",
      "Test:  0.3845840096473694\n",
      "0.38702814419134857\n",
      "Epoch: 420\n",
      "Train:  1.11316420153905\n",
      "Test:  0.3841787576675415\n",
      "0.3864582668865872\n",
      "Epoch: 421\n",
      "Train:  1.0929609679727144\n",
      "Test:  0.3837806284427643\n",
      "0.3859227391978226\n",
      "Epoch: 422\n",
      "Train:  1.087452706630512\n",
      "Test:  0.38362839818000793\n",
      "0.38546387099425966\n",
      "Epoch: 423\n",
      "Train:  1.0999852428513188\n",
      "Test:  0.3836597800254822\n",
      "0.38510305280050416\n",
      "Epoch: 424\n",
      "Train:  1.090009549933095\n",
      "Test:  0.3840126395225525\n",
      "0.38488497014491385\n",
      "Epoch: 425\n",
      "Train:  1.0817336957621317\n",
      "Test:  0.38453978300094604\n",
      "0.3848159327161203\n",
      "Epoch: 426\n",
      "Train:  1.0940364304409231\n",
      "Test:  0.38490748405456543\n",
      "0.3848342429838093\n",
      "Epoch: 427\n",
      "Train:  1.084943602802933\n",
      "Test:  0.38518068194389343\n",
      "0.3849035307758261\n",
      "Epoch: 428\n",
      "Train:  1.0795267757869536\n",
      "Test:  0.3843258023262024\n",
      "0.38478798508590134\n",
      "Epoch: 429\n",
      "Train:  1.0659338194195942\n",
      "Test:  0.3832848072052002\n",
      "0.3844873495097611\n",
      "Epoch: 430\n",
      "Train:  1.075505768099139\n",
      "Test:  0.3826700747013092\n",
      "0.3841238945480707\n",
      "Epoch: 431\n",
      "Train:  1.067126134390472\n",
      "Test:  0.3818638324737549\n",
      "0.3836718821332075\n",
      "Epoch: 432\n",
      "Train:  1.0722538702270037\n",
      "Test:  0.3815695345401764\n",
      "0.3832514126146013\n",
      "Epoch: 433\n",
      "Train:  1.0616391801065015\n",
      "Test:  0.38168013095855713\n",
      "0.3829371562833925\n",
      "Epoch: 434\n",
      "Train:  1.0687042412257963\n",
      "Test:  0.38229501247406006\n",
      "0.38280872752152595\n",
      "Epoch: 435\n",
      "Train:  1.0681195861549788\n",
      "Test:  0.38358214497566223\n",
      "0.3829634110123532\n",
      "Epoch: 436\n",
      "Train:  1.0614673181246685\n",
      "Test:  0.3851785659790039\n",
      "0.38340644200568336\n",
      "Epoch: 437\n",
      "Train:  1.0763641020623587\n",
      "Test:  0.3866404592990875\n",
      "0.3840532454643642\n",
      "Epoch: 438\n",
      "Train:  1.0735840164525534\n",
      "Test:  0.38763779401779175\n",
      "0.3847701551750497\n",
      "Epoch: 439\n",
      "Train:  1.055275700425589\n",
      "Test:  0.38765373826026917\n",
      "0.3853468717920936\n",
      "Epoch: 440\n",
      "Train:  1.0746857248326784\n",
      "Test:  0.3863513767719269\n",
      "0.38554777278806024\n",
      "Epoch: 441\n",
      "Train:  1.070986131987264\n",
      "Test:  0.3848055601119995\n",
      "0.3853993302528481\n",
      "Epoch: 442\n",
      "Train:  1.0575213357005069\n",
      "Test:  0.38355931639671326\n",
      "0.38503132748162117\n",
      "Epoch: 443\n",
      "Train:  1.0503411789735158\n",
      "Test:  0.38275501132011414\n",
      "0.3845760642493198\n",
      "Epoch: 444\n",
      "Train:  1.0551761952779626\n",
      "Test:  0.3827133774757385\n",
      "0.3842035268946036\n",
      "Epoch: 445\n",
      "Train:  1.059690002792625\n",
      "Test:  0.3829241096973419\n",
      "0.38394764345515126\n",
      "Epoch: 446\n",
      "Train:  1.0473288079743743\n",
      "Test:  0.38323038816452026\n",
      "0.38380419239702507\n",
      "Epoch: 447\n",
      "Train:  1.0432310389575137\n",
      "Test:  0.3839409351348877\n",
      "0.3838315409445976\n",
      "Epoch: 448\n",
      "Train:  1.0309250728097012\n",
      "Test:  0.3856487572193146\n",
      "0.384194984199541\n",
      "Epoch: 449\n",
      "Train:  1.0481833860438357\n",
      "Test:  0.38788512349128723\n",
      "0.38493301205789027\n",
      "Epoch: 450\n",
      "Train:  1.0434783402309622\n",
      "Test:  0.3895028829574585\n",
      "0.38584698623780395\n",
      "Epoch: 451\n",
      "Train:  1.063507410787767\n",
      "Test:  0.39009755849838257\n",
      "0.3866971006899197\n",
      "Epoch: 452\n",
      "Train:  1.0672083695729573\n",
      "Test:  0.38960346579551697\n",
      "0.3872783737110392\n",
      "Epoch: 453\n",
      "Train:  1.0422212409396325\n",
      "Test:  0.3885967433452606\n",
      "0.38754204763788347\n",
      "Epoch: 454\n",
      "Train:  1.0417663560759636\n",
      "Test:  0.38737544417381287\n",
      "0.3875087269450694\n",
      "Epoch: 455\n",
      "Train:  1.0438885994015201\n",
      "Test:  0.3864908814430237\n",
      "0.38730515784466024\n",
      "Epoch: 456\n",
      "Train:  1.0326703862958058\n",
      "Test:  0.38641756772994995\n",
      "0.38712763982171816\n",
      "Epoch: 457\n",
      "Train:  1.03136098232641\n",
      "Test:  0.38684675097465515\n",
      "0.3870714620523056\n",
      "Epoch: 458\n",
      "Train:  1.0389538386496164\n",
      "Test:  0.3872682750225067\n",
      "0.38711082464634583\n",
      "Epoch: 459\n",
      "Train:  1.043160891260511\n",
      "Test:  0.3879595100879669\n",
      "0.38728056173467007\n",
      "Epoch: 460\n",
      "Train:  1.0294526998874962\n",
      "Test:  0.38942211866378784\n",
      "0.3877088731204936\n",
      "Epoch: 461\n",
      "Train:  1.0428344891757093\n",
      "Test:  0.3913552165031433\n",
      "0.3884381417970235\n",
      "Epoch: 462\n",
      "Train:  1.0386669006078475\n",
      "Test:  0.3933049142360687\n",
      "0.38941149628483257\n",
      "Epoch: 463\n",
      "Train:  1.038509851501834\n",
      "Test:  0.395010769367218\n",
      "0.39053135090130964\n",
      "Epoch: 464\n",
      "Train:  1.0474476070013097\n",
      "Test:  0.3956815302371979\n",
      "0.3915613867684873\n",
      "Epoch: 465\n",
      "Train:  1.0492365100691396\n",
      "Test:  0.39442333579063416\n",
      "0.39213377657291665\n",
      "Epoch: 466\n",
      "Train:  1.0444712827923477\n",
      "Test:  0.3931206166744232\n",
      "0.39233114459321794\n",
      "Epoch: 467\n",
      "Train:  1.0411610195553431\n",
      "Test:  0.3920549750328064\n",
      "0.39227591068113565\n",
      "Epoch: 468\n",
      "Train:  1.034288456923859\n",
      "Test:  0.3921355903148651\n",
      "0.3922478466078816\n",
      "Epoch: 469\n",
      "Train:  1.0333828540739192\n",
      "Test:  0.39261630177497864\n",
      "0.39232153764130095\n",
      "Epoch: 470\n",
      "Train:  1.0304569866388076\n",
      "Test:  0.39329493045806885\n",
      "0.39251621620465454\n",
      "Epoch: 471\n",
      "Train:  1.0300464814388623\n",
      "Test:  0.3943367898464203\n",
      "0.39288033093300767\n",
      "Epoch: 472\n",
      "Train:  1.0326864341253876\n",
      "Test:  0.3959532380104065\n",
      "0.39349491234848744\n",
      "Epoch: 473\n",
      "Train:  1.0451015161891137\n",
      "Test:  0.3980286121368408\n",
      "0.3944016523061581\n",
      "Epoch: 474\n",
      "Train:  1.0372391002633239\n",
      "Test:  0.40064501762390137\n",
      "0.39565032536970673\n",
      "Epoch: 475\n",
      "Train:  1.0457141803958083\n",
      "Test:  0.40226247906684875\n",
      "0.3969727561091351\n",
      "Epoch: 476\n",
      "Train:  1.0550837346943476\n",
      "Test:  0.4028984308242798\n",
      "0.3981578910521641\n",
      "Epoch: 477\n",
      "Train:  1.0624485011863452\n",
      "Test:  0.4022441804409027\n",
      "0.3989751489299118\n",
      "Epoch: 478\n",
      "Train:  1.051467593158445\n",
      "Test:  0.4015112519264221\n",
      "0.3994823695292139\n",
      "Epoch: 479\n",
      "Train:  1.0403514034965986\n",
      "Test:  0.40048059821128845\n",
      "0.3996820152656288\n",
      "Epoch: 480\n",
      "Train:  1.037137257075438\n",
      "Test:  0.3997384011745453\n",
      "0.39969329244741214\n",
      "Epoch: 481\n",
      "Train:  1.0361301256443864\n",
      "Test:  0.3997843563556671\n",
      "0.3997115052290632\n",
      "Epoch: 482\n",
      "Train:  1.0340451871355374\n",
      "Test:  0.40025651454925537\n",
      "0.39982050709310163\n",
      "Epoch: 483\n",
      "Train:  1.0488453384048195\n",
      "Test:  0.40090423822402954\n",
      "0.4000372533192872\n",
      "Epoch: 484\n",
      "Train:  1.0381096466895072\n",
      "Test:  0.40153980255126953\n",
      "0.4003377631656837\n",
      "Epoch: 485\n",
      "Train:  1.0281041476835486\n",
      "Test:  0.40218859910964966\n",
      "0.40070793035447694\n",
      "Epoch: 486\n",
      "Train:  1.0299544847940887\n",
      "Test:  0.4029414653778076\n",
      "0.40115463735914303\n",
      "Epoch: 487\n",
      "Train:  1.0370286606973218\n",
      "Test:  0.40394532680511475\n",
      "0.4017127752483374\n",
      "Epoch: 488\n",
      "Train:  1.0360265434268983\n",
      "Test:  0.40556231141090393\n",
      "0.4024826824808507\n",
      "Epoch: 489\n",
      "Train:  1.0446301475968411\n",
      "Test:  0.4072819948196411\n",
      "0.40344254494860876\n",
      "Epoch: 490\n",
      "Train:  1.0457422416056357\n",
      "Test:  0.4088234305381775\n",
      "0.4045187220665225\n",
      "Epoch: 491\n",
      "Train:  1.0478059710353933\n",
      "Test:  0.4097553789615631\n",
      "0.4055660534455306\n",
      "Epoch: 492\n",
      "Train:  1.042602885073872\n",
      "Test:  0.41050395369529724\n",
      "0.4065536334954839\n",
      "Epoch: 493\n",
      "Train:  1.0526189318587702\n",
      "Test:  0.4106162190437317\n",
      "0.4073661506051334\n",
      "Epoch: 494\n",
      "Train:  1.0547495800000366\n",
      "Test:  0.4105333983898163\n",
      "0.40799960016207004\n",
      "Epoch: 495\n",
      "Train:  1.0542523398674943\n",
      "Test:  0.41041281819343567\n",
      "0.40848224376834313\n",
      "Epoch: 496\n",
      "Train:  1.0606976750717367\n",
      "Test:  0.41064929962158203\n",
      "0.4089156549389909\n",
      "Epoch: 497\n",
      "Train:  1.0557605192385695\n",
      "Test:  0.41108426451683044\n",
      "0.4093493768545588\n",
      "Epoch: 498\n",
      "Train:  1.0565560249391424\n",
      "Test:  0.41163724660873413\n",
      "0.40980695080539387\n",
      "Epoch: 499\n",
      "Train:  1.0509215178348685\n",
      "Test:  0.41235053539276123\n",
      "0.41031566772286737\n",
      "Epoch: 500\n",
      "Train:  1.0520100750589882\n",
      "Test:  0.41304105520248413\n",
      "0.41086074521879073\n",
      "Epoch: 501\n",
      "Train:  1.0462435459898365\n",
      "Test:  0.4138289988040924\n",
      "0.4114543959358511\n",
      "Epoch: 502\n",
      "Train:  1.063369134981786\n",
      "Test:  0.41478028893470764\n",
      "0.4121195745356224\n",
      "Epoch: 503\n",
      "Train:  1.0510674263200452\n",
      "Test:  0.4160175025463104\n",
      "0.41289916013776\n",
      "Epoch: 504\n",
      "Train:  1.0599791864233632\n",
      "Test:  0.41739124059677124\n",
      "0.41379757622956226\n",
      "Epoch: 505\n",
      "Train:  1.0622199360401399\n",
      "Test:  0.4191126823425293\n",
      "0.4148605974521557\n",
      "Epoch: 506\n",
      "Train:  1.063970722258091\n",
      "Test:  0.42081066966056824\n",
      "0.41605061189383824\n",
      "Epoch: 507\n",
      "Train:  1.0662768171519361\n",
      "Test:  0.4219379127025604\n",
      "0.41722807205558265\n",
      "Epoch: 508\n",
      "Train:  1.060925987899624\n",
      "Test:  0.4223889708518982\n",
      "0.4182602518148457\n",
      "Epoch: 509\n",
      "Train:  1.068896323762914\n",
      "Test:  0.4220525920391083\n",
      "0.41901871985969824\n",
      "Epoch: 510\n",
      "Train:  1.0595982548610499\n",
      "Test:  0.4218018054962158\n",
      "0.4195753369870018\n",
      "Epoch: 511\n",
      "Train:  1.0690141776075928\n",
      "Test:  0.42192092537879944\n",
      "0.42004445466536133\n",
      "Epoch: 512\n",
      "Train:  1.0671935014987504\n",
      "Test:  0.42257606983184814\n",
      "0.42055077769865873\n",
      "Epoch: 513\n",
      "Train:  1.0529962315834978\n",
      "Test:  0.42345115542411804\n",
      "0.4211308532437506\n",
      "Epoch: 514\n",
      "Train:  1.0608178053892428\n",
      "Test:  0.42457494139671326\n",
      "0.42181967087434313\n",
      "Epoch: 515\n",
      "Train:  1.0723776958322013\n",
      "Test:  0.4261973798274994\n",
      "0.4226952126649744\n",
      "Epoch: 516\n",
      "Train:  1.0660715667951493\n",
      "Test:  0.4281465411186218\n",
      "0.42378547835570385\n",
      "Epoch: 517\n",
      "Train:  1.0757738851731824\n",
      "Test:  0.43001678586006165\n",
      "0.4250317398565754\n",
      "Epoch: 518\n",
      "Train:  1.0768162578504572\n",
      "Test:  0.43094903230667114\n",
      "0.42621519834659455\n",
      "Epoch: 519\n",
      "Train:  1.0726964494553946\n",
      "Test:  0.4315756857395172\n",
      "0.4272872958251791\n",
      "Epoch: 520\n",
      "Train:  1.0805032352205886\n",
      "Test:  0.43138429522514343\n",
      "0.428106695705172\n",
      "Epoch: 521\n",
      "Train:  1.0758854843756205\n",
      "Test:  0.43124961853027344\n",
      "0.4287352802701923\n",
      "Epoch: 522\n",
      "Train:  1.0718027390940215\n",
      "Test:  0.43136656284332275\n",
      "0.42926153678481843\n",
      "Epoch: 523\n",
      "Train:  1.0733225427728186\n",
      "Test:  0.43181848526000977\n",
      "0.4297729264798567\n",
      "Epoch: 524\n",
      "Train:  1.0781413773375172\n",
      "Test:  0.4324987828731537\n",
      "0.4303180977585161\n",
      "Epoch: 525\n",
      "Train:  1.0841868061932824\n",
      "Test:  0.4333484470844269\n",
      "0.43092416762369823\n",
      "Epoch: 526\n",
      "Train:  1.0834389636994048\n",
      "Test:  0.43428516387939453\n",
      "0.4315963668748375\n",
      "Epoch: 527\n",
      "Train:  1.0814084046950905\n",
      "Test:  0.4353894293308258\n",
      "0.4323549793660352\n",
      "Epoch: 528\n",
      "Train:  1.0835201019641532\n",
      "Test:  0.43659746646881104\n",
      "0.4332034767865904\n",
      "Epoch: 529\n",
      "Train:  1.0817266386603155\n",
      "Test:  0.4381042718887329\n",
      "0.4341836358070189\n",
      "Epoch: 530\n",
      "Train:  1.0952036312671118\n",
      "Test:  0.44024813175201416\n",
      "0.43539653499601794\n",
      "Epoch: 531\n",
      "Train:  1.0983481022619432\n",
      "Test:  0.4418467879295349\n",
      "0.43668658558272133\n",
      "Epoch: 532\n",
      "Train:  1.1012633833032783\n",
      "Test:  0.4433448612689972\n",
      "0.4380182407199765\n",
      "Epoch: 533\n",
      "Train:  1.0958932600114295\n",
      "Test:  0.4445233941078186\n",
      "0.43931927139754495\n",
      "Epoch: 534\n",
      "Train:  1.0990291505811676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-15 17:53:16,778]\u001b[0m Trial 2 finished with value: 0.38280872752152595 and parameters: {'hidden_size_0': 512, 'hidden_size_1': 384, 'hidden_size_2': 384, 'hidden_size_3': 384, 'hidden_size_4': 256, 'hidden_size_5': 256, 'learning_rate': 1.0437634068410516e-05, 'b1': 0.9327956414431716}. Best is trial 0 with value: 0.37949389343331275.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  0.44542020559310913\n",
      "0.44053945823665774\n"
     ]
    }
   ],
   "source": [
    "import optuna \n",
    "study = optuna.create_study(direction = \"minimize\", sampler = optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials = 3, timeout=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqBc65tU7X0W",
    "outputId": "2e977c9a-3c7c-4778-832f-1f0218aa1961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value: 0.37949389343331275\n",
      "Best parameters: {'hidden_size_0': 256, 'hidden_size_1': 256, 'hidden_size_2': 256, 'hidden_size_3': 384, 'hidden_size_4': 384, 'hidden_size_5': 512, 'learning_rate': 1.7023104360630722e-05, 'b1': 0.9217182628689928}\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "print(\"Best value:\", best_trial.value)\n",
    "print(\"Best parameters:\", best_trial.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hM7_-wUa9WsI"
   },
   "source": [
    "##Using optimal hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLyGIgEH7X0X",
    "outputId": "b9d7ea7d-6636-4d23-8783-f8e4b43f2677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(test_data_pol.num_features,\n",
    "            [256, 256, 256, 384, 384, 512],\n",
    "            1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1.7023104360630722e-05,\n",
    "                             betas=(0.9217182628689928, 0.99))\n",
    "lossff = torch.nn.BCELoss()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChQprCc27X0X"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # print(out)\n",
    "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
    "        # print(loss)\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        all_preds.append(torch.reshape(out, (-1,)))\n",
    "        all_labels.append(data.y.float())\n",
    "    # print(all_preds)\n",
    "    accuracy, f1 = metrics(all_preds, all_labels)\n",
    "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
    "\n",
    "\n",
    "def metrics(preds, gts):\n",
    "    preds = torch.round(torch.cat(preds))\n",
    "    gts = torch.cat(gts)\n",
    "    # print(preds.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m3TPbj17X0X",
    "outputId": "3d50ab3d-2408-4d80-c94e-4e87f79d99bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TrainLoss: 0.69329 | TestLoss: 0.69326 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 01 |  TrainLoss: 0.69389 | TestLoss: 0.69326 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 02 |  TrainLoss: 0.69397 | TestLoss: 0.69326 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 03 |  TrainLoss: 0.69301 | TestLoss: 0.69327 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 04 |  TrainLoss: 0.69269 | TestLoss: 0.69327 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 05 |  TrainLoss: 0.69324 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 06 |  TrainLoss: 0.69263 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 07 |  TrainLoss: 0.69217 | TestLoss: 0.69329 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 08 |  TrainLoss: 0.69245 | TestLoss: 0.69330 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 09 |  TrainLoss: 0.69263 | TestLoss: 0.69331 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 10 |  TrainLoss: 0.69196 | TestLoss: 0.69332 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 11 |  TrainLoss: 0.69326 | TestLoss: 0.69333 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 12 |  TrainLoss: 0.69147 | TestLoss: 0.69334 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 13 |  TrainLoss: 0.69264 | TestLoss: 0.69335 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 14 |  TrainLoss: 0.69139 | TestLoss: 0.69336 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 15 |  TrainLoss: 0.69265 | TestLoss: 0.69337 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 16 |  TrainLoss: 0.69098 | TestLoss: 0.69338 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 17 |  TrainLoss: 0.69133 | TestLoss: 0.69338 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 18 |  TrainLoss: 0.69261 | TestLoss: 0.69339 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 19 |  TrainLoss: 0.69095 | TestLoss: 0.69340 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 20 |  TrainLoss: 0.69241 | TestLoss: 0.69341 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 21 |  TrainLoss: 0.68920 | TestLoss: 0.69341 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 22 |  TrainLoss: 0.69000 | TestLoss: 0.69342 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 23 |  TrainLoss: 0.69058 | TestLoss: 0.69343 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 24 |  TrainLoss: 0.69161 | TestLoss: 0.69344 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 25 |  TrainLoss: 0.69153 | TestLoss: 0.69345 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 26 |  TrainLoss: 0.69158 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 27 |  TrainLoss: 0.69183 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 28 |  TrainLoss: 0.69159 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 29 |  TrainLoss: 0.69155 | TestLoss: 0.69347 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 30 |  TrainLoss: 0.69013 | TestLoss: 0.69347 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 31 |  TrainLoss: 0.69072 | TestLoss: 0.69347 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 32 |  TrainLoss: 0.69065 | TestLoss: 0.69347 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 33 |  TrainLoss: 0.69224 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 34 |  TrainLoss: 0.69172 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 35 |  TrainLoss: 0.69164 | TestLoss: 0.69345 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 36 |  TrainLoss: 0.69022 | TestLoss: 0.69345 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 37 |  TrainLoss: 0.69076 | TestLoss: 0.69344 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 38 |  TrainLoss: 0.69113 | TestLoss: 0.69343 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 39 |  TrainLoss: 0.69091 | TestLoss: 0.69342 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 40 |  TrainLoss: 0.69144 | TestLoss: 0.69341 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 41 |  TrainLoss: 0.69055 | TestLoss: 0.69340 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 42 |  TrainLoss: 0.69172 | TestLoss: 0.69339 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 43 |  TrainLoss: 0.69003 | TestLoss: 0.69338 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 44 |  TrainLoss: 0.69037 | TestLoss: 0.69336 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 45 |  TrainLoss: 0.68960 | TestLoss: 0.69334 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 46 |  TrainLoss: 0.69058 | TestLoss: 0.69332 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 47 |  TrainLoss: 0.68953 | TestLoss: 0.69330 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 48 |  TrainLoss: 0.69107 | TestLoss: 0.69327 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 49 |  TrainLoss: 0.69079 | TestLoss: 0.69324 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 50 |  TrainLoss: 0.69083 | TestLoss: 0.69320 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 51 |  TrainLoss: 0.69185 | TestLoss: 0.69316 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 52 |  TrainLoss: 0.69039 | TestLoss: 0.69313 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 53 |  TrainLoss: 0.69087 | TestLoss: 0.69309 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 54 |  TrainLoss: 0.68979 | TestLoss: 0.69305 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 55 |  TrainLoss: 0.68938 | TestLoss: 0.69300 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 56 |  TrainLoss: 0.68877 | TestLoss: 0.69295 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 57 |  TrainLoss: 0.68813 | TestLoss: 0.69290 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 58 |  TrainLoss: 0.68850 | TestLoss: 0.69286 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 59 |  TrainLoss: 0.68917 | TestLoss: 0.69281 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 60 |  TrainLoss: 0.68960 | TestLoss: 0.69275 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 61 |  TrainLoss: 0.68979 | TestLoss: 0.69268 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 62 |  TrainLoss: 0.68974 | TestLoss: 0.69262 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 63 |  TrainLoss: 0.68899 | TestLoss: 0.69255 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 64 |  TrainLoss: 0.68999 | TestLoss: 0.69249 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 65 |  TrainLoss: 0.68971 | TestLoss: 0.69242 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 66 |  TrainLoss: 0.68979 | TestLoss: 0.69235 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 67 |  TrainLoss: 0.68910 | TestLoss: 0.69229 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 68 |  TrainLoss: 0.68914 | TestLoss: 0.69221 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 69 |  TrainLoss: 0.68732 | TestLoss: 0.69215 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 70 |  TrainLoss: 0.68932 | TestLoss: 0.69208 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 71 |  TrainLoss: 0.68867 | TestLoss: 0.69201 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 72 |  TrainLoss: 0.68825 | TestLoss: 0.69194 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 73 |  TrainLoss: 0.68729 | TestLoss: 0.69187 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 74 |  TrainLoss: 0.68707 | TestLoss: 0.69179 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 75 |  TrainLoss: 0.68740 | TestLoss: 0.69172 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 76 |  TrainLoss: 0.68744 | TestLoss: 0.69163 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 77 |  TrainLoss: 0.68678 | TestLoss: 0.69155 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 78 |  TrainLoss: 0.68763 | TestLoss: 0.69146 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 79 |  TrainLoss: 0.68743 | TestLoss: 0.69137 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 80 |  TrainLoss: 0.68707 | TestLoss: 0.69126 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 81 |  TrainLoss: 0.68769 | TestLoss: 0.69116 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 82 |  TrainLoss: 0.68802 | TestLoss: 0.69104 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 83 |  TrainLoss: 0.68657 | TestLoss: 0.69094 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 84 |  TrainLoss: 0.68810 | TestLoss: 0.69083 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 85 |  TrainLoss: 0.68579 | TestLoss: 0.69070 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 86 |  TrainLoss: 0.68597 | TestLoss: 0.69057 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 87 |  TrainLoss: 0.68598 | TestLoss: 0.69044 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 88 |  TrainLoss: 0.68534 | TestLoss: 0.69029 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 89 |  TrainLoss: 0.68541 | TestLoss: 0.69014 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 90 |  TrainLoss: 0.68396 | TestLoss: 0.69000 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 91 |  TrainLoss: 0.68582 | TestLoss: 0.68984 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 92 |  TrainLoss: 0.68554 | TestLoss: 0.68969 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 93 |  TrainLoss: 0.68491 | TestLoss: 0.68952 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 94 |  TrainLoss: 0.68572 | TestLoss: 0.68935 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 95 |  TrainLoss: 0.68491 | TestLoss: 0.68916 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 96 |  TrainLoss: 0.68514 | TestLoss: 0.68896 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 97 |  TrainLoss: 0.68407 | TestLoss: 0.68875 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 98 |  TrainLoss: 0.68312 | TestLoss: 0.68854 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 99 |  TrainLoss: 0.68228 | TestLoss: 0.68831 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 100 |  TrainLoss: 0.68198 | TestLoss: 0.68807 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 101 |  TrainLoss: 0.68297 | TestLoss: 0.68782 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 102 |  TrainLoss: 0.68219 | TestLoss: 0.68755 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 103 |  TrainLoss: 0.68334 | TestLoss: 0.68727 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 104 |  TrainLoss: 0.68255 | TestLoss: 0.68698 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 105 |  TrainLoss: 0.67946 | TestLoss: 0.68670 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 106 |  TrainLoss: 0.68048 | TestLoss: 0.68640 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 107 |  TrainLoss: 0.67952 | TestLoss: 0.68608 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 108 |  TrainLoss: 0.67888 | TestLoss: 0.68578 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 109 |  TrainLoss: 0.68024 | TestLoss: 0.68546 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 110 |  TrainLoss: 0.68002 | TestLoss: 0.68514 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 111 |  TrainLoss: 0.67777 | TestLoss: 0.68480 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 112 |  TrainLoss: 0.67796 | TestLoss: 0.68445 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 113 |  TrainLoss: 0.67892 | TestLoss: 0.68408 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 114 |  TrainLoss: 0.67659 | TestLoss: 0.68373 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 115 |  TrainLoss: 0.67479 | TestLoss: 0.68337 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 116 |  TrainLoss: 0.67562 | TestLoss: 0.68299 | TestAcc: 0.48869 | TestF1: 0.00\n",
      "Epoch: 117 |  TrainLoss: 0.67517 | TestLoss: 0.68261 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 118 |  TrainLoss: 0.67529 | TestLoss: 0.68221 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 119 |  TrainLoss: 0.67535 | TestLoss: 0.68181 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 120 |  TrainLoss: 0.67417 | TestLoss: 0.68140 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 121 |  TrainLoss: 0.67460 | TestLoss: 0.68094 | TestAcc: 0.49321 | TestF1: 0.02\n",
      "Epoch: 122 |  TrainLoss: 0.67408 | TestLoss: 0.68045 | TestAcc: 0.50226 | TestF1: 0.05\n",
      "Epoch: 123 |  TrainLoss: 0.67334 | TestLoss: 0.67992 | TestAcc: 0.50226 | TestF1: 0.05\n",
      "Epoch: 124 |  TrainLoss: 0.67136 | TestLoss: 0.67939 | TestAcc: 0.50679 | TestF1: 0.07\n",
      "Epoch: 125 |  TrainLoss: 0.67073 | TestLoss: 0.67885 | TestAcc: 0.51584 | TestF1: 0.10\n",
      "Epoch: 126 |  TrainLoss: 0.67312 | TestLoss: 0.67831 | TestAcc: 0.52036 | TestF1: 0.12\n",
      "Epoch: 127 |  TrainLoss: 0.67101 | TestLoss: 0.67773 | TestAcc: 0.53394 | TestF1: 0.16\n",
      "Epoch: 128 |  TrainLoss: 0.66766 | TestLoss: 0.67715 | TestAcc: 0.53846 | TestF1: 0.18\n",
      "Epoch: 129 |  TrainLoss: 0.66778 | TestLoss: 0.67656 | TestAcc: 0.54751 | TestF1: 0.21\n",
      "Epoch: 130 |  TrainLoss: 0.66818 | TestLoss: 0.67594 | TestAcc: 0.56109 | TestF1: 0.25\n",
      "Epoch: 131 |  TrainLoss: 0.66716 | TestLoss: 0.67531 | TestAcc: 0.56561 | TestF1: 0.26\n",
      "Epoch: 132 |  TrainLoss: 0.66719 | TestLoss: 0.67469 | TestAcc: 0.57466 | TestF1: 0.29\n",
      "Epoch: 133 |  TrainLoss: 0.66738 | TestLoss: 0.67409 | TestAcc: 0.57466 | TestF1: 0.29\n",
      "Epoch: 134 |  TrainLoss: 0.66303 | TestLoss: 0.67349 | TestAcc: 0.57466 | TestF1: 0.29\n",
      "Epoch: 135 |  TrainLoss: 0.66365 | TestLoss: 0.67285 | TestAcc: 0.58371 | TestF1: 0.31\n",
      "Epoch: 136 |  TrainLoss: 0.66107 | TestLoss: 0.67215 | TestAcc: 0.58371 | TestF1: 0.31\n",
      "Epoch: 137 |  TrainLoss: 0.66195 | TestLoss: 0.67146 | TestAcc: 0.58371 | TestF1: 0.31\n",
      "Epoch: 138 |  TrainLoss: 0.65933 | TestLoss: 0.67071 | TestAcc: 0.59276 | TestF1: 0.34\n",
      "Epoch: 139 |  TrainLoss: 0.65718 | TestLoss: 0.66991 | TestAcc: 0.61086 | TestF1: 0.39\n",
      "Epoch: 140 |  TrainLoss: 0.65646 | TestLoss: 0.66910 | TestAcc: 0.61538 | TestF1: 0.41\n",
      "Epoch: 141 |  TrainLoss: 0.65878 | TestLoss: 0.66824 | TestAcc: 0.62896 | TestF1: 0.44\n",
      "Epoch: 142 |  TrainLoss: 0.65670 | TestLoss: 0.66737 | TestAcc: 0.63348 | TestF1: 0.45\n",
      "Epoch: 143 |  TrainLoss: 0.65470 | TestLoss: 0.66650 | TestAcc: 0.63348 | TestF1: 0.45\n",
      "Epoch: 144 |  TrainLoss: 0.65127 | TestLoss: 0.66564 | TestAcc: 0.63801 | TestF1: 0.46\n",
      "Epoch: 145 |  TrainLoss: 0.65300 | TestLoss: 0.66476 | TestAcc: 0.63801 | TestF1: 0.46\n",
      "Epoch: 146 |  TrainLoss: 0.65074 | TestLoss: 0.66380 | TestAcc: 0.65158 | TestF1: 0.49\n",
      "Epoch: 147 |  TrainLoss: 0.65139 | TestLoss: 0.66281 | TestAcc: 0.64253 | TestF1: 0.48\n",
      "Epoch: 148 |  TrainLoss: 0.64861 | TestLoss: 0.66181 | TestAcc: 0.65158 | TestF1: 0.50\n",
      "Epoch: 149 |  TrainLoss: 0.64863 | TestLoss: 0.66080 | TestAcc: 0.65158 | TestF1: 0.50\n",
      "Epoch: 150 |  TrainLoss: 0.64687 | TestLoss: 0.65977 | TestAcc: 0.66516 | TestF1: 0.53\n",
      "Epoch: 151 |  TrainLoss: 0.64305 | TestLoss: 0.65870 | TestAcc: 0.66516 | TestF1: 0.53\n",
      "Epoch: 152 |  TrainLoss: 0.64045 | TestLoss: 0.65759 | TestAcc: 0.67873 | TestF1: 0.56\n",
      "Epoch: 153 |  TrainLoss: 0.64035 | TestLoss: 0.65643 | TestAcc: 0.68326 | TestF1: 0.57\n",
      "Epoch: 154 |  TrainLoss: 0.63495 | TestLoss: 0.65527 | TestAcc: 0.69231 | TestF1: 0.59\n",
      "Epoch: 155 |  TrainLoss: 0.63906 | TestLoss: 0.65412 | TestAcc: 0.69231 | TestF1: 0.59\n",
      "Epoch: 156 |  TrainLoss: 0.63682 | TestLoss: 0.65288 | TestAcc: 0.69683 | TestF1: 0.59\n",
      "Epoch: 157 |  TrainLoss: 0.63310 | TestLoss: 0.65154 | TestAcc: 0.71041 | TestF1: 0.62\n",
      "Epoch: 158 |  TrainLoss: 0.63646 | TestLoss: 0.65015 | TestAcc: 0.72851 | TestF1: 0.65\n",
      "Epoch: 159 |  TrainLoss: 0.63353 | TestLoss: 0.64875 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 160 |  TrainLoss: 0.63471 | TestLoss: 0.64731 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 161 |  TrainLoss: 0.62423 | TestLoss: 0.64593 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 162 |  TrainLoss: 0.62635 | TestLoss: 0.64455 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 163 |  TrainLoss: 0.62383 | TestLoss: 0.64315 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 164 |  TrainLoss: 0.62390 | TestLoss: 0.64178 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 165 |  TrainLoss: 0.61955 | TestLoss: 0.64034 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 166 |  TrainLoss: 0.61787 | TestLoss: 0.63887 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 167 |  TrainLoss: 0.61466 | TestLoss: 0.63730 | TestAcc: 0.73756 | TestF1: 0.67\n",
      "Epoch: 168 |  TrainLoss: 0.61174 | TestLoss: 0.63545 | TestAcc: 0.75113 | TestF1: 0.70\n",
      "Epoch: 169 |  TrainLoss: 0.61447 | TestLoss: 0.63347 | TestAcc: 0.76018 | TestF1: 0.71\n",
      "Epoch: 170 |  TrainLoss: 0.61008 | TestLoss: 0.63145 | TestAcc: 0.78281 | TestF1: 0.74\n",
      "Epoch: 171 |  TrainLoss: 0.59952 | TestLoss: 0.62943 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 172 |  TrainLoss: 0.60348 | TestLoss: 0.62751 | TestAcc: 0.78281 | TestF1: 0.75\n",
      "Epoch: 173 |  TrainLoss: 0.59695 | TestLoss: 0.62571 | TestAcc: 0.78281 | TestF1: 0.75\n",
      "Epoch: 174 |  TrainLoss: 0.60471 | TestLoss: 0.62405 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 175 |  TrainLoss: 0.59678 | TestLoss: 0.62240 | TestAcc: 0.77376 | TestF1: 0.74\n",
      "Epoch: 176 |  TrainLoss: 0.59364 | TestLoss: 0.62080 | TestAcc: 0.76923 | TestF1: 0.73\n",
      "Epoch: 177 |  TrainLoss: 0.59264 | TestLoss: 0.61916 | TestAcc: 0.76923 | TestF1: 0.72\n",
      "Epoch: 178 |  TrainLoss: 0.58430 | TestLoss: 0.61732 | TestAcc: 0.76923 | TestF1: 0.72\n",
      "Epoch: 179 |  TrainLoss: 0.58738 | TestLoss: 0.61511 | TestAcc: 0.76923 | TestF1: 0.73\n",
      "Epoch: 180 |  TrainLoss: 0.58471 | TestLoss: 0.61267 | TestAcc: 0.77828 | TestF1: 0.75\n",
      "Epoch: 181 |  TrainLoss: 0.57778 | TestLoss: 0.61015 | TestAcc: 0.78733 | TestF1: 0.76\n",
      "Epoch: 182 |  TrainLoss: 0.56990 | TestLoss: 0.60755 | TestAcc: 0.79186 | TestF1: 0.77\n",
      "Epoch: 183 |  TrainLoss: 0.57161 | TestLoss: 0.60505 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 184 |  TrainLoss: 0.56735 | TestLoss: 0.60266 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 185 |  TrainLoss: 0.56141 | TestLoss: 0.60048 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 186 |  TrainLoss: 0.56395 | TestLoss: 0.59846 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 187 |  TrainLoss: 0.56154 | TestLoss: 0.59641 | TestAcc: 0.79638 | TestF1: 0.77\n",
      "Epoch: 188 |  TrainLoss: 0.55874 | TestLoss: 0.59424 | TestAcc: 0.79638 | TestF1: 0.77\n",
      "Epoch: 189 |  TrainLoss: 0.55518 | TestLoss: 0.59198 | TestAcc: 0.79186 | TestF1: 0.77\n",
      "Epoch: 190 |  TrainLoss: 0.55058 | TestLoss: 0.58935 | TestAcc: 0.79638 | TestF1: 0.77\n",
      "Epoch: 191 |  TrainLoss: 0.54826 | TestLoss: 0.58647 | TestAcc: 0.80090 | TestF1: 0.78\n",
      "Epoch: 192 |  TrainLoss: 0.53889 | TestLoss: 0.58363 | TestAcc: 0.80543 | TestF1: 0.78\n",
      "Epoch: 193 |  TrainLoss: 0.53108 | TestLoss: 0.58081 | TestAcc: 0.80995 | TestF1: 0.79\n",
      "Epoch: 194 |  TrainLoss: 0.52758 | TestLoss: 0.57822 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 195 |  TrainLoss: 0.53445 | TestLoss: 0.57558 | TestAcc: 0.81448 | TestF1: 0.80\n",
      "Epoch: 196 |  TrainLoss: 0.52828 | TestLoss: 0.57286 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 197 |  TrainLoss: 0.52719 | TestLoss: 0.57027 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 198 |  TrainLoss: 0.52343 | TestLoss: 0.56735 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 199 |  TrainLoss: 0.51863 | TestLoss: 0.56450 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 200 |  TrainLoss: 0.51530 | TestLoss: 0.56150 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 201 |  TrainLoss: 0.50753 | TestLoss: 0.55834 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 202 |  TrainLoss: 0.49685 | TestLoss: 0.55524 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 203 |  TrainLoss: 0.50100 | TestLoss: 0.55201 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 204 |  TrainLoss: 0.49031 | TestLoss: 0.54887 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 205 |  TrainLoss: 0.47903 | TestLoss: 0.54581 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 206 |  TrainLoss: 0.48354 | TestLoss: 0.54279 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 207 |  TrainLoss: 0.47969 | TestLoss: 0.53986 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 208 |  TrainLoss: 0.47569 | TestLoss: 0.53719 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 209 |  TrainLoss: 0.47287 | TestLoss: 0.53418 | TestAcc: 0.81900 | TestF1: 0.80\n",
      "Epoch: 210 |  TrainLoss: 0.46687 | TestLoss: 0.53040 | TestAcc: 0.82353 | TestF1: 0.81\n",
      "Epoch: 211 |  TrainLoss: 0.45799 | TestLoss: 0.52645 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 212 |  TrainLoss: 0.45959 | TestLoss: 0.52260 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 213 |  TrainLoss: 0.45157 | TestLoss: 0.51918 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 214 |  TrainLoss: 0.44342 | TestLoss: 0.51582 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 215 |  TrainLoss: 0.43941 | TestLoss: 0.51278 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 216 |  TrainLoss: 0.43592 | TestLoss: 0.51010 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 217 |  TrainLoss: 0.43507 | TestLoss: 0.50752 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 218 |  TrainLoss: 0.42365 | TestLoss: 0.50453 | TestAcc: 0.82805 | TestF1: 0.82\n",
      "Epoch: 219 |  TrainLoss: 0.41924 | TestLoss: 0.50075 | TestAcc: 0.83258 | TestF1: 0.82\n",
      "Epoch: 220 |  TrainLoss: 0.41862 | TestLoss: 0.49720 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 221 |  TrainLoss: 0.40861 | TestLoss: 0.49325 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 222 |  TrainLoss: 0.40992 | TestLoss: 0.48989 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 223 |  TrainLoss: 0.40104 | TestLoss: 0.48696 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 224 |  TrainLoss: 0.39228 | TestLoss: 0.48415 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 225 |  TrainLoss: 0.38362 | TestLoss: 0.48148 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 226 |  TrainLoss: 0.37832 | TestLoss: 0.47889 | TestAcc: 0.83710 | TestF1: 0.83\n",
      "Epoch: 227 |  TrainLoss: 0.37463 | TestLoss: 0.47491 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 228 |  TrainLoss: 0.36646 | TestLoss: 0.47064 | TestAcc: 0.84163 | TestF1: 0.84\n",
      "Epoch: 229 |  TrainLoss: 0.36283 | TestLoss: 0.46670 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 230 |  TrainLoss: 0.36943 | TestLoss: 0.46347 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 231 |  TrainLoss: 0.36547 | TestLoss: 0.46084 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 232 |  TrainLoss: 0.34860 | TestLoss: 0.45886 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 233 |  TrainLoss: 0.35034 | TestLoss: 0.45616 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 234 |  TrainLoss: 0.35026 | TestLoss: 0.45402 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 235 |  TrainLoss: 0.34026 | TestLoss: 0.45074 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 236 |  TrainLoss: 0.33556 | TestLoss: 0.44821 | TestAcc: 0.85068 | TestF1: 0.85\n",
      "Epoch: 237 |  TrainLoss: 0.32624 | TestLoss: 0.44600 | TestAcc: 0.84615 | TestF1: 0.84\n",
      "Epoch: 238 |  TrainLoss: 0.32911 | TestLoss: 0.44286 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 239 |  TrainLoss: 0.32337 | TestLoss: 0.43951 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 240 |  TrainLoss: 0.31256 | TestLoss: 0.43633 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 241 |  TrainLoss: 0.29461 | TestLoss: 0.43351 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 242 |  TrainLoss: 0.29844 | TestLoss: 0.43122 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 243 |  TrainLoss: 0.29284 | TestLoss: 0.42967 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 244 |  TrainLoss: 0.27954 | TestLoss: 0.42828 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 245 |  TrainLoss: 0.28337 | TestLoss: 0.42681 | TestAcc: 0.85520 | TestF1: 0.85\n",
      "Epoch: 246 |  TrainLoss: 0.28204 | TestLoss: 0.42493 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 247 |  TrainLoss: 0.27538 | TestLoss: 0.42255 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 248 |  TrainLoss: 0.26886 | TestLoss: 0.41956 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 249 |  TrainLoss: 0.26071 | TestLoss: 0.41626 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 250 |  TrainLoss: 0.25558 | TestLoss: 0.41363 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 251 |  TrainLoss: 0.25438 | TestLoss: 0.41195 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 252 |  TrainLoss: 0.25313 | TestLoss: 0.41048 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 253 |  TrainLoss: 0.24646 | TestLoss: 0.40962 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 254 |  TrainLoss: 0.24608 | TestLoss: 0.40973 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 255 |  TrainLoss: 0.23442 | TestLoss: 0.40879 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 256 |  TrainLoss: 0.23254 | TestLoss: 0.40637 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 257 |  TrainLoss: 0.23586 | TestLoss: 0.40342 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 258 |  TrainLoss: 0.22561 | TestLoss: 0.40106 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 259 |  TrainLoss: 0.22784 | TestLoss: 0.40005 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 260 |  TrainLoss: 0.21740 | TestLoss: 0.40074 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 261 |  TrainLoss: 0.20571 | TestLoss: 0.40226 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 262 |  TrainLoss: 0.20273 | TestLoss: 0.40328 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 263 |  TrainLoss: 0.20145 | TestLoss: 0.40103 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 264 |  TrainLoss: 0.19568 | TestLoss: 0.39788 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 265 |  TrainLoss: 0.19618 | TestLoss: 0.39508 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 266 |  TrainLoss: 0.19677 | TestLoss: 0.39342 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 267 |  TrainLoss: 0.19414 | TestLoss: 0.39284 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 268 |  TrainLoss: 0.18398 | TestLoss: 0.39328 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 269 |  TrainLoss: 0.18185 | TestLoss: 0.39561 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 270 |  TrainLoss: 0.18247 | TestLoss: 0.39813 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 271 |  TrainLoss: 0.17093 | TestLoss: 0.39748 | TestAcc: 0.85973 | TestF1: 0.86\n",
      "Epoch: 272 |  TrainLoss: 0.15676 | TestLoss: 0.39578 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 273 |  TrainLoss: 0.16365 | TestLoss: 0.39427 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 274 |  TrainLoss: 0.15617 | TestLoss: 0.39280 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 275 |  TrainLoss: 0.14938 | TestLoss: 0.39214 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 276 |  TrainLoss: 0.14853 | TestLoss: 0.39237 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 277 |  TrainLoss: 0.15626 | TestLoss: 0.39394 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 278 |  TrainLoss: 0.15088 | TestLoss: 0.39543 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 279 |  TrainLoss: 0.14117 | TestLoss: 0.39592 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 280 |  TrainLoss: 0.15166 | TestLoss: 0.39667 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 281 |  TrainLoss: 0.14185 | TestLoss: 0.39652 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 282 |  TrainLoss: 0.13494 | TestLoss: 0.39682 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 283 |  TrainLoss: 0.13147 | TestLoss: 0.39616 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 284 |  TrainLoss: 0.12703 | TestLoss: 0.39616 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 285 |  TrainLoss: 0.13418 | TestLoss: 0.39566 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 286 |  TrainLoss: 0.12516 | TestLoss: 0.39690 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 287 |  TrainLoss: 0.12243 | TestLoss: 0.39981 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 288 |  TrainLoss: 0.11322 | TestLoss: 0.40260 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 289 |  TrainLoss: 0.11837 | TestLoss: 0.40220 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 290 |  TrainLoss: 0.10423 | TestLoss: 0.40015 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 291 |  TrainLoss: 0.11140 | TestLoss: 0.39848 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 292 |  TrainLoss: 0.11848 | TestLoss: 0.39873 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 293 |  TrainLoss: 0.10744 | TestLoss: 0.40038 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 294 |  TrainLoss: 0.10029 | TestLoss: 0.40278 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 295 |  TrainLoss: 0.10827 | TestLoss: 0.40597 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 296 |  TrainLoss: 0.10053 | TestLoss: 0.40785 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 297 |  TrainLoss: 0.10340 | TestLoss: 0.40891 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 298 |  TrainLoss: 0.11031 | TestLoss: 0.40993 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 299 |  TrainLoss: 0.08619 | TestLoss: 0.41088 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 300 |  TrainLoss: 0.09759 | TestLoss: 0.41238 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 301 |  TrainLoss: 0.09050 | TestLoss: 0.41316 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 302 |  TrainLoss: 0.09580 | TestLoss: 0.41451 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 303 |  TrainLoss: 0.08083 | TestLoss: 0.41515 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 304 |  TrainLoss: 0.07898 | TestLoss: 0.41632 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 305 |  TrainLoss: 0.08648 | TestLoss: 0.41803 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 306 |  TrainLoss: 0.08107 | TestLoss: 0.42126 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 307 |  TrainLoss: 0.07651 | TestLoss: 0.42270 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 308 |  TrainLoss: 0.08059 | TestLoss: 0.42271 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 309 |  TrainLoss: 0.07954 | TestLoss: 0.42369 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 310 |  TrainLoss: 0.07580 | TestLoss: 0.42350 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 311 |  TrainLoss: 0.06985 | TestLoss: 0.42385 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 312 |  TrainLoss: 0.06856 | TestLoss: 0.42543 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 313 |  TrainLoss: 0.06180 | TestLoss: 0.42755 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 314 |  TrainLoss: 0.06173 | TestLoss: 0.43151 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 315 |  TrainLoss: 0.06460 | TestLoss: 0.43372 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 316 |  TrainLoss: 0.06296 | TestLoss: 0.43573 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 317 |  TrainLoss: 0.07502 | TestLoss: 0.43586 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 318 |  TrainLoss: 0.05599 | TestLoss: 0.43710 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 319 |  TrainLoss: 0.06188 | TestLoss: 0.43784 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 320 |  TrainLoss: 0.05484 | TestLoss: 0.43853 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 321 |  TrainLoss: 0.05308 | TestLoss: 0.43846 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 322 |  TrainLoss: 0.05271 | TestLoss: 0.43859 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 323 |  TrainLoss: 0.05476 | TestLoss: 0.43982 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 324 |  TrainLoss: 0.04938 | TestLoss: 0.44191 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 325 |  TrainLoss: 0.05115 | TestLoss: 0.44544 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 326 |  TrainLoss: 0.05279 | TestLoss: 0.45012 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 327 |  TrainLoss: 0.05279 | TestLoss: 0.45527 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 328 |  TrainLoss: 0.04322 | TestLoss: 0.45766 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 329 |  TrainLoss: 0.04462 | TestLoss: 0.45698 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 330 |  TrainLoss: 0.04839 | TestLoss: 0.45516 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 331 |  TrainLoss: 0.04825 | TestLoss: 0.45417 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 332 |  TrainLoss: 0.03944 | TestLoss: 0.45561 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 333 |  TrainLoss: 0.04393 | TestLoss: 0.45752 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 334 |  TrainLoss: 0.04280 | TestLoss: 0.46106 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 335 |  TrainLoss: 0.04090 | TestLoss: 0.46572 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 336 |  TrainLoss: 0.03362 | TestLoss: 0.47077 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 337 |  TrainLoss: 0.04318 | TestLoss: 0.47383 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 338 |  TrainLoss: 0.04152 | TestLoss: 0.47227 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 339 |  TrainLoss: 0.03547 | TestLoss: 0.47005 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 340 |  TrainLoss: 0.03275 | TestLoss: 0.46863 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 341 |  TrainLoss: 0.04652 | TestLoss: 0.46863 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 342 |  TrainLoss: 0.03990 | TestLoss: 0.47033 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 343 |  TrainLoss: 0.04567 | TestLoss: 0.47283 | TestAcc: 0.86878 | TestF1: 0.87\n",
      "Epoch: 344 |  TrainLoss: 0.02703 | TestLoss: 0.47668 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 345 |  TrainLoss: 0.03349 | TestLoss: 0.48138 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 346 |  TrainLoss: 0.03075 | TestLoss: 0.48577 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 347 |  TrainLoss: 0.03868 | TestLoss: 0.48732 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 348 |  TrainLoss: 0.03136 | TestLoss: 0.48779 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 349 |  TrainLoss: 0.02805 | TestLoss: 0.48880 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 350 |  TrainLoss: 0.03144 | TestLoss: 0.48826 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 351 |  TrainLoss: 0.03388 | TestLoss: 0.48921 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 352 |  TrainLoss: 0.02605 | TestLoss: 0.49083 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 353 |  TrainLoss: 0.03285 | TestLoss: 0.49209 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 354 |  TrainLoss: 0.02468 | TestLoss: 0.49298 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 355 |  TrainLoss: 0.02713 | TestLoss: 0.49356 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 356 |  TrainLoss: 0.02590 | TestLoss: 0.49479 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 357 |  TrainLoss: 0.02712 | TestLoss: 0.49680 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 358 |  TrainLoss: 0.02177 | TestLoss: 0.49989 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 359 |  TrainLoss: 0.02382 | TestLoss: 0.50431 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 360 |  TrainLoss: 0.02038 | TestLoss: 0.50819 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 361 |  TrainLoss: 0.02407 | TestLoss: 0.51052 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 362 |  TrainLoss: 0.02214 | TestLoss: 0.51280 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 363 |  TrainLoss: 0.02091 | TestLoss: 0.51438 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 364 |  TrainLoss: 0.02139 | TestLoss: 0.51463 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 365 |  TrainLoss: 0.02220 | TestLoss: 0.51372 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 366 |  TrainLoss: 0.02030 | TestLoss: 0.51327 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 367 |  TrainLoss: 0.02287 | TestLoss: 0.51371 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 368 |  TrainLoss: 0.01834 | TestLoss: 0.51509 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 369 |  TrainLoss: 0.01897 | TestLoss: 0.51792 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 370 |  TrainLoss: 0.02099 | TestLoss: 0.52142 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 371 |  TrainLoss: 0.01790 | TestLoss: 0.52500 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 372 |  TrainLoss: 0.02179 | TestLoss: 0.52888 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 373 |  TrainLoss: 0.02157 | TestLoss: 0.53288 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 374 |  TrainLoss: 0.02010 | TestLoss: 0.53511 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 375 |  TrainLoss: 0.02230 | TestLoss: 0.53546 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 376 |  TrainLoss: 0.01830 | TestLoss: 0.53495 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 377 |  TrainLoss: 0.01446 | TestLoss: 0.53524 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 378 |  TrainLoss: 0.01470 | TestLoss: 0.53487 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 379 |  TrainLoss: 0.01452 | TestLoss: 0.53528 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 380 |  TrainLoss: 0.01512 | TestLoss: 0.53574 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 381 |  TrainLoss: 0.01518 | TestLoss: 0.53726 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 382 |  TrainLoss: 0.01693 | TestLoss: 0.53931 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 383 |  TrainLoss: 0.01565 | TestLoss: 0.54219 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 384 |  TrainLoss: 0.01608 | TestLoss: 0.54524 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 385 |  TrainLoss: 0.01520 | TestLoss: 0.54772 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 386 |  TrainLoss: 0.01353 | TestLoss: 0.55056 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 387 |  TrainLoss: 0.01338 | TestLoss: 0.55259 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 388 |  TrainLoss: 0.01729 | TestLoss: 0.55708 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 389 |  TrainLoss: 0.01352 | TestLoss: 0.56013 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 390 |  TrainLoss: 0.01260 | TestLoss: 0.56250 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 391 |  TrainLoss: 0.01238 | TestLoss: 0.56369 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 392 |  TrainLoss: 0.01621 | TestLoss: 0.56355 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 393 |  TrainLoss: 0.00987 | TestLoss: 0.56288 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 394 |  TrainLoss: 0.01132 | TestLoss: 0.56186 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 395 |  TrainLoss: 0.01241 | TestLoss: 0.56199 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 396 |  TrainLoss: 0.01074 | TestLoss: 0.56173 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 397 |  TrainLoss: 0.01771 | TestLoss: 0.56289 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 398 |  TrainLoss: 0.01172 | TestLoss: 0.56492 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 399 |  TrainLoss: 0.01273 | TestLoss: 0.56722 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 400 |  TrainLoss: 0.01087 | TestLoss: 0.57032 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 401 |  TrainLoss: 0.01142 | TestLoss: 0.57366 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 402 |  TrainLoss: 0.01012 | TestLoss: 0.57655 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 403 |  TrainLoss: 0.01175 | TestLoss: 0.57743 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 404 |  TrainLoss: 0.01038 | TestLoss: 0.57888 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 405 |  TrainLoss: 0.01213 | TestLoss: 0.58038 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 406 |  TrainLoss: 0.01108 | TestLoss: 0.58177 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 407 |  TrainLoss: 0.00902 | TestLoss: 0.58373 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 408 |  TrainLoss: 0.00926 | TestLoss: 0.58544 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 409 |  TrainLoss: 0.00842 | TestLoss: 0.58598 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 410 |  TrainLoss: 0.01063 | TestLoss: 0.58621 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 411 |  TrainLoss: 0.00962 | TestLoss: 0.58594 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 412 |  TrainLoss: 0.00883 | TestLoss: 0.58538 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 413 |  TrainLoss: 0.00934 | TestLoss: 0.58549 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 414 |  TrainLoss: 0.00891 | TestLoss: 0.58660 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 415 |  TrainLoss: 0.00874 | TestLoss: 0.58836 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 416 |  TrainLoss: 0.01043 | TestLoss: 0.59133 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 417 |  TrainLoss: 0.00774 | TestLoss: 0.59460 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 418 |  TrainLoss: 0.00983 | TestLoss: 0.59922 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 419 |  TrainLoss: 0.00918 | TestLoss: 0.60330 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 420 |  TrainLoss: 0.00709 | TestLoss: 0.60566 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 421 |  TrainLoss: 0.01056 | TestLoss: 0.60656 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 422 |  TrainLoss: 0.00798 | TestLoss: 0.60641 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 423 |  TrainLoss: 0.00750 | TestLoss: 0.60561 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 424 |  TrainLoss: 0.00659 | TestLoss: 0.60463 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 425 |  TrainLoss: 0.00621 | TestLoss: 0.60386 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 426 |  TrainLoss: 0.00737 | TestLoss: 0.60406 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 427 |  TrainLoss: 0.00650 | TestLoss: 0.60422 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 428 |  TrainLoss: 0.00620 | TestLoss: 0.60522 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 429 |  TrainLoss: 0.00650 | TestLoss: 0.60740 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 430 |  TrainLoss: 0.00789 | TestLoss: 0.61036 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 431 |  TrainLoss: 0.00710 | TestLoss: 0.61338 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 432 |  TrainLoss: 0.00646 | TestLoss: 0.61584 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 433 |  TrainLoss: 0.00641 | TestLoss: 0.61804 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 434 |  TrainLoss: 0.00658 | TestLoss: 0.62028 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 435 |  TrainLoss: 0.00569 | TestLoss: 0.62193 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 436 |  TrainLoss: 0.00638 | TestLoss: 0.62399 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 437 |  TrainLoss: 0.00577 | TestLoss: 0.62562 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 438 |  TrainLoss: 0.00621 | TestLoss: 0.62595 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 439 |  TrainLoss: 0.00597 | TestLoss: 0.62665 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 440 |  TrainLoss: 0.00553 | TestLoss: 0.62637 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 441 |  TrainLoss: 0.00557 | TestLoss: 0.62598 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 442 |  TrainLoss: 0.00701 | TestLoss: 0.62561 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 443 |  TrainLoss: 0.00451 | TestLoss: 0.62547 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 444 |  TrainLoss: 0.00594 | TestLoss: 0.62670 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 445 |  TrainLoss: 0.00756 | TestLoss: 0.62949 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 446 |  TrainLoss: 0.00477 | TestLoss: 0.63312 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 447 |  TrainLoss: 0.00616 | TestLoss: 0.63750 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 448 |  TrainLoss: 0.00590 | TestLoss: 0.64108 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 449 |  TrainLoss: 0.00574 | TestLoss: 0.64384 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 450 |  TrainLoss: 0.00472 | TestLoss: 0.64507 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 451 |  TrainLoss: 0.00753 | TestLoss: 0.64446 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 452 |  TrainLoss: 0.00568 | TestLoss: 0.64303 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 453 |  TrainLoss: 0.00458 | TestLoss: 0.64165 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 454 |  TrainLoss: 0.00614 | TestLoss: 0.64164 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 455 |  TrainLoss: 0.00502 | TestLoss: 0.64232 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 456 |  TrainLoss: 0.00579 | TestLoss: 0.64411 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 457 |  TrainLoss: 0.00565 | TestLoss: 0.64658 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 458 |  TrainLoss: 0.00396 | TestLoss: 0.64978 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 459 |  TrainLoss: 0.00345 | TestLoss: 0.65365 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 460 |  TrainLoss: 0.00393 | TestLoss: 0.65708 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 461 |  TrainLoss: 0.00577 | TestLoss: 0.65911 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 462 |  TrainLoss: 0.00463 | TestLoss: 0.66071 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 463 |  TrainLoss: 0.00420 | TestLoss: 0.66230 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 464 |  TrainLoss: 0.00584 | TestLoss: 0.66170 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 465 |  TrainLoss: 0.00536 | TestLoss: 0.66117 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 466 |  TrainLoss: 0.00498 | TestLoss: 0.66102 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 467 |  TrainLoss: 0.00377 | TestLoss: 0.66097 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 468 |  TrainLoss: 0.00389 | TestLoss: 0.66158 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 469 |  TrainLoss: 0.00350 | TestLoss: 0.66240 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 470 |  TrainLoss: 0.00497 | TestLoss: 0.66260 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 471 |  TrainLoss: 0.00372 | TestLoss: 0.66319 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 472 |  TrainLoss: 0.00355 | TestLoss: 0.66406 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 473 |  TrainLoss: 0.00349 | TestLoss: 0.66484 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 474 |  TrainLoss: 0.00478 | TestLoss: 0.66628 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 475 |  TrainLoss: 0.00389 | TestLoss: 0.66828 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 476 |  TrainLoss: 0.00299 | TestLoss: 0.67051 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 477 |  TrainLoss: 0.00347 | TestLoss: 0.67374 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 478 |  TrainLoss: 0.00367 | TestLoss: 0.67637 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 479 |  TrainLoss: 0.00298 | TestLoss: 0.67906 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 480 |  TrainLoss: 0.00334 | TestLoss: 0.68188 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 481 |  TrainLoss: 0.00506 | TestLoss: 0.68575 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 482 |  TrainLoss: 0.00478 | TestLoss: 0.68711 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 483 |  TrainLoss: 0.00378 | TestLoss: 0.68677 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 484 |  TrainLoss: 0.00289 | TestLoss: 0.68608 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 485 |  TrainLoss: 0.00376 | TestLoss: 0.68588 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 486 |  TrainLoss: 0.00334 | TestLoss: 0.68636 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 487 |  TrainLoss: 0.00289 | TestLoss: 0.68771 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 488 |  TrainLoss: 0.00363 | TestLoss: 0.68842 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 489 |  TrainLoss: 0.00269 | TestLoss: 0.68883 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 490 |  TrainLoss: 0.00287 | TestLoss: 0.68901 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 491 |  TrainLoss: 0.00306 | TestLoss: 0.68940 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 492 |  TrainLoss: 0.00332 | TestLoss: 0.69065 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 493 |  TrainLoss: 0.00364 | TestLoss: 0.69040 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 494 |  TrainLoss: 0.00352 | TestLoss: 0.69073 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 495 |  TrainLoss: 0.00273 | TestLoss: 0.69139 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 496 |  TrainLoss: 0.00256 | TestLoss: 0.69241 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 497 |  TrainLoss: 0.00325 | TestLoss: 0.69485 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 498 |  TrainLoss: 0.00266 | TestLoss: 0.69813 | TestAcc: 0.86425 | TestF1: 0.86\n",
      "Epoch: 499 |  TrainLoss: 0.00399 | TestLoss: 0.70311 | TestAcc: 0.86425 | TestF1: 0.86\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wloss = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "weighted_loss = 0\n",
    "exp_param = 0.8\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(500):\n",
    "  train_loss = train(epoch)\n",
    "  test_loss, test_acc, test_f1 = test(epoch)\n",
    "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
    "  \n",
    "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
    "  train_losses.append(train_loss)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accs.append(test_acc)\n",
    "\n",
    "  if test_loss < best_test_loss:\n",
    "    best_test_loss = test_loss\n",
    "\n",
    "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
    "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pX159vqt9mE-"
   },
   "source": [
    "##Plot of Test Accuracy over best Loss and Best Accuracy Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b84UtVBJ7X0X",
    "outputId": "617c91b9-21ba-4d71-a855-5027d2f35d83"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accs, label='Test Accuracy')\n",
    "\n",
    "# Add legend and axis labels\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "\n",
    "# Add marker for best epoch\n",
    "best_epoch_loss = test_losses.index(min(test_losses))\n",
    "best_epoch_acc = test_accs.index(max(test_accs))\n",
    "plt.axvline(x=best_epoch_loss, color='r', linestyle='--', label='Best Loss Epoch')\n",
    "plt.axvline(x=best_epoch_acc, color='g', linestyle='--', label='Best Acc Epoch')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
