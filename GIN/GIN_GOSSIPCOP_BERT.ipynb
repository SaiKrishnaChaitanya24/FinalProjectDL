{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "x-3rfnKLMdDd",
      "metadata": {
        "id": "x-3rfnKLMdDd"
      },
      "source": [
        "#GIN Using Default values: gossipcop and BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9011698c",
      "metadata": {
        "id": "9011698c",
        "outputId": "95097e1a-e49c-46b9-d235-9c05732d889b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-scatter in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-sparse in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Requirement already satisfied: torch-geometric in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.8.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: pyparsing in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: requests in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: jinja2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0319ed74",
      "metadata": {
        "id": "0319ed74"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import UPFD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49aa550",
      "metadata": {
        "id": "c49aa550",
        "outputId": "6abb36c5-d1fc-45bd-8960-0b48f087e346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gossipcop Dataset\n",
            "Train Samples:  1092\n",
            "Validation Samples:  546\n",
            "Test Samples:  3826\n"
          ]
        }
      ],
      "source": [
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\",split=\"test\")\n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"val\")\n",
        "print(\"Gossipcop Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_gos))\n",
        "print(\"Validation Samples: \", len(val_data_gos))\n",
        "print(\"Test Samples: \", len(test_data_gos))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3961bd25",
      "metadata": {
        "id": "3961bd25",
        "outputId": "b7f43810-690d-435e-eea8-67e8e03b432a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          1, 70, 74],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "         73, 74, 75]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_gos[0].edge_index\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "EAH9Vh1QMrO_",
      "metadata": {
        "id": "EAH9Vh1QMrO_"
      },
      "source": [
        "##Loading Dataset Using DataLoader for train data and test data of gossipcop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513271a7",
      "metadata": {
        "id": "513271a7"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_data_gos, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_data_gos, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d33b4e",
      "metadata": {
        "id": "d0d33b4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from torch.nn import LeakyReLU\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "DTctEJjQM1AO",
      "metadata": {
        "id": "DTctEJjQM1AO"
      },
      "source": [
        "##Defining Architecture of GIN Using 3 GIN Convolutional layers  with 3 unit MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2739b8aa",
      "metadata": {
        "id": "2739b8aa"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU\n",
        "from torch_geometric.nn import GINConv, global_max_pool\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(Linear(in_channels, hidden_channels[0]), ReLU()))\n",
        "        self.conv2 = GINConv(Sequential(Linear(hidden_channels[0], hidden_channels[1]), ReLU()))\n",
        "        self.conv3 = GINConv(Sequential(Linear(hidden_channels[1], hidden_channels[2]), ReLU()))\n",
        "\n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
        "\n",
        "        # dropouts\n",
        "        self.dp1 = torch.nn.Dropout(0.2)\n",
        "        self.dp2 = torch.nn.Dropout(0.2)\n",
        "        self.dp3 = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = F.relu(h)\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp3(h)\n",
        "\n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033ed36e",
      "metadata": {
        "id": "033ed36e"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70692306",
      "metadata": {
        "id": "70692306",
        "outputId": "96c28399-a781-4225-e399-331621fb972d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #using adam optimiser and the learning rate as lr= 0.0001\n",
        "lossff = torch.nn.BCELoss() #binary cross entropy loss\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc3f00d",
      "metadata": {
        "id": "acc3f00d"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e77ee4d",
      "metadata": {
        "id": "0e77ee4d",
        "outputId": "ca307107-97ee-4d1a-c4ad-2507ad759522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69350 | TestLoss: 0.69252 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 01 |  TrainLoss: 0.69269 | TestLoss: 0.69242 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 02 |  TrainLoss: 0.69150 | TestLoss: 0.69114 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 03 |  TrainLoss: 0.69050 | TestLoss: 0.68923 | TestAcc: 0.49948 | TestF1: 0.00\n",
            "Epoch: 04 |  TrainLoss: 0.68802 | TestLoss: 0.68552 | TestAcc: 0.54025 | TestF1: 0.15\n",
            "Epoch: 05 |  TrainLoss: 0.68435 | TestLoss: 0.67798 | TestAcc: 0.70387 | TestF1: 0.61\n",
            "Epoch: 06 |  TrainLoss: 0.67672 | TestLoss: 0.66713 | TestAcc: 0.73732 | TestF1: 0.68\n",
            "Epoch: 07 |  TrainLoss: 0.66386 | TestLoss: 0.64675 | TestAcc: 0.75771 | TestF1: 0.74\n",
            "Epoch: 08 |  TrainLoss: 0.64235 | TestLoss: 0.61804 | TestAcc: 0.75274 | TestF1: 0.77\n",
            "Epoch: 09 |  TrainLoss: 0.61082 | TestLoss: 0.57300 | TestAcc: 0.76817 | TestF1: 0.78\n",
            "Epoch: 10 |  TrainLoss: 0.56792 | TestLoss: 0.52083 | TestAcc: 0.78254 | TestF1: 0.77\n",
            "Epoch: 11 |  TrainLoss: 0.51370 | TestLoss: 0.47002 | TestAcc: 0.79430 | TestF1: 0.78\n",
            "Epoch: 12 |  TrainLoss: 0.46221 | TestLoss: 0.46375 | TestAcc: 0.78542 | TestF1: 0.75\n",
            "Epoch: 13 |  TrainLoss: 0.42751 | TestLoss: 0.40029 | TestAcc: 0.81155 | TestF1: 0.81\n",
            "Epoch: 14 |  TrainLoss: 0.39490 | TestLoss: 0.41836 | TestAcc: 0.81286 | TestF1: 0.79\n",
            "Epoch: 15 |  TrainLoss: 0.37476 | TestLoss: 0.37404 | TestAcc: 0.83508 | TestF1: 0.82\n",
            "Epoch: 16 |  TrainLoss: 0.35068 | TestLoss: 0.44396 | TestAcc: 0.80110 | TestF1: 0.76\n",
            "Epoch: 17 |  TrainLoss: 0.36003 | TestLoss: 0.31083 | TestAcc: 0.87245 | TestF1: 0.87\n",
            "Epoch: 18 |  TrainLoss: 0.35831 | TestLoss: 0.36637 | TestAcc: 0.82645 | TestF1: 0.85\n",
            "Epoch: 19 |  TrainLoss: 0.32434 | TestLoss: 0.35740 | TestAcc: 0.84344 | TestF1: 0.82\n",
            "Epoch: 20 |  TrainLoss: 0.27575 | TestLoss: 0.27937 | TestAcc: 0.88395 | TestF1: 0.89\n",
            "Epoch: 21 |  TrainLoss: 0.26407 | TestLoss: 0.26146 | TestAcc: 0.89362 | TestF1: 0.89\n",
            "Epoch: 22 |  TrainLoss: 0.24555 | TestLoss: 0.29730 | TestAcc: 0.87350 | TestF1: 0.86\n",
            "Epoch: 23 |  TrainLoss: 0.24765 | TestLoss: 0.23399 | TestAcc: 0.90800 | TestF1: 0.91\n",
            "Epoch: 24 |  TrainLoss: 0.21901 | TestLoss: 0.22037 | TestAcc: 0.91532 | TestF1: 0.91\n",
            "Epoch: 25 |  TrainLoss: 0.20188 | TestLoss: 0.21066 | TestAcc: 0.91819 | TestF1: 0.92\n",
            "Epoch: 26 |  TrainLoss: 0.18030 | TestLoss: 0.22229 | TestAcc: 0.90826 | TestF1: 0.90\n",
            "Epoch: 27 |  TrainLoss: 0.17682 | TestLoss: 0.18783 | TestAcc: 0.93204 | TestF1: 0.93\n",
            "Epoch: 28 |  TrainLoss: 0.16552 | TestLoss: 0.18122 | TestAcc: 0.93283 | TestF1: 0.93\n",
            "Epoch: 29 |  TrainLoss: 0.19045 | TestLoss: 0.22342 | TestAcc: 0.91061 | TestF1: 0.90\n",
            "Epoch: 30 |  TrainLoss: 0.14000 | TestLoss: 0.17065 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 31 |  TrainLoss: 0.14365 | TestLoss: 0.17119 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 32 |  TrainLoss: 0.12800 | TestLoss: 0.15873 | TestAcc: 0.94119 | TestF1: 0.94\n",
            "Epoch: 33 |  TrainLoss: 0.11740 | TestLoss: 0.15559 | TestAcc: 0.94224 | TestF1: 0.94\n",
            "Epoch: 34 |  TrainLoss: 0.11926 | TestLoss: 0.19542 | TestAcc: 0.92237 | TestF1: 0.92\n",
            "Epoch: 35 |  TrainLoss: 0.12682 | TestLoss: 0.15136 | TestAcc: 0.94328 | TestF1: 0.94\n",
            "Epoch: 36 |  TrainLoss: 0.12376 | TestLoss: 0.15732 | TestAcc: 0.94119 | TestF1: 0.94\n",
            "Epoch: 37 |  TrainLoss: 0.12239 | TestLoss: 0.20460 | TestAcc: 0.91845 | TestF1: 0.91\n",
            "Epoch: 38 |  TrainLoss: 0.11296 | TestLoss: 0.17285 | TestAcc: 0.93988 | TestF1: 0.94\n",
            "Epoch: 39 |  TrainLoss: 0.12382 | TestLoss: 0.15687 | TestAcc: 0.94224 | TestF1: 0.94\n",
            "Epoch: 40 |  TrainLoss: 0.10232 | TestLoss: 0.15046 | TestAcc: 0.94407 | TestF1: 0.94\n",
            "Epoch: 41 |  TrainLoss: 0.10088 | TestLoss: 0.15747 | TestAcc: 0.94171 | TestF1: 0.94\n",
            "Epoch: 42 |  TrainLoss: 0.10307 | TestLoss: 0.23686 | TestAcc: 0.90617 | TestF1: 0.90\n",
            "Epoch: 43 |  TrainLoss: 0.13082 | TestLoss: 0.15911 | TestAcc: 0.94276 | TestF1: 0.94\n",
            "Epoch: 44 |  TrainLoss: 0.09334 | TestLoss: 0.15406 | TestAcc: 0.94276 | TestF1: 0.94\n",
            "Epoch: 45 |  TrainLoss: 0.09858 | TestLoss: 0.15482 | TestAcc: 0.94224 | TestF1: 0.94\n",
            "Epoch: 46 |  TrainLoss: 0.10962 | TestLoss: 0.15706 | TestAcc: 0.94485 | TestF1: 0.95\n",
            "Epoch: 47 |  TrainLoss: 0.10761 | TestLoss: 0.19461 | TestAcc: 0.92629 | TestF1: 0.92\n",
            "Epoch: 48 |  TrainLoss: 0.09465 | TestLoss: 0.15543 | TestAcc: 0.94354 | TestF1: 0.94\n",
            "Epoch: 49 |  TrainLoss: 0.09299 | TestLoss: 0.17250 | TestAcc: 0.93257 | TestF1: 0.93\n",
            "Epoch: 50 |  TrainLoss: 0.08181 | TestLoss: 0.14941 | TestAcc: 0.94616 | TestF1: 0.95\n",
            "Epoch: 51 |  TrainLoss: 0.06879 | TestLoss: 0.17305 | TestAcc: 0.93492 | TestF1: 0.93\n",
            "Epoch: 52 |  TrainLoss: 0.06865 | TestLoss: 0.15038 | TestAcc: 0.94459 | TestF1: 0.94\n",
            "Epoch: 53 |  TrainLoss: 0.07447 | TestLoss: 0.15299 | TestAcc: 0.94302 | TestF1: 0.94\n",
            "Epoch: 54 |  TrainLoss: 0.08122 | TestLoss: 0.17116 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 55 |  TrainLoss: 0.06338 | TestLoss: 0.15391 | TestAcc: 0.94459 | TestF1: 0.94\n",
            "Epoch: 56 |  TrainLoss: 0.06010 | TestLoss: 0.15540 | TestAcc: 0.94485 | TestF1: 0.94\n",
            "Epoch: 57 |  TrainLoss: 0.05450 | TestLoss: 0.16291 | TestAcc: 0.94302 | TestF1: 0.94\n",
            "Epoch: 58 |  TrainLoss: 0.05669 | TestLoss: 0.15770 | TestAcc: 0.94537 | TestF1: 0.95\n",
            "Epoch: 59 |  TrainLoss: 0.05740 | TestLoss: 0.16106 | TestAcc: 0.94433 | TestF1: 0.94\n",
            "Epoch: 60 |  TrainLoss: 0.05581 | TestLoss: 0.17796 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 61 |  TrainLoss: 0.06009 | TestLoss: 0.15930 | TestAcc: 0.94407 | TestF1: 0.94\n",
            "Epoch: 62 |  TrainLoss: 0.05213 | TestLoss: 0.17648 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 63 |  TrainLoss: 0.04905 | TestLoss: 0.16594 | TestAcc: 0.94250 | TestF1: 0.94\n",
            "Epoch: 64 |  TrainLoss: 0.04744 | TestLoss: 0.16936 | TestAcc: 0.94145 | TestF1: 0.94\n",
            "Epoch: 65 |  TrainLoss: 0.04612 | TestLoss: 0.17538 | TestAcc: 0.94041 | TestF1: 0.94\n",
            "Epoch: 66 |  TrainLoss: 0.04330 | TestLoss: 0.16641 | TestAcc: 0.94354 | TestF1: 0.94\n",
            "Epoch: 67 |  TrainLoss: 0.04323 | TestLoss: 0.16525 | TestAcc: 0.94616 | TestF1: 0.95\n",
            "Epoch: 68 |  TrainLoss: 0.04498 | TestLoss: 0.20375 | TestAcc: 0.93100 | TestF1: 0.93\n",
            "Epoch: 69 |  TrainLoss: 0.05128 | TestLoss: 0.17275 | TestAcc: 0.94119 | TestF1: 0.94\n",
            "Epoch: 70 |  TrainLoss: 0.04759 | TestLoss: 0.18763 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 71 |  TrainLoss: 0.04384 | TestLoss: 0.16914 | TestAcc: 0.94511 | TestF1: 0.95\n",
            "Epoch: 72 |  TrainLoss: 0.04186 | TestLoss: 0.16887 | TestAcc: 0.94564 | TestF1: 0.95\n",
            "Epoch: 73 |  TrainLoss: 0.03918 | TestLoss: 0.17862 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 74 |  TrainLoss: 0.03435 | TestLoss: 0.16994 | TestAcc: 0.94407 | TestF1: 0.94\n",
            "Epoch: 75 |  TrainLoss: 0.03232 | TestLoss: 0.18440 | TestAcc: 0.93962 | TestF1: 0.94\n",
            "Epoch: 76 |  TrainLoss: 0.02860 | TestLoss: 0.18905 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 77 |  TrainLoss: 0.03377 | TestLoss: 0.22713 | TestAcc: 0.92499 | TestF1: 0.92\n",
            "Epoch: 78 |  TrainLoss: 0.03192 | TestLoss: 0.17948 | TestAcc: 0.94433 | TestF1: 0.94\n",
            "Epoch: 79 |  TrainLoss: 0.02509 | TestLoss: 0.19667 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 80 |  TrainLoss: 0.02799 | TestLoss: 0.19084 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 81 |  TrainLoss: 0.03152 | TestLoss: 0.23920 | TestAcc: 0.92943 | TestF1: 0.93\n",
            "Epoch: 82 |  TrainLoss: 0.04383 | TestLoss: 0.17717 | TestAcc: 0.94485 | TestF1: 0.95\n",
            "Epoch: 83 |  TrainLoss: 0.05660 | TestLoss: 0.40845 | TestAcc: 0.87559 | TestF1: 0.86\n",
            "Epoch: 84 |  TrainLoss: 0.07521 | TestLoss: 0.17413 | TestAcc: 0.94511 | TestF1: 0.95\n",
            "Epoch: 85 |  TrainLoss: 0.04110 | TestLoss: 0.18046 | TestAcc: 0.93962 | TestF1: 0.94\n",
            "Epoch: 86 |  TrainLoss: 0.02889 | TestLoss: 0.16958 | TestAcc: 0.94328 | TestF1: 0.94\n",
            "Epoch: 87 |  TrainLoss: 0.02465 | TestLoss: 0.19465 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 88 |  TrainLoss: 0.02131 | TestLoss: 0.17626 | TestAcc: 0.94668 | TestF1: 0.95\n",
            "Epoch: 89 |  TrainLoss: 0.02449 | TestLoss: 0.19170 | TestAcc: 0.94015 | TestF1: 0.94\n",
            "Epoch: 90 |  TrainLoss: 0.01909 | TestLoss: 0.17909 | TestAcc: 0.94459 | TestF1: 0.94\n",
            "Epoch: 91 |  TrainLoss: 0.01638 | TestLoss: 0.18342 | TestAcc: 0.94511 | TestF1: 0.95\n",
            "Epoch: 92 |  TrainLoss: 0.01558 | TestLoss: 0.19145 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 93 |  TrainLoss: 0.01237 | TestLoss: 0.19587 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 94 |  TrainLoss: 0.01038 | TestLoss: 0.19475 | TestAcc: 0.94328 | TestF1: 0.94\n",
            "Epoch: 95 |  TrainLoss: 0.00699 | TestLoss: 0.19666 | TestAcc: 0.94276 | TestF1: 0.94\n",
            "Epoch: 96 |  TrainLoss: 0.00626 | TestLoss: 0.20638 | TestAcc: 0.94015 | TestF1: 0.94\n",
            "Epoch: 97 |  TrainLoss: 0.00627 | TestLoss: 0.20612 | TestAcc: 0.94224 | TestF1: 0.94\n",
            "Epoch: 98 |  TrainLoss: 0.00716 | TestLoss: 0.23382 | TestAcc: 0.93100 | TestF1: 0.93\n",
            "Epoch: 99 |  TrainLoss: 0.00505 | TestLoss: 0.22212 | TestAcc: 0.93988 | TestF1: 0.94\n",
            "Epoch: 100 |  TrainLoss: 0.00475 | TestLoss: 0.21988 | TestAcc: 0.94171 | TestF1: 0.94\n",
            "Epoch: 101 |  TrainLoss: 0.00412 | TestLoss: 0.22760 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 102 |  TrainLoss: 0.00369 | TestLoss: 0.22435 | TestAcc: 0.94067 | TestF1: 0.94\n",
            "Epoch: 103 |  TrainLoss: 0.00313 | TestLoss: 0.22604 | TestAcc: 0.94250 | TestF1: 0.94\n",
            "Epoch: 104 |  TrainLoss: 0.00385 | TestLoss: 0.24375 | TestAcc: 0.93335 | TestF1: 0.93\n",
            "Epoch: 105 |  TrainLoss: 0.00300 | TestLoss: 0.24137 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 106 |  TrainLoss: 0.00436 | TestLoss: 0.24652 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 107 |  TrainLoss: 0.00262 | TestLoss: 0.24416 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 108 |  TrainLoss: 0.00157 | TestLoss: 0.24096 | TestAcc: 0.94171 | TestF1: 0.94\n",
            "Epoch: 109 |  TrainLoss: 0.00177 | TestLoss: 0.25606 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 110 |  TrainLoss: 0.00171 | TestLoss: 0.24556 | TestAcc: 0.94171 | TestF1: 0.94\n",
            "Epoch: 111 |  TrainLoss: 0.00154 | TestLoss: 0.24986 | TestAcc: 0.93962 | TestF1: 0.94\n",
            "Epoch: 112 |  TrainLoss: 0.00158 | TestLoss: 0.26124 | TestAcc: 0.93466 | TestF1: 0.93\n",
            "Epoch: 113 |  TrainLoss: 0.00133 | TestLoss: 0.25247 | TestAcc: 0.94145 | TestF1: 0.94\n",
            "Epoch: 114 |  TrainLoss: 0.00101 | TestLoss: 0.25808 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 115 |  TrainLoss: 0.00105 | TestLoss: 0.25940 | TestAcc: 0.93962 | TestF1: 0.94\n",
            "Epoch: 116 |  TrainLoss: 0.00095 | TestLoss: 0.26236 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 117 |  TrainLoss: 0.00109 | TestLoss: 0.26459 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 118 |  TrainLoss: 0.00091 | TestLoss: 0.27006 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 119 |  TrainLoss: 0.00083 | TestLoss: 0.26401 | TestAcc: 0.94093 | TestF1: 0.94\n",
            "Epoch: 120 |  TrainLoss: 0.00088 | TestLoss: 0.26911 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 121 |  TrainLoss: 0.00141 | TestLoss: 0.26762 | TestAcc: 0.94067 | TestF1: 0.94\n",
            "Epoch: 122 |  TrainLoss: 0.00099 | TestLoss: 0.26708 | TestAcc: 0.94198 | TestF1: 0.94\n",
            "Epoch: 123 |  TrainLoss: 0.00076 | TestLoss: 0.28346 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 124 |  TrainLoss: 0.00071 | TestLoss: 0.28998 | TestAcc: 0.93283 | TestF1: 0.93\n",
            "Epoch: 125 |  TrainLoss: 0.00066 | TestLoss: 0.27142 | TestAcc: 0.94093 | TestF1: 0.94\n",
            "Epoch: 126 |  TrainLoss: 0.00084 | TestLoss: 0.27537 | TestAcc: 0.94093 | TestF1: 0.94\n",
            "Epoch: 127 |  TrainLoss: 0.00070 | TestLoss: 0.28899 | TestAcc: 0.93361 | TestF1: 0.93\n",
            "Epoch: 128 |  TrainLoss: 0.00075 | TestLoss: 0.27786 | TestAcc: 0.94119 | TestF1: 0.94\n",
            "Epoch: 129 |  TrainLoss: 0.00072 | TestLoss: 0.28316 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 130 |  TrainLoss: 0.00077 | TestLoss: 0.29421 | TestAcc: 0.93335 | TestF1: 0.93\n",
            "Epoch: 131 |  TrainLoss: 0.00077 | TestLoss: 0.27904 | TestAcc: 0.94250 | TestF1: 0.94\n",
            "Epoch: 132 |  TrainLoss: 0.00054 | TestLoss: 0.28236 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 133 |  TrainLoss: 0.00046 | TestLoss: 0.30144 | TestAcc: 0.93283 | TestF1: 0.93\n",
            "Epoch: 134 |  TrainLoss: 0.00050 | TestLoss: 0.29250 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 135 |  TrainLoss: 0.00043 | TestLoss: 0.28836 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 136 |  TrainLoss: 0.00043 | TestLoss: 0.28647 | TestAcc: 0.94224 | TestF1: 0.94\n",
            "Epoch: 137 |  TrainLoss: 0.00044 | TestLoss: 0.28926 | TestAcc: 0.94067 | TestF1: 0.94\n",
            "Epoch: 138 |  TrainLoss: 0.00047 | TestLoss: 0.30503 | TestAcc: 0.93387 | TestF1: 0.93\n",
            "Epoch: 139 |  TrainLoss: 0.00045 | TestLoss: 0.30039 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 140 |  TrainLoss: 0.00035 | TestLoss: 0.29503 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 141 |  TrainLoss: 0.00039 | TestLoss: 0.30125 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 142 |  TrainLoss: 0.00039 | TestLoss: 0.30542 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 143 |  TrainLoss: 0.00042 | TestLoss: 0.30452 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 144 |  TrainLoss: 0.00030 | TestLoss: 0.30012 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 145 |  TrainLoss: 0.00035 | TestLoss: 0.30086 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 146 |  TrainLoss: 0.00035 | TestLoss: 0.30579 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 147 |  TrainLoss: 0.00033 | TestLoss: 0.30688 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 148 |  TrainLoss: 0.00028 | TestLoss: 0.30498 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 149 |  TrainLoss: 0.00028 | TestLoss: 0.30660 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 150 |  TrainLoss: 0.00033 | TestLoss: 0.31562 | TestAcc: 0.93492 | TestF1: 0.93\n",
            "Epoch: 151 |  TrainLoss: 0.00030 | TestLoss: 0.30916 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 152 |  TrainLoss: 0.00031 | TestLoss: 0.30501 | TestAcc: 0.94145 | TestF1: 0.94\n",
            "Epoch: 153 |  TrainLoss: 0.00024 | TestLoss: 0.30737 | TestAcc: 0.93988 | TestF1: 0.94\n",
            "Epoch: 154 |  TrainLoss: 0.00023 | TestLoss: 0.31526 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 155 |  TrainLoss: 0.00022 | TestLoss: 0.31773 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 156 |  TrainLoss: 0.00023 | TestLoss: 0.31783 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 157 |  TrainLoss: 0.00028 | TestLoss: 0.31839 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 158 |  TrainLoss: 0.00024 | TestLoss: 0.31665 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 159 |  TrainLoss: 0.00023 | TestLoss: 0.31718 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 160 |  TrainLoss: 0.00036 | TestLoss: 0.31811 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 161 |  TrainLoss: 0.00017 | TestLoss: 0.31631 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 162 |  TrainLoss: 0.00023 | TestLoss: 0.31583 | TestAcc: 0.94067 | TestF1: 0.94\n",
            "Epoch: 163 |  TrainLoss: 0.00022 | TestLoss: 0.31921 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 164 |  TrainLoss: 0.00023 | TestLoss: 0.32734 | TestAcc: 0.93466 | TestF1: 0.93\n",
            "Epoch: 165 |  TrainLoss: 0.00015 | TestLoss: 0.32698 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 166 |  TrainLoss: 0.00018 | TestLoss: 0.32184 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 167 |  TrainLoss: 0.00014 | TestLoss: 0.32016 | TestAcc: 0.94067 | TestF1: 0.94\n",
            "Epoch: 168 |  TrainLoss: 0.00017 | TestLoss: 0.32398 | TestAcc: 0.93988 | TestF1: 0.94\n",
            "Epoch: 169 |  TrainLoss: 0.00012 | TestLoss: 0.32951 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 170 |  TrainLoss: 0.00014 | TestLoss: 0.33214 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 171 |  TrainLoss: 0.00017 | TestLoss: 0.33221 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 172 |  TrainLoss: 0.00015 | TestLoss: 0.32721 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 173 |  TrainLoss: 0.00014 | TestLoss: 0.32568 | TestAcc: 0.94067 | TestF1: 0.94\n",
            "Epoch: 174 |  TrainLoss: 0.00014 | TestLoss: 0.32758 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 175 |  TrainLoss: 0.00015 | TestLoss: 0.33347 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 176 |  TrainLoss: 0.00016 | TestLoss: 0.33682 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 177 |  TrainLoss: 0.00017 | TestLoss: 0.33325 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 178 |  TrainLoss: 0.00016 | TestLoss: 0.33399 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 179 |  TrainLoss: 0.00015 | TestLoss: 0.33825 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 180 |  TrainLoss: 0.00013 | TestLoss: 0.33966 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 181 |  TrainLoss: 0.00016 | TestLoss: 0.33484 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 182 |  TrainLoss: 0.00015 | TestLoss: 0.33831 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 183 |  TrainLoss: 0.00012 | TestLoss: 0.34039 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 184 |  TrainLoss: 0.00014 | TestLoss: 0.34217 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 185 |  TrainLoss: 0.00013 | TestLoss: 0.34289 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 186 |  TrainLoss: 0.00011 | TestLoss: 0.34243 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 187 |  TrainLoss: 0.00012 | TestLoss: 0.34093 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 188 |  TrainLoss: 0.00017 | TestLoss: 0.33860 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 189 |  TrainLoss: 0.00013 | TestLoss: 0.34135 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 190 |  TrainLoss: 0.00012 | TestLoss: 0.34802 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 191 |  TrainLoss: 0.00012 | TestLoss: 0.34553 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 192 |  TrainLoss: 0.00010 | TestLoss: 0.34264 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 193 |  TrainLoss: 0.00009 | TestLoss: 0.34045 | TestAcc: 0.94015 | TestF1: 0.94\n",
            "Epoch: 194 |  TrainLoss: 0.00026 | TestLoss: 0.35808 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 195 |  TrainLoss: 0.00016 | TestLoss: 0.36360 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 196 |  TrainLoss: 0.00012 | TestLoss: 0.35231 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 197 |  TrainLoss: 0.00010 | TestLoss: 0.34448 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 198 |  TrainLoss: 0.00010 | TestLoss: 0.34384 | TestAcc: 0.94015 | TestF1: 0.94\n",
            "Epoch: 199 |  TrainLoss: 0.00009 | TestLoss: 0.34634 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 200 |  TrainLoss: 0.00010 | TestLoss: 0.35442 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 201 |  TrainLoss: 0.00009 | TestLoss: 0.35786 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 202 |  TrainLoss: 0.00012 | TestLoss: 0.35056 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 203 |  TrainLoss: 0.00009 | TestLoss: 0.34982 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 204 |  TrainLoss: 0.00015 | TestLoss: 0.35684 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 205 |  TrainLoss: 0.00014 | TestLoss: 0.36615 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 206 |  TrainLoss: 0.00008 | TestLoss: 0.35669 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 207 |  TrainLoss: 0.00009 | TestLoss: 0.35321 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 208 |  TrainLoss: 0.00006 | TestLoss: 0.35140 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 209 |  TrainLoss: 0.00009 | TestLoss: 0.35386 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 210 |  TrainLoss: 0.00008 | TestLoss: 0.36133 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 211 |  TrainLoss: 0.00009 | TestLoss: 0.36034 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 212 |  TrainLoss: 0.00007 | TestLoss: 0.35846 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 213 |  TrainLoss: 0.00008 | TestLoss: 0.35764 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 214 |  TrainLoss: 0.00007 | TestLoss: 0.35655 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 215 |  TrainLoss: 0.00009 | TestLoss: 0.35999 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 216 |  TrainLoss: 0.00007 | TestLoss: 0.36091 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 217 |  TrainLoss: 0.00009 | TestLoss: 0.35759 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 218 |  TrainLoss: 0.00008 | TestLoss: 0.35689 | TestAcc: 0.93962 | TestF1: 0.94\n",
            "Epoch: 219 |  TrainLoss: 0.00007 | TestLoss: 0.36001 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 220 |  TrainLoss: 0.00006 | TestLoss: 0.36455 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 221 |  TrainLoss: 0.00006 | TestLoss: 0.36738 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 222 |  TrainLoss: 0.00007 | TestLoss: 0.36490 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 223 |  TrainLoss: 0.00006 | TestLoss: 0.36205 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 224 |  TrainLoss: 0.00007 | TestLoss: 0.36234 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 225 |  TrainLoss: 0.00006 | TestLoss: 0.36365 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 226 |  TrainLoss: 0.00005 | TestLoss: 0.36565 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 227 |  TrainLoss: 0.00007 | TestLoss: 0.36622 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 228 |  TrainLoss: 0.00007 | TestLoss: 0.36541 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 229 |  TrainLoss: 0.00007 | TestLoss: 0.36756 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 230 |  TrainLoss: 0.00006 | TestLoss: 0.37068 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 231 |  TrainLoss: 0.00004 | TestLoss: 0.37077 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 232 |  TrainLoss: 0.00006 | TestLoss: 0.36978 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 233 |  TrainLoss: 0.00006 | TestLoss: 0.36768 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 234 |  TrainLoss: 0.00005 | TestLoss: 0.36688 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 235 |  TrainLoss: 0.00005 | TestLoss: 0.36983 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 236 |  TrainLoss: 0.00008 | TestLoss: 0.37665 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 237 |  TrainLoss: 0.00005 | TestLoss: 0.37853 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 238 |  TrainLoss: 0.00008 | TestLoss: 0.37026 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 239 |  TrainLoss: 0.00007 | TestLoss: 0.36493 | TestAcc: 0.94198 | TestF1: 0.94\n",
            "Epoch: 240 |  TrainLoss: 0.00007 | TestLoss: 0.36560 | TestAcc: 0.94119 | TestF1: 0.94\n",
            "Epoch: 241 |  TrainLoss: 0.00007 | TestLoss: 0.37704 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 242 |  TrainLoss: 0.00007 | TestLoss: 0.38927 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 243 |  TrainLoss: 0.00007 | TestLoss: 0.38840 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 244 |  TrainLoss: 0.00005 | TestLoss: 0.37768 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 245 |  TrainLoss: 0.00005 | TestLoss: 0.37262 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 246 |  TrainLoss: 0.00007 | TestLoss: 0.37236 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 247 |  TrainLoss: 0.00005 | TestLoss: 0.37597 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 248 |  TrainLoss: 0.00006 | TestLoss: 0.37635 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 249 |  TrainLoss: 0.00005 | TestLoss: 0.37756 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 250 |  TrainLoss: 0.00004 | TestLoss: 0.38035 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 251 |  TrainLoss: 0.00004 | TestLoss: 0.38246 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 252 |  TrainLoss: 0.00005 | TestLoss: 0.38025 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 253 |  TrainLoss: 0.00004 | TestLoss: 0.37904 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 254 |  TrainLoss: 0.00006 | TestLoss: 0.38090 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 255 |  TrainLoss: 0.00003 | TestLoss: 0.38366 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 256 |  TrainLoss: 0.00005 | TestLoss: 0.39143 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 257 |  TrainLoss: 0.00004 | TestLoss: 0.39375 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 258 |  TrainLoss: 0.00005 | TestLoss: 0.39263 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 259 |  TrainLoss: 0.00005 | TestLoss: 0.38704 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 260 |  TrainLoss: 0.00003 | TestLoss: 0.38363 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 261 |  TrainLoss: 0.00003 | TestLoss: 0.38205 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 262 |  TrainLoss: 0.00004 | TestLoss: 0.38217 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 263 |  TrainLoss: 0.00004 | TestLoss: 0.38426 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 264 |  TrainLoss: 0.00003 | TestLoss: 0.38589 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 265 |  TrainLoss: 0.00004 | TestLoss: 0.38767 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 266 |  TrainLoss: 0.00003 | TestLoss: 0.38695 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 267 |  TrainLoss: 0.00004 | TestLoss: 0.38657 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 268 |  TrainLoss: 0.00005 | TestLoss: 0.39031 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 269 |  TrainLoss: 0.00003 | TestLoss: 0.39168 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 270 |  TrainLoss: 0.00003 | TestLoss: 0.38994 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 271 |  TrainLoss: 0.00003 | TestLoss: 0.38860 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 272 |  TrainLoss: 0.00004 | TestLoss: 0.38641 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 273 |  TrainLoss: 0.00004 | TestLoss: 0.38710 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 274 |  TrainLoss: 0.00003 | TestLoss: 0.38843 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 275 |  TrainLoss: 0.00004 | TestLoss: 0.39013 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 276 |  TrainLoss: 0.00003 | TestLoss: 0.39408 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 277 |  TrainLoss: 0.00004 | TestLoss: 0.39389 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 278 |  TrainLoss: 0.00003 | TestLoss: 0.39181 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 279 |  TrainLoss: 0.00003 | TestLoss: 0.39134 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 280 |  TrainLoss: 0.00004 | TestLoss: 0.39387 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 281 |  TrainLoss: 0.00004 | TestLoss: 0.39744 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 282 |  TrainLoss: 0.00004 | TestLoss: 0.40019 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 283 |  TrainLoss: 0.00003 | TestLoss: 0.39631 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 284 |  TrainLoss: 0.00003 | TestLoss: 0.39511 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 285 |  TrainLoss: 0.00002 | TestLoss: 0.39977 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 286 |  TrainLoss: 0.00003 | TestLoss: 0.40108 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 287 |  TrainLoss: 0.00004 | TestLoss: 0.40514 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 288 |  TrainLoss: 0.00004 | TestLoss: 0.40670 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 289 |  TrainLoss: 0.00003 | TestLoss: 0.40221 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 290 |  TrainLoss: 0.00002 | TestLoss: 0.39908 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 291 |  TrainLoss: 0.00002 | TestLoss: 0.40054 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 292 |  TrainLoss: 0.00002 | TestLoss: 0.40433 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 293 |  TrainLoss: 0.00003 | TestLoss: 0.40705 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 294 |  TrainLoss: 0.00002 | TestLoss: 0.40651 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 295 |  TrainLoss: 0.00004 | TestLoss: 0.39941 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 296 |  TrainLoss: 0.00004 | TestLoss: 0.39841 | TestAcc: 0.93910 | TestF1: 0.94\n",
            "Epoch: 297 |  TrainLoss: 0.00004 | TestLoss: 0.40451 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 298 |  TrainLoss: 0.00003 | TestLoss: 0.41336 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 299 |  TrainLoss: 0.00003 | TestLoss: 0.41480 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 300 |  TrainLoss: 0.00005 | TestLoss: 0.40351 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 301 |  TrainLoss: 0.00003 | TestLoss: 0.39736 | TestAcc: 0.93962 | TestF1: 0.94\n",
            "Epoch: 302 |  TrainLoss: 0.00003 | TestLoss: 0.39649 | TestAcc: 0.94015 | TestF1: 0.94\n",
            "Epoch: 303 |  TrainLoss: 0.00002 | TestLoss: 0.39857 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 304 |  TrainLoss: 0.00004 | TestLoss: 0.40646 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 305 |  TrainLoss: 0.00003 | TestLoss: 0.41388 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 306 |  TrainLoss: 0.00003 | TestLoss: 0.41525 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 307 |  TrainLoss: 0.00003 | TestLoss: 0.41419 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 308 |  TrainLoss: 0.00002 | TestLoss: 0.40927 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 309 |  TrainLoss: 0.00003 | TestLoss: 0.40632 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 310 |  TrainLoss: 0.00002 | TestLoss: 0.40496 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 311 |  TrainLoss: 0.00004 | TestLoss: 0.40844 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 312 |  TrainLoss: 0.00002 | TestLoss: 0.41044 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 313 |  TrainLoss: 0.00002 | TestLoss: 0.41101 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 314 |  TrainLoss: 0.00003 | TestLoss: 0.41175 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 315 |  TrainLoss: 0.00002 | TestLoss: 0.41146 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 316 |  TrainLoss: 0.00002 | TestLoss: 0.41050 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 317 |  TrainLoss: 0.00002 | TestLoss: 0.41080 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 318 |  TrainLoss: 0.00002 | TestLoss: 0.41005 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 319 |  TrainLoss: 0.00003 | TestLoss: 0.41185 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 320 |  TrainLoss: 0.00002 | TestLoss: 0.41768 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 321 |  TrainLoss: 0.00002 | TestLoss: 0.42031 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 322 |  TrainLoss: 0.00003 | TestLoss: 0.41696 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 323 |  TrainLoss: 0.00002 | TestLoss: 0.41431 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 324 |  TrainLoss: 0.00006 | TestLoss: 0.42226 | TestAcc: 0.93570 | TestF1: 0.93\n",
            "Epoch: 325 |  TrainLoss: 0.00003 | TestLoss: 0.42643 | TestAcc: 0.93361 | TestF1: 0.93\n",
            "Epoch: 326 |  TrainLoss: 0.00002 | TestLoss: 0.42297 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 327 |  TrainLoss: 0.00002 | TestLoss: 0.41883 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 328 |  TrainLoss: 0.00003 | TestLoss: 0.41580 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 329 |  TrainLoss: 0.00005 | TestLoss: 0.42617 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 330 |  TrainLoss: 0.00003 | TestLoss: 0.42989 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 331 |  TrainLoss: 0.00002 | TestLoss: 0.42562 | TestAcc: 0.93387 | TestF1: 0.93\n",
            "Epoch: 332 |  TrainLoss: 0.00002 | TestLoss: 0.42052 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 333 |  TrainLoss: 0.00003 | TestLoss: 0.41480 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 334 |  TrainLoss: 0.00002 | TestLoss: 0.41231 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 335 |  TrainLoss: 0.00002 | TestLoss: 0.41302 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 336 |  TrainLoss: 0.00002 | TestLoss: 0.41866 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 337 |  TrainLoss: 0.00001 | TestLoss: 0.42247 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 338 |  TrainLoss: 0.00001 | TestLoss: 0.42444 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 339 |  TrainLoss: 0.00002 | TestLoss: 0.42286 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 340 |  TrainLoss: 0.00001 | TestLoss: 0.42067 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 341 |  TrainLoss: 0.00001 | TestLoss: 0.41963 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 342 |  TrainLoss: 0.00002 | TestLoss: 0.41884 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 343 |  TrainLoss: 0.00001 | TestLoss: 0.41985 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 344 |  TrainLoss: 0.00002 | TestLoss: 0.42110 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 345 |  TrainLoss: 0.00001 | TestLoss: 0.42229 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 346 |  TrainLoss: 0.00001 | TestLoss: 0.42235 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 347 |  TrainLoss: 0.00002 | TestLoss: 0.42516 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 348 |  TrainLoss: 0.00001 | TestLoss: 0.42789 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 349 |  TrainLoss: 0.00002 | TestLoss: 0.43082 | TestAcc: 0.93466 | TestF1: 0.93\n",
            "Epoch: 350 |  TrainLoss: 0.00002 | TestLoss: 0.42970 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 351 |  TrainLoss: 0.00001 | TestLoss: 0.42723 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 352 |  TrainLoss: 0.00002 | TestLoss: 0.42646 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 353 |  TrainLoss: 0.00001 | TestLoss: 0.42684 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 354 |  TrainLoss: 0.00001 | TestLoss: 0.42775 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 355 |  TrainLoss: 0.00002 | TestLoss: 0.43292 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 356 |  TrainLoss: 0.00002 | TestLoss: 0.43279 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 357 |  TrainLoss: 0.00002 | TestLoss: 0.43061 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 358 |  TrainLoss: 0.00002 | TestLoss: 0.42718 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 359 |  TrainLoss: 0.00002 | TestLoss: 0.42686 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 360 |  TrainLoss: 0.00002 | TestLoss: 0.42920 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 361 |  TrainLoss: 0.00002 | TestLoss: 0.42938 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 362 |  TrainLoss: 0.00001 | TestLoss: 0.42923 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 363 |  TrainLoss: 0.00003 | TestLoss: 0.43449 | TestAcc: 0.93518 | TestF1: 0.93\n",
            "Epoch: 364 |  TrainLoss: 0.00001 | TestLoss: 0.45566 | TestAcc: 0.93361 | TestF1: 0.93\n",
            "Epoch: 365 |  TrainLoss: 0.00002 | TestLoss: 0.45875 | TestAcc: 0.93361 | TestF1: 0.93\n",
            "Epoch: 366 |  TrainLoss: 0.00003 | TestLoss: 0.44439 | TestAcc: 0.93387 | TestF1: 0.93\n",
            "Epoch: 367 |  TrainLoss: 0.00001 | TestLoss: 0.43269 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 368 |  TrainLoss: 0.00002 | TestLoss: 0.43515 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 369 |  TrainLoss: 0.00001 | TestLoss: 0.43658 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 370 |  TrainLoss: 0.00001 | TestLoss: 0.43752 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 371 |  TrainLoss: 0.00001 | TestLoss: 0.43692 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 372 |  TrainLoss: 0.00001 | TestLoss: 0.43576 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 373 |  TrainLoss: 0.00001 | TestLoss: 0.43528 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 374 |  TrainLoss: 0.00001 | TestLoss: 0.43483 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 375 |  TrainLoss: 0.00001 | TestLoss: 0.43526 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 376 |  TrainLoss: 0.00001 | TestLoss: 0.43678 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 377 |  TrainLoss: 0.00001 | TestLoss: 0.43961 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 378 |  TrainLoss: 0.00001 | TestLoss: 0.44181 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 379 |  TrainLoss: 0.00001 | TestLoss: 0.44155 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 380 |  TrainLoss: 0.00001 | TestLoss: 0.44123 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 381 |  TrainLoss: 0.00001 | TestLoss: 0.43931 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 382 |  TrainLoss: 0.00001 | TestLoss: 0.43912 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 383 |  TrainLoss: 0.00001 | TestLoss: 0.44227 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 384 |  TrainLoss: 0.00002 | TestLoss: 0.44447 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 385 |  TrainLoss: 0.00001 | TestLoss: 0.44403 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 386 |  TrainLoss: 0.00001 | TestLoss: 0.44222 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 387 |  TrainLoss: 0.00001 | TestLoss: 0.44060 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 388 |  TrainLoss: 0.00001 | TestLoss: 0.44173 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 389 |  TrainLoss: 0.00001 | TestLoss: 0.44291 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 390 |  TrainLoss: 0.00001 | TestLoss: 0.44525 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 391 |  TrainLoss: 0.00002 | TestLoss: 0.44911 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 392 |  TrainLoss: 0.00001 | TestLoss: 0.44751 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 393 |  TrainLoss: 0.00001 | TestLoss: 0.44701 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 394 |  TrainLoss: 0.00001 | TestLoss: 0.44577 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 395 |  TrainLoss: 0.00001 | TestLoss: 0.44475 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 396 |  TrainLoss: 0.00001 | TestLoss: 0.44517 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 397 |  TrainLoss: 0.00001 | TestLoss: 0.44684 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 398 |  TrainLoss: 0.00003 | TestLoss: 0.46298 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 399 |  TrainLoss: 0.00001 | TestLoss: 0.46210 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 400 |  TrainLoss: 0.00001 | TestLoss: 0.46453 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 401 |  TrainLoss: 0.00001 | TestLoss: 0.44623 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 402 |  TrainLoss: 0.00001 | TestLoss: 0.44961 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 403 |  TrainLoss: 0.00001 | TestLoss: 0.45056 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 404 |  TrainLoss: 0.00001 | TestLoss: 0.45177 | TestAcc: 0.93544 | TestF1: 0.93\n",
            "Epoch: 405 |  TrainLoss: 0.00001 | TestLoss: 0.45019 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 406 |  TrainLoss: 0.00001 | TestLoss: 0.46905 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 407 |  TrainLoss: 0.00001 | TestLoss: 0.46684 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 408 |  TrainLoss: 0.00001 | TestLoss: 0.46796 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 409 |  TrainLoss: 0.00001 | TestLoss: 0.46830 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 410 |  TrainLoss: 0.00001 | TestLoss: 0.47028 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 411 |  TrainLoss: 0.00001 | TestLoss: 0.47266 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 412 |  TrainLoss: 0.00001 | TestLoss: 0.47339 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 413 |  TrainLoss: 0.00001 | TestLoss: 0.47304 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 414 |  TrainLoss: 0.00001 | TestLoss: 0.47499 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 415 |  TrainLoss: 0.00001 | TestLoss: 0.47452 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 416 |  TrainLoss: 0.00001 | TestLoss: 0.47522 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 417 |  TrainLoss: 0.00001 | TestLoss: 0.47825 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 418 |  TrainLoss: 0.00001 | TestLoss: 0.47811 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 419 |  TrainLoss: 0.00001 | TestLoss: 0.47643 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 420 |  TrainLoss: 0.00001 | TestLoss: 0.47463 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 421 |  TrainLoss: 0.00001 | TestLoss: 0.47521 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 422 |  TrainLoss: 0.00001 | TestLoss: 0.47697 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 423 |  TrainLoss: 0.00001 | TestLoss: 0.47701 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 424 |  TrainLoss: 0.00001 | TestLoss: 0.47631 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 425 |  TrainLoss: 0.00001 | TestLoss: 0.47629 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 426 |  TrainLoss: 0.00002 | TestLoss: 0.48402 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 427 |  TrainLoss: 0.00001 | TestLoss: 0.48705 | TestAcc: 0.93335 | TestF1: 0.93\n",
            "Epoch: 428 |  TrainLoss: 0.00003 | TestLoss: 0.48024 | TestAcc: 0.93387 | TestF1: 0.93\n",
            "Epoch: 429 |  TrainLoss: 0.00001 | TestLoss: 0.48199 | TestAcc: 0.93623 | TestF1: 0.94\n",
            "Epoch: 430 |  TrainLoss: 0.00001 | TestLoss: 0.47293 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 431 |  TrainLoss: 0.00001 | TestLoss: 0.47128 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 432 |  TrainLoss: 0.00001 | TestLoss: 0.47098 | TestAcc: 0.93858 | TestF1: 0.94\n",
            "Epoch: 433 |  TrainLoss: 0.00002 | TestLoss: 0.47958 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 434 |  TrainLoss: 0.00001 | TestLoss: 0.49120 | TestAcc: 0.93440 | TestF1: 0.93\n",
            "Epoch: 435 |  TrainLoss: 0.00001 | TestLoss: 0.49556 | TestAcc: 0.93413 | TestF1: 0.93\n",
            "Epoch: 436 |  TrainLoss: 0.00001 | TestLoss: 0.48577 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 437 |  TrainLoss: 0.00002 | TestLoss: 0.47971 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 438 |  TrainLoss: 0.00001 | TestLoss: 0.47717 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 439 |  TrainLoss: 0.00001 | TestLoss: 0.47621 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 440 |  TrainLoss: 0.00000 | TestLoss: 0.47641 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 441 |  TrainLoss: 0.00001 | TestLoss: 0.47780 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 442 |  TrainLoss: 0.00001 | TestLoss: 0.48108 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 443 |  TrainLoss: 0.00001 | TestLoss: 0.48453 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 444 |  TrainLoss: 0.00001 | TestLoss: 0.48808 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 445 |  TrainLoss: 0.00001 | TestLoss: 0.48633 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 446 |  TrainLoss: 0.00001 | TestLoss: 0.48186 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 447 |  TrainLoss: 0.00001 | TestLoss: 0.47946 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 448 |  TrainLoss: 0.00001 | TestLoss: 0.47823 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 449 |  TrainLoss: 0.00001 | TestLoss: 0.48115 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 450 |  TrainLoss: 0.00000 | TestLoss: 0.48532 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 451 |  TrainLoss: 0.00001 | TestLoss: 0.48755 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 452 |  TrainLoss: 0.00001 | TestLoss: 0.48874 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 453 |  TrainLoss: 0.00001 | TestLoss: 0.48874 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 454 |  TrainLoss: 0.00000 | TestLoss: 0.48885 | TestAcc: 0.93675 | TestF1: 0.94\n",
            "Epoch: 455 |  TrainLoss: 0.00001 | TestLoss: 0.48767 | TestAcc: 0.93701 | TestF1: 0.94\n",
            "Epoch: 456 |  TrainLoss: 0.00001 | TestLoss: 0.48524 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 457 |  TrainLoss: 0.00001 | TestLoss: 0.48538 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 458 |  TrainLoss: 0.00001 | TestLoss: 0.48603 | TestAcc: 0.93806 | TestF1: 0.94\n",
            "Epoch: 459 |  TrainLoss: 0.00001 | TestLoss: 0.48587 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 460 |  TrainLoss: 0.00001 | TestLoss: 0.48728 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 461 |  TrainLoss: 0.00001 | TestLoss: 0.48836 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 462 |  TrainLoss: 0.00001 | TestLoss: 0.48908 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 463 |  TrainLoss: 0.00001 | TestLoss: 0.48966 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 464 |  TrainLoss: 0.00001 | TestLoss: 0.49032 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 465 |  TrainLoss: 0.00000 | TestLoss: 0.49048 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 466 |  TrainLoss: 0.00000 | TestLoss: 0.49003 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 467 |  TrainLoss: 0.00000 | TestLoss: 0.49016 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 468 |  TrainLoss: 0.00001 | TestLoss: 0.49023 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 469 |  TrainLoss: 0.00001 | TestLoss: 0.49158 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 470 |  TrainLoss: 0.00001 | TestLoss: 0.49349 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 471 |  TrainLoss: 0.00001 | TestLoss: 0.49349 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 472 |  TrainLoss: 0.00000 | TestLoss: 0.49267 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 473 |  TrainLoss: 0.00001 | TestLoss: 0.49188 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 474 |  TrainLoss: 0.00001 | TestLoss: 0.49291 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 475 |  TrainLoss: 0.00001 | TestLoss: 0.49474 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 476 |  TrainLoss: 0.00001 | TestLoss: 0.49559 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 477 |  TrainLoss: 0.00001 | TestLoss: 0.49522 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 478 |  TrainLoss: 0.00001 | TestLoss: 0.49507 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 479 |  TrainLoss: 0.00001 | TestLoss: 0.49825 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 480 |  TrainLoss: 0.00000 | TestLoss: 0.49981 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 0.50003 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 482 |  TrainLoss: 0.00001 | TestLoss: 0.49860 | TestAcc: 0.93649 | TestF1: 0.94\n",
            "Epoch: 483 |  TrainLoss: 0.00001 | TestLoss: 0.49705 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 484 |  TrainLoss: 0.00001 | TestLoss: 0.49803 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 485 |  TrainLoss: 0.00001 | TestLoss: 0.50348 | TestAcc: 0.93596 | TestF1: 0.94\n",
            "Epoch: 486 |  TrainLoss: 0.00001 | TestLoss: 0.50824 | TestAcc: 0.93492 | TestF1: 0.93\n",
            "Epoch: 487 |  TrainLoss: 0.00001 | TestLoss: 0.50252 | TestAcc: 0.93570 | TestF1: 0.94\n",
            "Epoch: 488 |  TrainLoss: 0.00000 | TestLoss: 0.49876 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 489 |  TrainLoss: 0.00000 | TestLoss: 0.49629 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 490 |  TrainLoss: 0.00000 | TestLoss: 0.49530 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 491 |  TrainLoss: 0.00000 | TestLoss: 0.49574 | TestAcc: 0.93727 | TestF1: 0.94\n",
            "Epoch: 492 |  TrainLoss: 0.00001 | TestLoss: 0.49841 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 493 |  TrainLoss: 0.00000 | TestLoss: 0.49939 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 494 |  TrainLoss: 0.00001 | TestLoss: 0.49929 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 495 |  TrainLoss: 0.00000 | TestLoss: 0.49991 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 496 |  TrainLoss: 0.00000 | TestLoss: 0.49961 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 497 |  TrainLoss: 0.00000 | TestLoss: 0.49985 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 498 |  TrainLoss: 0.00000 | TestLoss: 0.49975 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 499 |  TrainLoss: 0.00001 | TestLoss: 0.49939 | TestAcc: 0.93753 | TestF1: 0.94\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wloss = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "\n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "NDOMu7AzM_fa",
      "metadata": {
        "id": "NDOMu7AzM_fa"
      },
      "source": [
        "##Plot of Test Accuracy over best Loss and Best Accuracy Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4cc4b53",
      "metadata": {
        "id": "d4cc4b53",
        "outputId": "2dd7351a-fff9-48a7-8adb-bd13cf98c3b6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(test_accs, label='Test Accuracy')\n",
        "\n",
        "# Add legend and axis labels\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "\n",
        "# Add marker for best epoch\n",
        "best_epoch_loss = test_losses.index(min(test_losses))\n",
        "best_epoch_acc = test_accs.index(max(test_accs))\n",
        "plt.axvline(x=best_epoch_loss, color='r', linestyle='--', label='Best Loss Epoch')\n",
        "plt.axvline(x=best_epoch_acc, color='g', linestyle='--', label='Best Acc Epoch')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
