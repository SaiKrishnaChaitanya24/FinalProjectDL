{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "RUlKe2ttEB0g",
      "metadata": {
        "id": "RUlKe2ttEB0g"
      },
      "source": [
        "#GIN Using Default values: Politifact and CONTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270a9d35",
      "metadata": {
        "id": "270a9d35",
        "outputId": "fa198d5c-775c-49ea-dd88-3e23c86efd79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-scatter in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Requirement already satisfied: torch-sparse in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (0.6.17)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Requirement already satisfied: torch-geometric in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (2.4.0)\n",
            "Requirement already satisfied: pyparsing in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (5.8.0)\n",
            "Requirement already satisfied: requests in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: scipy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: numpy in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/krishnachaitanya_annavazala/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e304952d",
      "metadata": {
        "id": "e304952d"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import UPFD #importing the UPFD Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163f9ae1",
      "metadata": {
        "id": "163f9ae1",
        "outputId": "f3c66a35-e22a-4944-c173-6a3170356d5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gossipcop Dataset\n",
            "Train Samples:  1092\n",
            "Validation Samples:  546\n",
            "Test Samples:  3826\n",
            "Politifact Dataset\n",
            "Train Samples:  93\n",
            "Validation Samples:  31\n",
            "Test Samples:  221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\",split=\"test\")\n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"val\")\n",
        "\n",
        "test_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"content\",split=\"test\")\n",
        "train_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"content\", split=\"train\")\n",
        "val_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"content\", split=\"val\")\n",
        "train_data_pol = train_data_pol + val_data_pol\n",
        "\n",
        "print(\"Gossipcop Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_gos))\n",
        "print(\"Validation Samples: \", len(val_data_gos))\n",
        "print(\"Test Samples: \", len(test_data_gos))\n",
        "\n",
        "print(\"Politifact Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_pol))\n",
        "print(\"Validation Samples: \", len(val_data_pol))\n",
        "print(\"Test Samples: \", len(test_data_pol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7efc2a2",
      "metadata": {
        "id": "f7efc2a2",
        "outputId": "e01b845d-be11-4b5d-949f-b13468bca4ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  8, 16, 16, 16, 16, 16, 16,\n",
              "         24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
              "         24, 24, 24, 24, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 60],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_pol[0].edge_index\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "mYu2wXhUEW-S",
      "metadata": {
        "id": "mYu2wXhUEW-S"
      },
      "source": [
        "##Loading Dataset Using DataLoader for train data and test data of Politifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e07a3f",
      "metadata": {
        "id": "57e07a3f"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_data_pol, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_data_pol, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6150d8",
      "metadata": {
        "id": "7b6150d8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from torch.nn import LeakyReLU\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "X9u1MImLEe-C",
      "metadata": {
        "id": "X9u1MImLEe-C"
      },
      "source": [
        "##Defining Architecture of GIN Using 3 GIN Convolutional layers  with 3 unit MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594fc4c6",
      "metadata": {
        "id": "594fc4c6"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU\n",
        "from torch_geometric.nn import GINConv, global_max_pool\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(Linear(in_channels, hidden_channels[0]), ReLU()))\n",
        "        self.conv2 = GINConv(Sequential(Linear(hidden_channels[0], hidden_channels[1]), ReLU()))\n",
        "        self.conv3 = GINConv(Sequential(Linear(hidden_channels[1], hidden_channels[2]), ReLU()))\n",
        "\n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
        "\n",
        "        # dropouts\n",
        "        self.dp1 = torch.nn.Dropout(0.2)\n",
        "        self.dp2 = torch.nn.Dropout(0.2)\n",
        "        self.dp3 = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = F.relu(h)\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h)\n",
        "        h = F.relu(h)\n",
        "        h = self.dp3(h)\n",
        "\n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557fe119",
      "metadata": {
        "id": "557fe119"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "zfzgIyfoE3Tf",
      "metadata": {
        "id": "zfzgIyfoE3Tf"
      },
      "source": [
        "##Train and Test of model using Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d46c458",
      "metadata": {
        "id": "1d46c458",
        "outputId": "d6f5d192-838e-481a-9a50-7fc672e8ff13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  #using adam optimiser and the learning rate as lr= 0.0001\n",
        "lossff = torch.nn.BCELoss() #binary cross entropy loss\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879162a9",
      "metadata": {
        "id": "879162a9"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86441897",
      "metadata": {
        "id": "86441897",
        "outputId": "930b0722-8013-4ee2-dbb0-2269318051fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69545 | TestLoss: 0.69309 | TestAcc: 0.51131 | TestF1: 0.68\n",
            "Epoch: 01 |  TrainLoss: 0.69405 | TestLoss: 0.69318 | TestAcc: 0.51131 | TestF1: 0.68\n",
            "Epoch: 02 |  TrainLoss: 0.69343 | TestLoss: 0.69328 | TestAcc: 0.48869 | TestF1: 0.60\n",
            "Epoch: 03 |  TrainLoss: 0.69299 | TestLoss: 0.69346 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 04 |  TrainLoss: 0.69399 | TestLoss: 0.69370 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 05 |  TrainLoss: 0.69252 | TestLoss: 0.69392 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 06 |  TrainLoss: 0.69168 | TestLoss: 0.69408 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 07 |  TrainLoss: 0.69013 | TestLoss: 0.69436 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 08 |  TrainLoss: 0.69084 | TestLoss: 0.69471 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 09 |  TrainLoss: 0.69069 | TestLoss: 0.69504 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 10 |  TrainLoss: 0.69243 | TestLoss: 0.69535 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 11 |  TrainLoss: 0.69326 | TestLoss: 0.69550 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 12 |  TrainLoss: 0.68909 | TestLoss: 0.69576 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 13 |  TrainLoss: 0.69043 | TestLoss: 0.69596 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 14 |  TrainLoss: 0.69153 | TestLoss: 0.69617 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 15 |  TrainLoss: 0.69138 | TestLoss: 0.69634 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 16 |  TrainLoss: 0.68930 | TestLoss: 0.69657 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 17 |  TrainLoss: 0.69221 | TestLoss: 0.69670 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 18 |  TrainLoss: 0.68931 | TestLoss: 0.69664 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 19 |  TrainLoss: 0.68936 | TestLoss: 0.69646 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 20 |  TrainLoss: 0.69001 | TestLoss: 0.69623 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 21 |  TrainLoss: 0.69025 | TestLoss: 0.69611 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 22 |  TrainLoss: 0.69262 | TestLoss: 0.69587 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 23 |  TrainLoss: 0.69038 | TestLoss: 0.69558 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 24 |  TrainLoss: 0.69163 | TestLoss: 0.69507 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 25 |  TrainLoss: 0.69050 | TestLoss: 0.69466 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 26 |  TrainLoss: 0.69085 | TestLoss: 0.69433 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 27 |  TrainLoss: 0.68685 | TestLoss: 0.69410 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 28 |  TrainLoss: 0.69321 | TestLoss: 0.69380 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 29 |  TrainLoss: 0.69029 | TestLoss: 0.69339 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 30 |  TrainLoss: 0.68798 | TestLoss: 0.69304 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 31 |  TrainLoss: 0.68976 | TestLoss: 0.69267 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 32 |  TrainLoss: 0.68876 | TestLoss: 0.69231 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 33 |  TrainLoss: 0.68535 | TestLoss: 0.69211 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 34 |  TrainLoss: 0.68545 | TestLoss: 0.69196 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 35 |  TrainLoss: 0.68690 | TestLoss: 0.69162 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 36 |  TrainLoss: 0.68714 | TestLoss: 0.69135 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 37 |  TrainLoss: 0.68468 | TestLoss: 0.69142 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 38 |  TrainLoss: 0.68845 | TestLoss: 0.69117 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 39 |  TrainLoss: 0.68737 | TestLoss: 0.69085 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 40 |  TrainLoss: 0.68835 | TestLoss: 0.68980 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 41 |  TrainLoss: 0.68323 | TestLoss: 0.68887 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 42 |  TrainLoss: 0.68651 | TestLoss: 0.68786 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 43 |  TrainLoss: 0.68153 | TestLoss: 0.68717 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 44 |  TrainLoss: 0.68138 | TestLoss: 0.68625 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 45 |  TrainLoss: 0.68014 | TestLoss: 0.68561 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 46 |  TrainLoss: 0.67874 | TestLoss: 0.68441 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 47 |  TrainLoss: 0.67745 | TestLoss: 0.68249 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 48 |  TrainLoss: 0.67752 | TestLoss: 0.68123 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 49 |  TrainLoss: 0.67871 | TestLoss: 0.67992 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 50 |  TrainLoss: 0.67422 | TestLoss: 0.67919 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 51 |  TrainLoss: 0.67611 | TestLoss: 0.67828 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 52 |  TrainLoss: 0.67290 | TestLoss: 0.67499 | TestAcc: 0.49774 | TestF1: 0.03\n",
            "Epoch: 53 |  TrainLoss: 0.66869 | TestLoss: 0.67314 | TestAcc: 0.50226 | TestF1: 0.05\n",
            "Epoch: 54 |  TrainLoss: 0.66575 | TestLoss: 0.67106 | TestAcc: 0.50679 | TestF1: 0.07\n",
            "Epoch: 55 |  TrainLoss: 0.65997 | TestLoss: 0.66735 | TestAcc: 0.61538 | TestF1: 0.40\n",
            "Epoch: 56 |  TrainLoss: 0.66247 | TestLoss: 0.66662 | TestAcc: 0.51131 | TestF1: 0.08\n",
            "Epoch: 57 |  TrainLoss: 0.65498 | TestLoss: 0.66364 | TestAcc: 0.53394 | TestF1: 0.16\n",
            "Epoch: 58 |  TrainLoss: 0.65108 | TestLoss: 0.65714 | TestAcc: 0.70588 | TestF1: 0.63\n",
            "Epoch: 59 |  TrainLoss: 0.64853 | TestLoss: 0.65323 | TestAcc: 0.71946 | TestF1: 0.65\n",
            "Epoch: 60 |  TrainLoss: 0.64351 | TestLoss: 0.65318 | TestAcc: 0.60633 | TestF1: 0.37\n",
            "Epoch: 61 |  TrainLoss: 0.64231 | TestLoss: 0.64648 | TestAcc: 0.67421 | TestF1: 0.56\n",
            "Epoch: 62 |  TrainLoss: 0.62910 | TestLoss: 0.63922 | TestAcc: 0.76923 | TestF1: 0.73\n",
            "Epoch: 63 |  TrainLoss: 0.63354 | TestLoss: 0.63723 | TestAcc: 0.67421 | TestF1: 0.56\n",
            "Epoch: 64 |  TrainLoss: 0.62868 | TestLoss: 0.62737 | TestAcc: 0.77376 | TestF1: 0.74\n",
            "Epoch: 65 |  TrainLoss: 0.61284 | TestLoss: 0.62560 | TestAcc: 0.68326 | TestF1: 0.58\n",
            "Epoch: 66 |  TrainLoss: 0.60523 | TestLoss: 0.61812 | TestAcc: 0.69231 | TestF1: 0.60\n",
            "Epoch: 67 |  TrainLoss: 0.59956 | TestLoss: 0.60586 | TestAcc: 0.81448 | TestF1: 0.81\n",
            "Epoch: 68 |  TrainLoss: 0.59173 | TestLoss: 0.61844 | TestAcc: 0.62443 | TestF1: 0.42\n",
            "Epoch: 69 |  TrainLoss: 0.59284 | TestLoss: 0.59151 | TestAcc: 0.77828 | TestF1: 0.76\n",
            "Epoch: 70 |  TrainLoss: 0.57254 | TestLoss: 0.58323 | TestAcc: 0.79638 | TestF1: 0.79\n",
            "Epoch: 71 |  TrainLoss: 0.56496 | TestLoss: 0.58654 | TestAcc: 0.71493 | TestF1: 0.64\n",
            "Epoch: 72 |  TrainLoss: 0.56135 | TestLoss: 0.57132 | TestAcc: 0.77376 | TestF1: 0.74\n",
            "Epoch: 73 |  TrainLoss: 0.53633 | TestLoss: 0.55746 | TestAcc: 0.80543 | TestF1: 0.80\n",
            "Epoch: 74 |  TrainLoss: 0.53296 | TestLoss: 0.55863 | TestAcc: 0.76018 | TestF1: 0.72\n",
            "Epoch: 75 |  TrainLoss: 0.51897 | TestLoss: 0.54015 | TestAcc: 0.79186 | TestF1: 0.78\n",
            "Epoch: 76 |  TrainLoss: 0.50223 | TestLoss: 0.53481 | TestAcc: 0.78281 | TestF1: 0.76\n",
            "Epoch: 77 |  TrainLoss: 0.50771 | TestLoss: 0.52008 | TestAcc: 0.78733 | TestF1: 0.77\n",
            "Epoch: 78 |  TrainLoss: 0.48260 | TestLoss: 0.50804 | TestAcc: 0.79638 | TestF1: 0.79\n",
            "Epoch: 79 |  TrainLoss: 0.46602 | TestLoss: 0.51365 | TestAcc: 0.77828 | TestF1: 0.75\n",
            "Epoch: 80 |  TrainLoss: 0.45338 | TestLoss: 0.49501 | TestAcc: 0.83258 | TestF1: 0.85\n",
            "Epoch: 81 |  TrainLoss: 0.47296 | TestLoss: 0.55895 | TestAcc: 0.68778 | TestF1: 0.59\n",
            "Epoch: 82 |  TrainLoss: 0.46292 | TestLoss: 0.47655 | TestAcc: 0.79186 | TestF1: 0.78\n",
            "Epoch: 83 |  TrainLoss: 0.41382 | TestLoss: 0.47083 | TestAcc: 0.83258 | TestF1: 0.85\n",
            "Epoch: 84 |  TrainLoss: 0.44581 | TestLoss: 0.46752 | TestAcc: 0.79186 | TestF1: 0.78\n",
            "Epoch: 85 |  TrainLoss: 0.38784 | TestLoss: 0.51090 | TestAcc: 0.75113 | TestF1: 0.70\n",
            "Epoch: 86 |  TrainLoss: 0.38748 | TestLoss: 0.44027 | TestAcc: 0.82805 | TestF1: 0.83\n",
            "Epoch: 87 |  TrainLoss: 0.38497 | TestLoss: 0.43634 | TestAcc: 0.82805 | TestF1: 0.84\n",
            "Epoch: 88 |  TrainLoss: 0.36680 | TestLoss: 0.47578 | TestAcc: 0.77828 | TestF1: 0.75\n",
            "Epoch: 89 |  TrainLoss: 0.37384 | TestLoss: 0.45155 | TestAcc: 0.78733 | TestF1: 0.77\n",
            "Epoch: 90 |  TrainLoss: 0.33485 | TestLoss: 0.41985 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 91 |  TrainLoss: 0.35470 | TestLoss: 0.42581 | TestAcc: 0.81448 | TestF1: 0.81\n",
            "Epoch: 92 |  TrainLoss: 0.30884 | TestLoss: 0.46241 | TestAcc: 0.78733 | TestF1: 0.76\n",
            "Epoch: 93 |  TrainLoss: 0.33193 | TestLoss: 0.40947 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 94 |  TrainLoss: 0.31849 | TestLoss: 0.40820 | TestAcc: 0.82805 | TestF1: 0.83\n",
            "Epoch: 95 |  TrainLoss: 0.29496 | TestLoss: 0.52438 | TestAcc: 0.76471 | TestF1: 0.72\n",
            "Epoch: 96 |  TrainLoss: 0.36971 | TestLoss: 0.40725 | TestAcc: 0.83710 | TestF1: 0.85\n",
            "Epoch: 97 |  TrainLoss: 0.31722 | TestLoss: 0.40736 | TestAcc: 0.83258 | TestF1: 0.84\n",
            "Epoch: 98 |  TrainLoss: 0.32227 | TestLoss: 0.52001 | TestAcc: 0.77828 | TestF1: 0.74\n",
            "Epoch: 99 |  TrainLoss: 0.33927 | TestLoss: 0.44580 | TestAcc: 0.80543 | TestF1: 0.79\n",
            "Epoch: 100 |  TrainLoss: 0.25842 | TestLoss: 0.41031 | TestAcc: 0.84163 | TestF1: 0.85\n",
            "Epoch: 101 |  TrainLoss: 0.33123 | TestLoss: 0.39876 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 102 |  TrainLoss: 0.29114 | TestLoss: 0.50651 | TestAcc: 0.79186 | TestF1: 0.76\n",
            "Epoch: 103 |  TrainLoss: 0.28419 | TestLoss: 0.43482 | TestAcc: 0.80543 | TestF1: 0.80\n",
            "Epoch: 104 |  TrainLoss: 0.23363 | TestLoss: 0.39760 | TestAcc: 0.84163 | TestF1: 0.85\n",
            "Epoch: 105 |  TrainLoss: 0.29465 | TestLoss: 0.39617 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 106 |  TrainLoss: 0.27480 | TestLoss: 0.45559 | TestAcc: 0.79638 | TestF1: 0.78\n",
            "Epoch: 107 |  TrainLoss: 0.26678 | TestLoss: 0.41350 | TestAcc: 0.80995 | TestF1: 0.80\n",
            "Epoch: 108 |  TrainLoss: 0.25396 | TestLoss: 0.39246 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 109 |  TrainLoss: 0.23021 | TestLoss: 0.40049 | TestAcc: 0.81900 | TestF1: 0.81\n",
            "Epoch: 110 |  TrainLoss: 0.25252 | TestLoss: 0.41380 | TestAcc: 0.80995 | TestF1: 0.80\n",
            "Epoch: 111 |  TrainLoss: 0.22443 | TestLoss: 0.41655 | TestAcc: 0.80995 | TestF1: 0.80\n",
            "Epoch: 112 |  TrainLoss: 0.22605 | TestLoss: 0.40614 | TestAcc: 0.81448 | TestF1: 0.81\n",
            "Epoch: 113 |  TrainLoss: 0.23694 | TestLoss: 0.38626 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 114 |  TrainLoss: 0.24254 | TestLoss: 0.39831 | TestAcc: 0.82353 | TestF1: 0.82\n",
            "Epoch: 115 |  TrainLoss: 0.22657 | TestLoss: 0.39694 | TestAcc: 0.82353 | TestF1: 0.82\n",
            "Epoch: 116 |  TrainLoss: 0.23898 | TestLoss: 0.42061 | TestAcc: 0.80995 | TestF1: 0.80\n",
            "Epoch: 117 |  TrainLoss: 0.21344 | TestLoss: 0.37977 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 118 |  TrainLoss: 0.24130 | TestLoss: 0.39466 | TestAcc: 0.82805 | TestF1: 0.82\n",
            "Epoch: 119 |  TrainLoss: 0.20551 | TestLoss: 0.43234 | TestAcc: 0.81448 | TestF1: 0.80\n",
            "Epoch: 120 |  TrainLoss: 0.24520 | TestLoss: 0.39238 | TestAcc: 0.83258 | TestF1: 0.83\n",
            "Epoch: 121 |  TrainLoss: 0.22015 | TestLoss: 0.37580 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 122 |  TrainLoss: 0.20509 | TestLoss: 0.46941 | TestAcc: 0.81900 | TestF1: 0.80\n",
            "Epoch: 123 |  TrainLoss: 0.24322 | TestLoss: 0.37883 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 124 |  TrainLoss: 0.22855 | TestLoss: 0.38780 | TestAcc: 0.82805 | TestF1: 0.82\n",
            "Epoch: 125 |  TrainLoss: 0.17791 | TestLoss: 0.49893 | TestAcc: 0.81900 | TestF1: 0.80\n",
            "Epoch: 126 |  TrainLoss: 0.22652 | TestLoss: 0.38194 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 127 |  TrainLoss: 0.22599 | TestLoss: 0.37430 | TestAcc: 0.83258 | TestF1: 0.83\n",
            "Epoch: 128 |  TrainLoss: 0.17097 | TestLoss: 0.52007 | TestAcc: 0.80543 | TestF1: 0.77\n",
            "Epoch: 129 |  TrainLoss: 0.22348 | TestLoss: 0.37568 | TestAcc: 0.83258 | TestF1: 0.83\n",
            "Epoch: 130 |  TrainLoss: 0.15980 | TestLoss: 0.38916 | TestAcc: 0.85520 | TestF1: 0.87\n",
            "Epoch: 131 |  TrainLoss: 0.24747 | TestLoss: 0.45355 | TestAcc: 0.82353 | TestF1: 0.81\n",
            "Epoch: 132 |  TrainLoss: 0.19973 | TestLoss: 0.45114 | TestAcc: 0.82805 | TestF1: 0.81\n",
            "Epoch: 133 |  TrainLoss: 0.19963 | TestLoss: 0.39111 | TestAcc: 0.85520 | TestF1: 0.87\n",
            "Epoch: 134 |  TrainLoss: 0.23996 | TestLoss: 0.38024 | TestAcc: 0.82353 | TestF1: 0.82\n",
            "Epoch: 135 |  TrainLoss: 0.16242 | TestLoss: 0.49660 | TestAcc: 0.81448 | TestF1: 0.79\n",
            "Epoch: 136 |  TrainLoss: 0.20258 | TestLoss: 0.36810 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 137 |  TrainLoss: 0.17509 | TestLoss: 0.37036 | TestAcc: 0.84163 | TestF1: 0.85\n",
            "Epoch: 138 |  TrainLoss: 0.17683 | TestLoss: 0.45004 | TestAcc: 0.82805 | TestF1: 0.81\n",
            "Epoch: 139 |  TrainLoss: 0.15721 | TestLoss: 0.45257 | TestAcc: 0.82805 | TestF1: 0.81\n",
            "Epoch: 140 |  TrainLoss: 0.16356 | TestLoss: 0.37416 | TestAcc: 0.84615 | TestF1: 0.86\n",
            "Epoch: 141 |  TrainLoss: 0.17982 | TestLoss: 0.36629 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 142 |  TrainLoss: 0.13799 | TestLoss: 0.51066 | TestAcc: 0.80543 | TestF1: 0.78\n",
            "Epoch: 143 |  TrainLoss: 0.19991 | TestLoss: 0.37160 | TestAcc: 0.83710 | TestF1: 0.84\n",
            "Epoch: 144 |  TrainLoss: 0.15945 | TestLoss: 0.37607 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 145 |  TrainLoss: 0.17635 | TestLoss: 0.43570 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 146 |  TrainLoss: 0.14932 | TestLoss: 0.44554 | TestAcc: 0.83258 | TestF1: 0.82\n",
            "Epoch: 147 |  TrainLoss: 0.15572 | TestLoss: 0.36701 | TestAcc: 0.84163 | TestF1: 0.85\n",
            "Epoch: 148 |  TrainLoss: 0.14409 | TestLoss: 0.36944 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 149 |  TrainLoss: 0.12208 | TestLoss: 0.42800 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 150 |  TrainLoss: 0.15055 | TestLoss: 0.41568 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 151 |  TrainLoss: 0.12697 | TestLoss: 0.37102 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 152 |  TrainLoss: 0.12621 | TestLoss: 0.38894 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 153 |  TrainLoss: 0.11728 | TestLoss: 0.42294 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 154 |  TrainLoss: 0.13873 | TestLoss: 0.37736 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 155 |  TrainLoss: 0.13631 | TestLoss: 0.37607 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 156 |  TrainLoss: 0.10527 | TestLoss: 0.50166 | TestAcc: 0.82353 | TestF1: 0.80\n",
            "Epoch: 157 |  TrainLoss: 0.14049 | TestLoss: 0.37582 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 158 |  TrainLoss: 0.10519 | TestLoss: 0.37631 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 159 |  TrainLoss: 0.13947 | TestLoss: 0.45176 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 160 |  TrainLoss: 0.12638 | TestLoss: 0.39166 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 161 |  TrainLoss: 0.10395 | TestLoss: 0.37646 | TestAcc: 0.83258 | TestF1: 0.84\n",
            "Epoch: 162 |  TrainLoss: 0.10742 | TestLoss: 0.43271 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 163 |  TrainLoss: 0.10544 | TestLoss: 0.41881 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 164 |  TrainLoss: 0.09229 | TestLoss: 0.38029 | TestAcc: 0.84163 | TestF1: 0.85\n",
            "Epoch: 165 |  TrainLoss: 0.09814 | TestLoss: 0.39517 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 166 |  TrainLoss: 0.10607 | TestLoss: 0.50447 | TestAcc: 0.83710 | TestF1: 0.82\n",
            "Epoch: 167 |  TrainLoss: 0.13350 | TestLoss: 0.38804 | TestAcc: 0.85973 | TestF1: 0.87\n",
            "Epoch: 168 |  TrainLoss: 0.11324 | TestLoss: 0.39314 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 169 |  TrainLoss: 0.09424 | TestLoss: 0.51897 | TestAcc: 0.83258 | TestF1: 0.82\n",
            "Epoch: 170 |  TrainLoss: 0.12962 | TestLoss: 0.38807 | TestAcc: 0.84163 | TestF1: 0.85\n",
            "Epoch: 171 |  TrainLoss: 0.10070 | TestLoss: 0.39109 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 172 |  TrainLoss: 0.11276 | TestLoss: 0.52410 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 173 |  TrainLoss: 0.12179 | TestLoss: 0.40115 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 174 |  TrainLoss: 0.08170 | TestLoss: 0.39949 | TestAcc: 0.85973 | TestF1: 0.87\n",
            "Epoch: 175 |  TrainLoss: 0.12785 | TestLoss: 0.51568 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 176 |  TrainLoss: 0.11628 | TestLoss: 0.40301 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 177 |  TrainLoss: 0.08667 | TestLoss: 0.40040 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 178 |  TrainLoss: 0.08840 | TestLoss: 0.43836 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 179 |  TrainLoss: 0.08212 | TestLoss: 0.50063 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 180 |  TrainLoss: 0.08507 | TestLoss: 0.40116 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 181 |  TrainLoss: 0.10568 | TestLoss: 0.40709 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 182 |  TrainLoss: 0.06022 | TestLoss: 0.45308 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 183 |  TrainLoss: 0.06872 | TestLoss: 0.41476 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 184 |  TrainLoss: 0.07370 | TestLoss: 0.43278 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 185 |  TrainLoss: 0.05139 | TestLoss: 0.45383 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 186 |  TrainLoss: 0.06668 | TestLoss: 0.42869 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 187 |  TrainLoss: 0.05396 | TestLoss: 0.41801 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 188 |  TrainLoss: 0.05770 | TestLoss: 0.45245 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 189 |  TrainLoss: 0.05390 | TestLoss: 0.45213 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 190 |  TrainLoss: 0.05793 | TestLoss: 0.43346 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 191 |  TrainLoss: 0.04128 | TestLoss: 0.42089 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 192 |  TrainLoss: 0.05525 | TestLoss: 0.47027 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 193 |  TrainLoss: 0.03928 | TestLoss: 0.48321 | TestAcc: 0.85068 | TestF1: 0.84\n",
            "Epoch: 194 |  TrainLoss: 0.04767 | TestLoss: 0.42776 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 195 |  TrainLoss: 0.05747 | TestLoss: 0.43743 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 196 |  TrainLoss: 0.03851 | TestLoss: 0.46222 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 197 |  TrainLoss: 0.03720 | TestLoss: 0.45802 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 198 |  TrainLoss: 0.04539 | TestLoss: 0.43732 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 199 |  TrainLoss: 0.05036 | TestLoss: 0.46071 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 200 |  TrainLoss: 0.03626 | TestLoss: 0.49900 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 201 |  TrainLoss: 0.04821 | TestLoss: 0.47996 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 202 |  TrainLoss: 0.03950 | TestLoss: 0.44721 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 203 |  TrainLoss: 0.05314 | TestLoss: 0.51675 | TestAcc: 0.85068 | TestF1: 0.84\n",
            "Epoch: 204 |  TrainLoss: 0.04706 | TestLoss: 0.50312 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 205 |  TrainLoss: 0.04436 | TestLoss: 0.46420 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 206 |  TrainLoss: 0.07430 | TestLoss: 0.52053 | TestAcc: 0.85068 | TestF1: 0.84\n",
            "Epoch: 207 |  TrainLoss: 0.03854 | TestLoss: 0.62976 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 208 |  TrainLoss: 0.07447 | TestLoss: 0.48210 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 209 |  TrainLoss: 0.08203 | TestLoss: 0.46386 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 210 |  TrainLoss: 0.04634 | TestLoss: 0.65445 | TestAcc: 0.83710 | TestF1: 0.82\n",
            "Epoch: 211 |  TrainLoss: 0.08766 | TestLoss: 0.47212 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 212 |  TrainLoss: 0.03776 | TestLoss: 0.46985 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 213 |  TrainLoss: 0.05629 | TestLoss: 0.55356 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 214 |  TrainLoss: 0.03982 | TestLoss: 0.54911 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 215 |  TrainLoss: 0.03557 | TestLoss: 0.47880 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 216 |  TrainLoss: 0.03068 | TestLoss: 0.47641 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 217 |  TrainLoss: 0.03448 | TestLoss: 0.55499 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 218 |  TrainLoss: 0.03585 | TestLoss: 0.52158 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 219 |  TrainLoss: 0.03130 | TestLoss: 0.48535 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 220 |  TrainLoss: 0.03659 | TestLoss: 0.49418 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 221 |  TrainLoss: 0.03486 | TestLoss: 0.66586 | TestAcc: 0.83258 | TestF1: 0.82\n",
            "Epoch: 222 |  TrainLoss: 0.06968 | TestLoss: 0.49003 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 223 |  TrainLoss: 0.04170 | TestLoss: 0.49345 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 224 |  TrainLoss: 0.05467 | TestLoss: 0.66488 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 225 |  TrainLoss: 0.08145 | TestLoss: 0.50095 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 226 |  TrainLoss: 0.02820 | TestLoss: 0.49961 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 227 |  TrainLoss: 0.05276 | TestLoss: 0.55810 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 228 |  TrainLoss: 0.02301 | TestLoss: 0.67577 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 229 |  TrainLoss: 0.05584 | TestLoss: 0.49981 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 230 |  TrainLoss: 0.03974 | TestLoss: 0.50253 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 231 |  TrainLoss: 0.03123 | TestLoss: 0.55515 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 232 |  TrainLoss: 0.02143 | TestLoss: 0.61124 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 233 |  TrainLoss: 0.03058 | TestLoss: 0.53199 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 234 |  TrainLoss: 0.01751 | TestLoss: 0.51263 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 235 |  TrainLoss: 0.02249 | TestLoss: 0.52132 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 236 |  TrainLoss: 0.01933 | TestLoss: 0.57395 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 237 |  TrainLoss: 0.02485 | TestLoss: 0.55975 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 238 |  TrainLoss: 0.01787 | TestLoss: 0.52925 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 239 |  TrainLoss: 0.01458 | TestLoss: 0.52578 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 240 |  TrainLoss: 0.02005 | TestLoss: 0.55388 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 241 |  TrainLoss: 0.02017 | TestLoss: 0.61006 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 242 |  TrainLoss: 0.01678 | TestLoss: 0.59746 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 243 |  TrainLoss: 0.01228 | TestLoss: 0.55131 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 244 |  TrainLoss: 0.01964 | TestLoss: 0.53449 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 245 |  TrainLoss: 0.02475 | TestLoss: 0.55062 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 246 |  TrainLoss: 0.01031 | TestLoss: 0.62578 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 247 |  TrainLoss: 0.02726 | TestLoss: 0.57821 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 248 |  TrainLoss: 0.02042 | TestLoss: 0.54280 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 249 |  TrainLoss: 0.03453 | TestLoss: 0.57237 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 250 |  TrainLoss: 0.01274 | TestLoss: 0.66136 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 251 |  TrainLoss: 0.02570 | TestLoss: 0.58357 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 252 |  TrainLoss: 0.01522 | TestLoss: 0.55074 | TestAcc: 0.85068 | TestF1: 0.86\n",
            "Epoch: 253 |  TrainLoss: 0.02124 | TestLoss: 0.55706 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 254 |  TrainLoss: 0.01420 | TestLoss: 0.63594 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 255 |  TrainLoss: 0.01371 | TestLoss: 0.66348 | TestAcc: 0.84163 | TestF1: 0.83\n",
            "Epoch: 256 |  TrainLoss: 0.02338 | TestLoss: 0.57398 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 257 |  TrainLoss: 0.00887 | TestLoss: 0.55825 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 258 |  TrainLoss: 0.01785 | TestLoss: 0.56940 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 259 |  TrainLoss: 0.00948 | TestLoss: 0.63113 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 260 |  TrainLoss: 0.01739 | TestLoss: 0.61486 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 261 |  TrainLoss: 0.01419 | TestLoss: 0.59017 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 262 |  TrainLoss: 0.01510 | TestLoss: 0.56718 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 263 |  TrainLoss: 0.01849 | TestLoss: 0.58642 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 264 |  TrainLoss: 0.00874 | TestLoss: 0.64385 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 265 |  TrainLoss: 0.01206 | TestLoss: 0.63636 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 266 |  TrainLoss: 0.01088 | TestLoss: 0.60140 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 267 |  TrainLoss: 0.00752 | TestLoss: 0.58714 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 268 |  TrainLoss: 0.00884 | TestLoss: 0.59251 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 269 |  TrainLoss: 0.00739 | TestLoss: 0.60833 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 270 |  TrainLoss: 0.00840 | TestLoss: 0.60993 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 271 |  TrainLoss: 0.00745 | TestLoss: 0.61350 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 272 |  TrainLoss: 0.00676 | TestLoss: 0.61104 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 273 |  TrainLoss: 0.00741 | TestLoss: 0.60899 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 274 |  TrainLoss: 0.00643 | TestLoss: 0.61196 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 275 |  TrainLoss: 0.00763 | TestLoss: 0.64436 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 276 |  TrainLoss: 0.00914 | TestLoss: 0.64986 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 277 |  TrainLoss: 0.00811 | TestLoss: 0.62402 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 278 |  TrainLoss: 0.00746 | TestLoss: 0.60915 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 279 |  TrainLoss: 0.00934 | TestLoss: 0.62703 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 280 |  TrainLoss: 0.00725 | TestLoss: 0.67337 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 281 |  TrainLoss: 0.00719 | TestLoss: 0.67932 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 282 |  TrainLoss: 0.00977 | TestLoss: 0.62833 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 283 |  TrainLoss: 0.01114 | TestLoss: 0.61112 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 284 |  TrainLoss: 0.00709 | TestLoss: 0.61367 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 285 |  TrainLoss: 0.00799 | TestLoss: 0.64317 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 286 |  TrainLoss: 0.00623 | TestLoss: 0.67446 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 287 |  TrainLoss: 0.00750 | TestLoss: 0.65485 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 288 |  TrainLoss: 0.00474 | TestLoss: 0.63250 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 289 |  TrainLoss: 0.00544 | TestLoss: 0.62862 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 290 |  TrainLoss: 0.00749 | TestLoss: 0.64976 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 291 |  TrainLoss: 0.00389 | TestLoss: 0.67198 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 292 |  TrainLoss: 0.00503 | TestLoss: 0.67458 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 293 |  TrainLoss: 0.00599 | TestLoss: 0.67435 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 294 |  TrainLoss: 0.00609 | TestLoss: 0.64748 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 295 |  TrainLoss: 0.00579 | TestLoss: 0.63179 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 296 |  TrainLoss: 0.00900 | TestLoss: 0.64025 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 297 |  TrainLoss: 0.00499 | TestLoss: 0.66192 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 298 |  TrainLoss: 0.00450 | TestLoss: 0.69088 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 299 |  TrainLoss: 0.00590 | TestLoss: 0.69336 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 300 |  TrainLoss: 0.00312 | TestLoss: 0.68264 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 301 |  TrainLoss: 0.00390 | TestLoss: 0.66357 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 302 |  TrainLoss: 0.00500 | TestLoss: 0.65107 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 303 |  TrainLoss: 0.00408 | TestLoss: 0.65154 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 304 |  TrainLoss: 0.00516 | TestLoss: 0.66256 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 305 |  TrainLoss: 0.00626 | TestLoss: 0.69617 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 306 |  TrainLoss: 0.00604 | TestLoss: 0.71118 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 307 |  TrainLoss: 0.00672 | TestLoss: 0.67555 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 308 |  TrainLoss: 0.00309 | TestLoss: 0.65417 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 309 |  TrainLoss: 0.00442 | TestLoss: 0.65254 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 310 |  TrainLoss: 0.01300 | TestLoss: 0.71726 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 311 |  TrainLoss: 0.00906 | TestLoss: 0.71902 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 312 |  TrainLoss: 0.00417 | TestLoss: 0.69651 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 313 |  TrainLoss: 0.00466 | TestLoss: 0.67445 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 314 |  TrainLoss: 0.00443 | TestLoss: 0.67040 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 315 |  TrainLoss: 0.00795 | TestLoss: 0.70072 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 316 |  TrainLoss: 0.00438 | TestLoss: 0.73918 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 317 |  TrainLoss: 0.00462 | TestLoss: 0.73342 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 318 |  TrainLoss: 0.00559 | TestLoss: 0.68910 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 319 |  TrainLoss: 0.00285 | TestLoss: 0.66732 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 320 |  TrainLoss: 0.00450 | TestLoss: 0.66734 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 321 |  TrainLoss: 0.00459 | TestLoss: 0.67965 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 322 |  TrainLoss: 0.00311 | TestLoss: 0.69806 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 323 |  TrainLoss: 0.00340 | TestLoss: 0.72488 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 324 |  TrainLoss: 0.00374 | TestLoss: 0.73114 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 325 |  TrainLoss: 0.00398 | TestLoss: 0.71190 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 326 |  TrainLoss: 0.00292 | TestLoss: 0.69839 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 327 |  TrainLoss: 0.00359 | TestLoss: 0.69454 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 328 |  TrainLoss: 0.00397 | TestLoss: 0.69663 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 329 |  TrainLoss: 0.00411 | TestLoss: 0.71303 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 330 |  TrainLoss: 0.00362 | TestLoss: 0.72357 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 331 |  TrainLoss: 0.00290 | TestLoss: 0.72990 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 332 |  TrainLoss: 0.00422 | TestLoss: 0.71705 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 333 |  TrainLoss: 0.00503 | TestLoss: 0.72643 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 334 |  TrainLoss: 0.00358 | TestLoss: 0.71743 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 335 |  TrainLoss: 0.00298 | TestLoss: 0.70640 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 336 |  TrainLoss: 0.00318 | TestLoss: 0.70627 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 337 |  TrainLoss: 0.00319 | TestLoss: 0.71336 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 338 |  TrainLoss: 0.00272 | TestLoss: 0.72821 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 339 |  TrainLoss: 0.00268 | TestLoss: 0.73610 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 340 |  TrainLoss: 0.00380 | TestLoss: 0.73578 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 341 |  TrainLoss: 0.00244 | TestLoss: 0.72627 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 342 |  TrainLoss: 0.00490 | TestLoss: 0.73484 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 343 |  TrainLoss: 0.00264 | TestLoss: 0.74419 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 344 |  TrainLoss: 0.00239 | TestLoss: 0.73882 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 345 |  TrainLoss: 0.00384 | TestLoss: 0.71830 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 346 |  TrainLoss: 0.00202 | TestLoss: 0.71024 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 347 |  TrainLoss: 0.00368 | TestLoss: 0.71657 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 348 |  TrainLoss: 0.00431 | TestLoss: 0.74559 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 349 |  TrainLoss: 0.00203 | TestLoss: 0.77863 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 350 |  TrainLoss: 0.00352 | TestLoss: 0.78045 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 351 |  TrainLoss: 0.00326 | TestLoss: 0.75123 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 352 |  TrainLoss: 0.00286 | TestLoss: 0.72499 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 353 |  TrainLoss: 0.00277 | TestLoss: 0.71624 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 354 |  TrainLoss: 0.00360 | TestLoss: 0.72232 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 355 |  TrainLoss: 0.00203 | TestLoss: 0.73495 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 356 |  TrainLoss: 0.00227 | TestLoss: 0.75238 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 357 |  TrainLoss: 0.00188 | TestLoss: 0.76985 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 358 |  TrainLoss: 0.00242 | TestLoss: 0.77265 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 359 |  TrainLoss: 0.00352 | TestLoss: 0.77689 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 360 |  TrainLoss: 0.00268 | TestLoss: 0.76082 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 361 |  TrainLoss: 0.00271 | TestLoss: 0.74657 | TestAcc: 0.84615 | TestF1: 0.85\n",
            "Epoch: 362 |  TrainLoss: 0.00271 | TestLoss: 0.73286 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 363 |  TrainLoss: 0.00218 | TestLoss: 0.72970 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 364 |  TrainLoss: 0.00202 | TestLoss: 0.73132 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 365 |  TrainLoss: 0.00529 | TestLoss: 0.77444 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 366 |  TrainLoss: 0.00301 | TestLoss: 0.80952 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 367 |  TrainLoss: 0.00417 | TestLoss: 0.79687 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 368 |  TrainLoss: 0.00399 | TestLoss: 0.75923 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 369 |  TrainLoss: 0.00227 | TestLoss: 0.74245 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 370 |  TrainLoss: 0.00310 | TestLoss: 0.73962 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 371 |  TrainLoss: 0.00423 | TestLoss: 0.75679 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 372 |  TrainLoss: 0.00138 | TestLoss: 0.78143 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 373 |  TrainLoss: 0.00206 | TestLoss: 0.79752 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 374 |  TrainLoss: 0.00178 | TestLoss: 0.80310 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 375 |  TrainLoss: 0.00205 | TestLoss: 0.79079 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 376 |  TrainLoss: 0.00181 | TestLoss: 0.77964 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 377 |  TrainLoss: 0.00221 | TestLoss: 0.77301 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 378 |  TrainLoss: 0.00194 | TestLoss: 0.75856 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 379 |  TrainLoss: 0.00257 | TestLoss: 0.76105 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 380 |  TrainLoss: 0.00247 | TestLoss: 0.77304 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 381 |  TrainLoss: 0.00233 | TestLoss: 0.78090 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 382 |  TrainLoss: 0.00148 | TestLoss: 0.78997 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 383 |  TrainLoss: 0.00189 | TestLoss: 0.79911 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 384 |  TrainLoss: 0.00194 | TestLoss: 0.79952 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 385 |  TrainLoss: 0.00214 | TestLoss: 0.78966 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 386 |  TrainLoss: 0.00160 | TestLoss: 0.77650 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 387 |  TrainLoss: 0.00335 | TestLoss: 0.77786 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 388 |  TrainLoss: 0.00161 | TestLoss: 0.77643 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 389 |  TrainLoss: 0.00319 | TestLoss: 0.76456 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 390 |  TrainLoss: 0.00184 | TestLoss: 0.76033 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 391 |  TrainLoss: 0.00411 | TestLoss: 0.77413 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 392 |  TrainLoss: 0.00194 | TestLoss: 0.80520 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 393 |  TrainLoss: 0.00199 | TestLoss: 0.82560 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 394 |  TrainLoss: 0.00277 | TestLoss: 0.81256 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 395 |  TrainLoss: 0.00109 | TestLoss: 0.79463 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 396 |  TrainLoss: 0.00143 | TestLoss: 0.78610 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 397 |  TrainLoss: 0.00276 | TestLoss: 0.80538 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 398 |  TrainLoss: 0.00186 | TestLoss: 0.80959 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 399 |  TrainLoss: 0.00141 | TestLoss: 0.80626 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 400 |  TrainLoss: 0.00173 | TestLoss: 0.79438 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 401 |  TrainLoss: 0.00135 | TestLoss: 0.78468 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 402 |  TrainLoss: 0.00241 | TestLoss: 0.77371 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 403 |  TrainLoss: 0.00135 | TestLoss: 0.77188 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 404 |  TrainLoss: 0.00146 | TestLoss: 0.77413 | TestAcc: 0.86425 | TestF1: 0.87\n",
            "Epoch: 405 |  TrainLoss: 0.00239 | TestLoss: 0.79063 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 406 |  TrainLoss: 0.00155 | TestLoss: 0.80721 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 407 |  TrainLoss: 0.00201 | TestLoss: 0.82273 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 408 |  TrainLoss: 0.00255 | TestLoss: 0.81051 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 409 |  TrainLoss: 0.00128 | TestLoss: 0.79600 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 410 |  TrainLoss: 0.00113 | TestLoss: 0.78671 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 411 |  TrainLoss: 0.00281 | TestLoss: 0.79322 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 412 |  TrainLoss: 0.00165 | TestLoss: 0.79804 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 413 |  TrainLoss: 0.00159 | TestLoss: 0.80918 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 414 |  TrainLoss: 0.00151 | TestLoss: 0.81738 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 415 |  TrainLoss: 0.00203 | TestLoss: 0.81888 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 416 |  TrainLoss: 0.00125 | TestLoss: 0.81515 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 417 |  TrainLoss: 0.00110 | TestLoss: 0.80974 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 418 |  TrainLoss: 0.00178 | TestLoss: 0.80603 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 419 |  TrainLoss: 0.00148 | TestLoss: 0.80397 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 420 |  TrainLoss: 0.00202 | TestLoss: 0.80042 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 421 |  TrainLoss: 0.00141 | TestLoss: 0.79959 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 422 |  TrainLoss: 0.00125 | TestLoss: 0.80429 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 423 |  TrainLoss: 0.00108 | TestLoss: 0.81174 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 424 |  TrainLoss: 0.00122 | TestLoss: 0.82316 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 425 |  TrainLoss: 0.00131 | TestLoss: 0.83749 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 426 |  TrainLoss: 0.00100 | TestLoss: 0.85052 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 427 |  TrainLoss: 0.00158 | TestLoss: 0.84679 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 428 |  TrainLoss: 0.00151 | TestLoss: 0.82949 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 429 |  TrainLoss: 0.00090 | TestLoss: 0.81883 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 430 |  TrainLoss: 0.00088 | TestLoss: 0.81177 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 431 |  TrainLoss: 0.00154 | TestLoss: 0.80726 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 432 |  TrainLoss: 0.00140 | TestLoss: 0.81182 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 433 |  TrainLoss: 0.00083 | TestLoss: 0.81787 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 434 |  TrainLoss: 0.00079 | TestLoss: 0.82840 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 435 |  TrainLoss: 0.00087 | TestLoss: 0.84064 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 436 |  TrainLoss: 0.00101 | TestLoss: 0.85410 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 437 |  TrainLoss: 0.00245 | TestLoss: 0.84743 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 438 |  TrainLoss: 0.00118 | TestLoss: 0.84234 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 439 |  TrainLoss: 0.00090 | TestLoss: 0.83724 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 440 |  TrainLoss: 0.00125 | TestLoss: 0.83104 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 441 |  TrainLoss: 0.00080 | TestLoss: 0.82672 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 442 |  TrainLoss: 0.00203 | TestLoss: 0.81783 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 443 |  TrainLoss: 0.00176 | TestLoss: 0.82701 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 444 |  TrainLoss: 0.00204 | TestLoss: 0.84790 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 445 |  TrainLoss: 0.00127 | TestLoss: 0.85968 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 446 |  TrainLoss: 0.00094 | TestLoss: 0.86662 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 447 |  TrainLoss: 0.00160 | TestLoss: 0.85537 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 448 |  TrainLoss: 0.00117 | TestLoss: 0.84955 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 449 |  TrainLoss: 0.00113 | TestLoss: 0.84173 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 450 |  TrainLoss: 0.00123 | TestLoss: 0.83886 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 451 |  TrainLoss: 0.00069 | TestLoss: 0.83588 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 452 |  TrainLoss: 0.00199 | TestLoss: 0.84711 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 453 |  TrainLoss: 0.00108 | TestLoss: 0.86158 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 454 |  TrainLoss: 0.00091 | TestLoss: 0.87792 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 455 |  TrainLoss: 0.00107 | TestLoss: 0.88679 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 456 |  TrainLoss: 0.00151 | TestLoss: 0.88992 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 457 |  TrainLoss: 0.00150 | TestLoss: 0.87144 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 458 |  TrainLoss: 0.00108 | TestLoss: 0.85528 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 459 |  TrainLoss: 0.00087 | TestLoss: 0.84620 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 460 |  TrainLoss: 0.00129 | TestLoss: 0.84359 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 461 |  TrainLoss: 0.00101 | TestLoss: 0.84685 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 462 |  TrainLoss: 0.00094 | TestLoss: 0.85411 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 463 |  TrainLoss: 0.00094 | TestLoss: 0.86700 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 464 |  TrainLoss: 0.00086 | TestLoss: 0.88674 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 465 |  TrainLoss: 0.00082 | TestLoss: 0.89824 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 466 |  TrainLoss: 0.00086 | TestLoss: 0.89928 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 467 |  TrainLoss: 0.00071 | TestLoss: 0.89022 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 468 |  TrainLoss: 0.00097 | TestLoss: 0.87171 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 469 |  TrainLoss: 0.00083 | TestLoss: 0.85617 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 470 |  TrainLoss: 0.00102 | TestLoss: 0.84710 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 471 |  TrainLoss: 0.00074 | TestLoss: 0.84372 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 472 |  TrainLoss: 0.00279 | TestLoss: 0.87420 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 473 |  TrainLoss: 0.00068 | TestLoss: 0.90854 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 474 |  TrainLoss: 0.00085 | TestLoss: 0.93523 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 475 |  TrainLoss: 0.00170 | TestLoss: 0.91374 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 476 |  TrainLoss: 0.00099 | TestLoss: 0.88924 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 477 |  TrainLoss: 0.00059 | TestLoss: 0.87116 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 478 |  TrainLoss: 0.00074 | TestLoss: 0.85730 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 479 |  TrainLoss: 0.00119 | TestLoss: 0.85729 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 480 |  TrainLoss: 0.00070 | TestLoss: 0.86084 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 481 |  TrainLoss: 0.00040 | TestLoss: 0.86275 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 482 |  TrainLoss: 0.00133 | TestLoss: 0.87602 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 483 |  TrainLoss: 0.00070 | TestLoss: 0.88956 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 484 |  TrainLoss: 0.00098 | TestLoss: 0.89512 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 485 |  TrainLoss: 0.00073 | TestLoss: 0.88867 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 486 |  TrainLoss: 0.00068 | TestLoss: 0.87924 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 487 |  TrainLoss: 0.00063 | TestLoss: 0.87006 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 488 |  TrainLoss: 0.00053 | TestLoss: 0.86350 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 489 |  TrainLoss: 0.00095 | TestLoss: 0.86605 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 490 |  TrainLoss: 0.00058 | TestLoss: 0.87320 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 491 |  TrainLoss: 0.00128 | TestLoss: 0.89630 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 492 |  TrainLoss: 0.00055 | TestLoss: 0.91785 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 493 |  TrainLoss: 0.00108 | TestLoss: 0.91752 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 494 |  TrainLoss: 0.00175 | TestLoss: 0.89098 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 495 |  TrainLoss: 0.00103 | TestLoss: 0.87347 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 496 |  TrainLoss: 0.00052 | TestLoss: 0.85902 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 497 |  TrainLoss: 0.00082 | TestLoss: 0.85591 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 498 |  TrainLoss: 0.00066 | TestLoss: 0.85853 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 499 |  TrainLoss: 0.00105 | TestLoss: 0.87010 | TestAcc: 0.86425 | TestF1: 0.86\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wloss = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_acc)\n",
        "\n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Phv7JBbzE7mV",
      "metadata": {
        "id": "Phv7JBbzE7mV"
      },
      "source": [
        "##Plot of Test Accuracy over best Loss and Best Accuracy Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb12adcb",
      "metadata": {
        "id": "bb12adcb",
        "outputId": "602e7b0e-f94d-4e8f-c0ba-dd84b7ca83f6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(test_accs, label='Test Accuracy')\n",
        "\n",
        "# Add legend and axis labels\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "\n",
        "# Add marker for best epoch\n",
        "best_epoch_loss = test_losses.index(min(test_losses))\n",
        "best_epoch_acc = test_accs.index(max(test_accs))\n",
        "plt.axvline(x=best_epoch_loss, color='r', linestyle='--', label='Best Loss Epoch')\n",
        "plt.axvline(x=best_epoch_acc, color='g', linestyle='--', label='Best Acc Epoch')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632eac26",
      "metadata": {
        "id": "632eac26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
