{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18387023",
      "metadata": {
        "id": "18387023"
      },
      "source": [
        "# Model-1 \n",
        "GraphSage + Content using default parameters and 3 Layer MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fa6d9dc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa6d9dc2",
        "outputId": "ff26cde8-e0e0-400d-8ae0-cd28c8ce9edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=489401 sha256=de3c10a5a6768062b7ec5071d9f447d8166dae34865feed793a120b62e119da6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.24.3)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.17-cp310-cp310-linux_x86_64.whl size=1078307 sha256=afa43251aca02db1c5af91d3e0e6e980a1342f9226d1c7ef57b88c4cf2a30431\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/25/e7/037b58fa47ba781444fd101a2f06c63a9d4e967ca6b910c53a\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=82517b0e550baff098c212d480178428f76ac7b2299df1f589e7f5ee6d982d9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b70ca2b6",
      "metadata": {
        "id": "b70ca2b6"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import UPFD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "272b4593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "272b4593",
        "outputId": "21e4a22f-e207-40e2-cd0a-c8adfd05aaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://docs.google.com/uc?export=download&id=1VskhAQ92PrT4sWEKQ2v2-AJhEcpp4A81&confirm=t\n",
            "Extracting ./gossipcop/raw/uc\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#defining the train and test split by defining the feature as content and setting the name as Gossipcop\n",
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\",split=\"test\")\n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"val\")\n",
        "train_data_gos = train_data_gos + val_data_gos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fab31589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fab31589",
        "outputId": "7c6eacc6-21cc-46b0-d1d7-8df528519dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gossipcop Dataset\n",
            "Train Samples:  1638\n",
            "Test Samples:  3826\n"
          ]
        }
      ],
      "source": [
        "print(\"Gossipcop Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_gos))\n",
        "print(\"Test Samples: \", len(test_data_gos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "db557cf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db557cf0",
        "outputId": "9d4f0aa2-1cde-4c36-8f02-941cb1e9c03b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          1, 70, 74],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
              "         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "         73, 74, 75]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_data_gos[0].edge_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b5baf8a6",
      "metadata": {
        "id": "b5baf8a6"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_data_gos, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_data_gos, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5707e0c6",
      "metadata": {
        "id": "5707e0c6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from torch.nn import LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "47fa68ed",
      "metadata": {
        "id": "47fa68ed"
      },
      "outputs": [],
      "source": [
        "#defining the GraphSage Model with 3 SageConv layers and 3 unit MLP\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4],hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5],out_channels)\n",
        "\n",
        "        #droupouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "        self.dp3 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h,batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h).relu()\n",
        "        h = self.dp3(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bed69539",
      "metadata": {
        "id": "bed69539"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "804f4b17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "804f4b17",
        "outputId": "6a13e451-a99a-4935-85db-790136d720f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "#specifying number of input features, hidden layer sizes, and number of output channels\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #setting optimiser and learning rate as defined by the paper \n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1b902c53",
      "metadata": {
        "id": "1b902c53"
      },
      "outputs": [],
      "source": [
        "#defining the train and test function for the model\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "461c1f4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "461c1f4e",
        "outputId": "45ddfec5-f92b-4991-cfac-3f7ac0e1a751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69305 | TestLoss: 0.69284 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 01 |  TrainLoss: 0.69266 | TestLoss: 0.69221 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 02 |  TrainLoss: 0.69184 | TestLoss: 0.69106 | TestAcc: 0.49922 | TestF1: 0.00\n",
            "Epoch: 03 |  TrainLoss: 0.69092 | TestLoss: 0.68902 | TestAcc: 0.51673 | TestF1: 0.07\n",
            "Epoch: 04 |  TrainLoss: 0.68813 | TestLoss: 0.68485 | TestAcc: 0.79378 | TestF1: 0.83\n",
            "Epoch: 05 |  TrainLoss: 0.68392 | TestLoss: 0.67598 | TestAcc: 0.90094 | TestF1: 0.90\n",
            "Epoch: 06 |  TrainLoss: 0.67362 | TestLoss: 0.66354 | TestAcc: 0.69812 | TestF1: 0.57\n",
            "Epoch: 07 |  TrainLoss: 0.65871 | TestLoss: 0.63625 | TestAcc: 0.87428 | TestF1: 0.89\n",
            "Epoch: 08 |  TrainLoss: 0.63039 | TestLoss: 0.61618 | TestAcc: 0.72347 | TestF1: 0.62\n",
            "Epoch: 09 |  TrainLoss: 0.59668 | TestLoss: 0.54759 | TestAcc: 0.81129 | TestF1: 0.77\n",
            "Epoch: 10 |  TrainLoss: 0.54577 | TestLoss: 0.49868 | TestAcc: 0.81521 | TestF1: 0.84\n",
            "Epoch: 11 |  TrainLoss: 0.47703 | TestLoss: 0.39494 | TestAcc: 0.89728 | TestF1: 0.89\n",
            "Epoch: 12 |  TrainLoss: 0.40791 | TestLoss: 0.36387 | TestAcc: 0.84292 | TestF1: 0.82\n",
            "Epoch: 13 |  TrainLoss: 0.34150 | TestLoss: 0.28422 | TestAcc: 0.91296 | TestF1: 0.92\n",
            "Epoch: 14 |  TrainLoss: 0.29573 | TestLoss: 0.26872 | TestAcc: 0.90591 | TestF1: 0.91\n",
            "Epoch: 15 |  TrainLoss: 0.27087 | TestLoss: 0.29924 | TestAcc: 0.85363 | TestF1: 0.83\n",
            "Epoch: 16 |  TrainLoss: 0.23294 | TestLoss: 0.18800 | TestAcc: 0.93544 | TestF1: 0.94\n",
            "Epoch: 17 |  TrainLoss: 0.19753 | TestLoss: 0.16486 | TestAcc: 0.94564 | TestF1: 0.95\n",
            "Epoch: 18 |  TrainLoss: 0.17387 | TestLoss: 0.15132 | TestAcc: 0.94956 | TestF1: 0.95\n",
            "Epoch: 19 |  TrainLoss: 0.16923 | TestLoss: 0.14256 | TestAcc: 0.95400 | TestF1: 0.95\n",
            "Epoch: 20 |  TrainLoss: 0.15061 | TestLoss: 0.18022 | TestAcc: 0.92682 | TestF1: 0.92\n",
            "Epoch: 21 |  TrainLoss: 0.15690 | TestLoss: 0.12687 | TestAcc: 0.95896 | TestF1: 0.96\n",
            "Epoch: 22 |  TrainLoss: 0.13852 | TestLoss: 0.12244 | TestAcc: 0.96132 | TestF1: 0.96\n",
            "Epoch: 23 |  TrainLoss: 0.15164 | TestLoss: 0.11954 | TestAcc: 0.96315 | TestF1: 0.96\n",
            "Epoch: 24 |  TrainLoss: 0.16405 | TestLoss: 0.20393 | TestAcc: 0.91793 | TestF1: 0.91\n",
            "Epoch: 25 |  TrainLoss: 0.16669 | TestLoss: 0.11504 | TestAcc: 0.96498 | TestF1: 0.96\n",
            "Epoch: 26 |  TrainLoss: 0.11833 | TestLoss: 0.11170 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 27 |  TrainLoss: 0.11803 | TestLoss: 0.10894 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 28 |  TrainLoss: 0.11798 | TestLoss: 0.10849 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 29 |  TrainLoss: 0.11823 | TestLoss: 0.10789 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 30 |  TrainLoss: 0.12399 | TestLoss: 0.11648 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 31 |  TrainLoss: 0.11929 | TestLoss: 0.14725 | TestAcc: 0.95243 | TestF1: 0.95\n",
            "Epoch: 32 |  TrainLoss: 0.13045 | TestLoss: 0.10345 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 33 |  TrainLoss: 0.11445 | TestLoss: 0.10422 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 34 |  TrainLoss: 0.12171 | TestLoss: 0.09999 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 35 |  TrainLoss: 0.11784 | TestLoss: 0.10016 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 36 |  TrainLoss: 0.11774 | TestLoss: 0.10394 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 37 |  TrainLoss: 0.13070 | TestLoss: 0.09788 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 38 |  TrainLoss: 0.12611 | TestLoss: 0.10410 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 39 |  TrainLoss: 0.12568 | TestLoss: 0.17657 | TestAcc: 0.94302 | TestF1: 0.95\n",
            "Epoch: 40 |  TrainLoss: 0.12591 | TestLoss: 0.11783 | TestAcc: 0.96524 | TestF1: 0.97\n",
            "Epoch: 41 |  TrainLoss: 0.11608 | TestLoss: 0.10003 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 42 |  TrainLoss: 0.10234 | TestLoss: 0.10021 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 43 |  TrainLoss: 0.09524 | TestLoss: 0.09367 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 44 |  TrainLoss: 0.09447 | TestLoss: 0.09357 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 45 |  TrainLoss: 0.09239 | TestLoss: 0.09762 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 46 |  TrainLoss: 0.09409 | TestLoss: 0.11327 | TestAcc: 0.96576 | TestF1: 0.97\n",
            "Epoch: 47 |  TrainLoss: 0.11491 | TestLoss: 0.12174 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 48 |  TrainLoss: 0.09562 | TestLoss: 0.09103 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 49 |  TrainLoss: 0.08622 | TestLoss: 0.09249 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 50 |  TrainLoss: 0.08690 | TestLoss: 0.09267 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 51 |  TrainLoss: 0.09301 | TestLoss: 0.08977 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 52 |  TrainLoss: 0.09026 | TestLoss: 0.08868 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 53 |  TrainLoss: 0.08988 | TestLoss: 0.09021 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 54 |  TrainLoss: 0.08285 | TestLoss: 0.09017 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 55 |  TrainLoss: 0.08202 | TestLoss: 0.08954 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 56 |  TrainLoss: 0.07915 | TestLoss: 0.09307 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 57 |  TrainLoss: 0.09044 | TestLoss: 0.11547 | TestAcc: 0.96393 | TestF1: 0.96\n",
            "Epoch: 58 |  TrainLoss: 0.10001 | TestLoss: 0.10345 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 59 |  TrainLoss: 0.09505 | TestLoss: 0.08764 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 60 |  TrainLoss: 0.08961 | TestLoss: 0.09427 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 61 |  TrainLoss: 0.08806 | TestLoss: 0.09785 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 62 |  TrainLoss: 0.08571 | TestLoss: 0.11300 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 63 |  TrainLoss: 0.08856 | TestLoss: 0.10060 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 64 |  TrainLoss: 0.08855 | TestLoss: 0.08496 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 65 |  TrainLoss: 0.07620 | TestLoss: 0.09490 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 66 |  TrainLoss: 0.07640 | TestLoss: 0.08919 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 67 |  TrainLoss: 0.08339 | TestLoss: 0.09159 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 68 |  TrainLoss: 0.08930 | TestLoss: 0.09751 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 69 |  TrainLoss: 0.12800 | TestLoss: 0.09139 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 70 |  TrainLoss: 0.09493 | TestLoss: 0.08728 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 71 |  TrainLoss: 0.08069 | TestLoss: 0.09296 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 72 |  TrainLoss: 0.07539 | TestLoss: 0.08876 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 73 |  TrainLoss: 0.07701 | TestLoss: 0.08488 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 74 |  TrainLoss: 0.07381 | TestLoss: 0.08541 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 75 |  TrainLoss: 0.07994 | TestLoss: 0.08965 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 76 |  TrainLoss: 0.08346 | TestLoss: 0.11655 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 77 |  TrainLoss: 0.07830 | TestLoss: 0.10690 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 78 |  TrainLoss: 0.07812 | TestLoss: 0.08525 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 79 |  TrainLoss: 0.07642 | TestLoss: 0.08778 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 80 |  TrainLoss: 0.07494 | TestLoss: 0.08744 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 81 |  TrainLoss: 0.07644 | TestLoss: 0.08391 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 82 |  TrainLoss: 0.06827 | TestLoss: 0.12908 | TestAcc: 0.96419 | TestF1: 0.97\n",
            "Epoch: 83 |  TrainLoss: 0.10569 | TestLoss: 0.11059 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 84 |  TrainLoss: 0.08326 | TestLoss: 0.09421 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 85 |  TrainLoss: 0.07913 | TestLoss: 0.08389 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 86 |  TrainLoss: 0.06843 | TestLoss: 0.08322 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 87 |  TrainLoss: 0.06862 | TestLoss: 0.08354 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 88 |  TrainLoss: 0.07100 | TestLoss: 0.08447 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 89 |  TrainLoss: 0.06873 | TestLoss: 0.08255 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 90 |  TrainLoss: 0.06771 | TestLoss: 0.08625 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 91 |  TrainLoss: 0.07010 | TestLoss: 0.08561 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 92 |  TrainLoss: 0.07143 | TestLoss: 0.08232 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 93 |  TrainLoss: 0.07302 | TestLoss: 0.08678 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 94 |  TrainLoss: 0.07185 | TestLoss: 0.08152 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 95 |  TrainLoss: 0.06706 | TestLoss: 0.08207 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 96 |  TrainLoss: 0.06529 | TestLoss: 0.08230 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 97 |  TrainLoss: 0.06371 | TestLoss: 0.08627 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 98 |  TrainLoss: 0.06323 | TestLoss: 0.08118 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 99 |  TrainLoss: 0.06660 | TestLoss: 0.08850 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 100 |  TrainLoss: 0.06647 | TestLoss: 0.10960 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 101 |  TrainLoss: 0.08716 | TestLoss: 0.09859 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 102 |  TrainLoss: 0.07669 | TestLoss: 0.08185 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 103 |  TrainLoss: 0.06729 | TestLoss: 0.08715 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 104 |  TrainLoss: 0.06940 | TestLoss: 0.08991 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 105 |  TrainLoss: 0.06355 | TestLoss: 0.08217 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 106 |  TrainLoss: 0.05825 | TestLoss: 0.08061 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 107 |  TrainLoss: 0.06359 | TestLoss: 0.09568 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 108 |  TrainLoss: 0.06835 | TestLoss: 0.08159 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 109 |  TrainLoss: 0.06965 | TestLoss: 0.08740 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 110 |  TrainLoss: 0.12014 | TestLoss: 0.09669 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 111 |  TrainLoss: 0.11296 | TestLoss: 0.10701 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 112 |  TrainLoss: 0.07965 | TestLoss: 0.08201 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 113 |  TrainLoss: 0.07376 | TestLoss: 0.10004 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 114 |  TrainLoss: 0.07883 | TestLoss: 0.09035 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 115 |  TrainLoss: 0.06415 | TestLoss: 0.08341 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 116 |  TrainLoss: 0.06443 | TestLoss: 0.08676 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 117 |  TrainLoss: 0.06902 | TestLoss: 0.08862 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 118 |  TrainLoss: 0.06419 | TestLoss: 0.09459 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 119 |  TrainLoss: 0.06413 | TestLoss: 0.08386 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 120 |  TrainLoss: 0.06069 | TestLoss: 0.08127 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 121 |  TrainLoss: 0.05921 | TestLoss: 0.09355 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 122 |  TrainLoss: 0.06113 | TestLoss: 0.08469 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 123 |  TrainLoss: 0.06268 | TestLoss: 0.08553 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 124 |  TrainLoss: 0.06011 | TestLoss: 0.08473 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 125 |  TrainLoss: 0.05772 | TestLoss: 0.08481 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 126 |  TrainLoss: 0.05827 | TestLoss: 0.09100 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 127 |  TrainLoss: 0.05446 | TestLoss: 0.09326 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 128 |  TrainLoss: 0.05833 | TestLoss: 0.08583 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 129 |  TrainLoss: 0.05841 | TestLoss: 0.10511 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 130 |  TrainLoss: 0.06790 | TestLoss: 0.11705 | TestAcc: 0.96289 | TestF1: 0.96\n",
            "Epoch: 131 |  TrainLoss: 0.07717 | TestLoss: 0.09748 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 132 |  TrainLoss: 0.06350 | TestLoss: 0.09871 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 133 |  TrainLoss: 0.06680 | TestLoss: 0.08956 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 134 |  TrainLoss: 0.05333 | TestLoss: 0.11838 | TestAcc: 0.96498 | TestF1: 0.97\n",
            "Epoch: 135 |  TrainLoss: 0.06572 | TestLoss: 0.10172 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 136 |  TrainLoss: 0.06238 | TestLoss: 0.08053 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 137 |  TrainLoss: 0.05273 | TestLoss: 0.08452 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 138 |  TrainLoss: 0.05938 | TestLoss: 0.09800 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 139 |  TrainLoss: 0.06587 | TestLoss: 0.08045 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 140 |  TrainLoss: 0.05254 | TestLoss: 0.09953 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 141 |  TrainLoss: 0.06162 | TestLoss: 0.08800 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 142 |  TrainLoss: 0.05531 | TestLoss: 0.09215 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 143 |  TrainLoss: 0.05016 | TestLoss: 0.08364 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 144 |  TrainLoss: 0.05274 | TestLoss: 0.08330 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 145 |  TrainLoss: 0.04818 | TestLoss: 0.08436 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 146 |  TrainLoss: 0.04919 | TestLoss: 0.08451 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 147 |  TrainLoss: 0.04684 | TestLoss: 0.08494 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 148 |  TrainLoss: 0.04580 | TestLoss: 0.11622 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 149 |  TrainLoss: 0.06243 | TestLoss: 0.09118 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 150 |  TrainLoss: 0.05162 | TestLoss: 0.11208 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 151 |  TrainLoss: 0.05864 | TestLoss: 0.10006 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 152 |  TrainLoss: 0.04590 | TestLoss: 0.08877 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 153 |  TrainLoss: 0.04365 | TestLoss: 0.08259 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 154 |  TrainLoss: 0.04634 | TestLoss: 0.08780 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 155 |  TrainLoss: 0.06367 | TestLoss: 0.10156 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 156 |  TrainLoss: 0.05368 | TestLoss: 0.08498 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 157 |  TrainLoss: 0.04189 | TestLoss: 0.09839 | TestAcc: 0.96681 | TestF1: 0.97\n",
            "Epoch: 158 |  TrainLoss: 0.05005 | TestLoss: 0.08668 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 159 |  TrainLoss: 0.04627 | TestLoss: 0.08485 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 160 |  TrainLoss: 0.04511 | TestLoss: 0.08937 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 161 |  TrainLoss: 0.04659 | TestLoss: 0.08369 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 162 |  TrainLoss: 0.04222 | TestLoss: 0.08917 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 163 |  TrainLoss: 0.04026 | TestLoss: 0.10155 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 164 |  TrainLoss: 0.03911 | TestLoss: 0.08478 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 165 |  TrainLoss: 0.04187 | TestLoss: 0.09542 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 166 |  TrainLoss: 0.04467 | TestLoss: 0.09138 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 167 |  TrainLoss: 0.04287 | TestLoss: 0.08627 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 168 |  TrainLoss: 0.05072 | TestLoss: 0.11056 | TestAcc: 0.96445 | TestF1: 0.96\n",
            "Epoch: 169 |  TrainLoss: 0.05022 | TestLoss: 0.09066 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 170 |  TrainLoss: 0.05841 | TestLoss: 0.08767 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 171 |  TrainLoss: 0.05317 | TestLoss: 0.09736 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 172 |  TrainLoss: 0.04910 | TestLoss: 0.17776 | TestAcc: 0.95583 | TestF1: 0.96\n",
            "Epoch: 173 |  TrainLoss: 0.07258 | TestLoss: 0.11090 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 174 |  TrainLoss: 0.04746 | TestLoss: 0.08721 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 175 |  TrainLoss: 0.05462 | TestLoss: 0.09634 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 176 |  TrainLoss: 0.03757 | TestLoss: 0.08686 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 177 |  TrainLoss: 0.03965 | TestLoss: 0.09242 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 178 |  TrainLoss: 0.03479 | TestLoss: 0.10227 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 179 |  TrainLoss: 0.03720 | TestLoss: 0.09689 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 180 |  TrainLoss: 0.03711 | TestLoss: 0.08859 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 181 |  TrainLoss: 0.03376 | TestLoss: 0.09062 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 182 |  TrainLoss: 0.04029 | TestLoss: 0.10566 | TestAcc: 0.96550 | TestF1: 0.96\n",
            "Epoch: 183 |  TrainLoss: 0.05427 | TestLoss: 0.10010 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 184 |  TrainLoss: 0.05326 | TestLoss: 0.08985 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 185 |  TrainLoss: 0.04600 | TestLoss: 0.08301 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 186 |  TrainLoss: 0.03422 | TestLoss: 0.08636 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 187 |  TrainLoss: 0.03310 | TestLoss: 0.09246 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 188 |  TrainLoss: 0.03196 | TestLoss: 0.09408 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 189 |  TrainLoss: 0.03316 | TestLoss: 0.09291 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 190 |  TrainLoss: 0.03131 | TestLoss: 0.09997 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 191 |  TrainLoss: 0.03159 | TestLoss: 0.11004 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 192 |  TrainLoss: 0.03106 | TestLoss: 0.10217 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 193 |  TrainLoss: 0.03098 | TestLoss: 0.09129 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 194 |  TrainLoss: 0.03190 | TestLoss: 0.09228 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 195 |  TrainLoss: 0.03222 | TestLoss: 0.09974 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 196 |  TrainLoss: 0.04369 | TestLoss: 0.09284 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 197 |  TrainLoss: 0.03058 | TestLoss: 0.09323 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 198 |  TrainLoss: 0.02927 | TestLoss: 0.11502 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 199 |  TrainLoss: 0.02972 | TestLoss: 0.09765 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 200 |  TrainLoss: 0.02646 | TestLoss: 0.09873 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 201 |  TrainLoss: 0.02746 | TestLoss: 0.11356 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 202 |  TrainLoss: 0.03054 | TestLoss: 0.10741 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 203 |  TrainLoss: 0.02972 | TestLoss: 0.10119 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 204 |  TrainLoss: 0.02713 | TestLoss: 0.10060 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 205 |  TrainLoss: 0.02844 | TestLoss: 0.10600 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 206 |  TrainLoss: 0.02989 | TestLoss: 0.09512 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 207 |  TrainLoss: 0.03741 | TestLoss: 0.10124 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 208 |  TrainLoss: 0.04867 | TestLoss: 0.11527 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 209 |  TrainLoss: 0.03823 | TestLoss: 0.09195 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 210 |  TrainLoss: 0.07164 | TestLoss: 0.20924 | TestAcc: 0.94433 | TestF1: 0.94\n",
            "Epoch: 211 |  TrainLoss: 0.10608 | TestLoss: 0.15728 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 212 |  TrainLoss: 0.09320 | TestLoss: 0.10347 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 213 |  TrainLoss: 0.06109 | TestLoss: 0.08906 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 214 |  TrainLoss: 0.05052 | TestLoss: 0.09623 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 215 |  TrainLoss: 0.05528 | TestLoss: 0.12879 | TestAcc: 0.96759 | TestF1: 0.97\n",
            "Epoch: 216 |  TrainLoss: 0.04255 | TestLoss: 0.10543 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 217 |  TrainLoss: 0.03674 | TestLoss: 0.09194 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 218 |  TrainLoss: 0.03129 | TestLoss: 0.14087 | TestAcc: 0.96393 | TestF1: 0.96\n",
            "Epoch: 219 |  TrainLoss: 0.04768 | TestLoss: 0.12276 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 220 |  TrainLoss: 0.05844 | TestLoss: 0.09270 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 221 |  TrainLoss: 0.03439 | TestLoss: 0.08815 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 222 |  TrainLoss: 0.03325 | TestLoss: 0.08731 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 223 |  TrainLoss: 0.02918 | TestLoss: 0.09851 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 224 |  TrainLoss: 0.03016 | TestLoss: 0.10139 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 225 |  TrainLoss: 0.02656 | TestLoss: 0.09587 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 226 |  TrainLoss: 0.02748 | TestLoss: 0.09344 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 227 |  TrainLoss: 0.02789 | TestLoss: 0.09427 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 228 |  TrainLoss: 0.02568 | TestLoss: 0.09789 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 229 |  TrainLoss: 0.02670 | TestLoss: 0.09636 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 230 |  TrainLoss: 0.02790 | TestLoss: 0.11440 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 231 |  TrainLoss: 0.03027 | TestLoss: 0.15308 | TestAcc: 0.96184 | TestF1: 0.96\n",
            "Epoch: 232 |  TrainLoss: 0.03261 | TestLoss: 0.12053 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 233 |  TrainLoss: 0.03033 | TestLoss: 0.12299 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 234 |  TrainLoss: 0.03345 | TestLoss: 0.10366 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 235 |  TrainLoss: 0.02694 | TestLoss: 0.09898 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 236 |  TrainLoss: 0.02690 | TestLoss: 0.09957 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 237 |  TrainLoss: 0.02990 | TestLoss: 0.09661 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 238 |  TrainLoss: 0.02419 | TestLoss: 0.09678 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 239 |  TrainLoss: 0.02627 | TestLoss: 0.10372 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 240 |  TrainLoss: 0.02934 | TestLoss: 0.10092 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 241 |  TrainLoss: 0.03266 | TestLoss: 0.09796 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 242 |  TrainLoss: 0.02854 | TestLoss: 0.11090 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 243 |  TrainLoss: 0.03953 | TestLoss: 0.12115 | TestAcc: 0.96524 | TestF1: 0.96\n",
            "Epoch: 244 |  TrainLoss: 0.07548 | TestLoss: 0.14689 | TestAcc: 0.95504 | TestF1: 0.95\n",
            "Epoch: 245 |  TrainLoss: 0.04910 | TestLoss: 0.10719 | TestAcc: 0.96681 | TestF1: 0.97\n",
            "Epoch: 246 |  TrainLoss: 0.04853 | TestLoss: 0.11350 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 247 |  TrainLoss: 0.04302 | TestLoss: 0.08748 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 248 |  TrainLoss: 0.03291 | TestLoss: 0.08924 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 249 |  TrainLoss: 0.02962 | TestLoss: 0.12083 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 250 |  TrainLoss: 0.03116 | TestLoss: 0.10658 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 251 |  TrainLoss: 0.03266 | TestLoss: 0.10683 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 252 |  TrainLoss: 0.02900 | TestLoss: 0.09427 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 253 |  TrainLoss: 0.02479 | TestLoss: 0.11594 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 254 |  TrainLoss: 0.02425 | TestLoss: 0.10308 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 255 |  TrainLoss: 0.02233 | TestLoss: 0.10770 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 256 |  TrainLoss: 0.02294 | TestLoss: 0.11508 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 257 |  TrainLoss: 0.02223 | TestLoss: 0.10026 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 258 |  TrainLoss: 0.01913 | TestLoss: 0.10336 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 259 |  TrainLoss: 0.02201 | TestLoss: 0.11150 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 260 |  TrainLoss: 0.01894 | TestLoss: 0.10222 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 261 |  TrainLoss: 0.01917 | TestLoss: 0.10506 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 262 |  TrainLoss: 0.01981 | TestLoss: 0.10190 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 263 |  TrainLoss: 0.02077 | TestLoss: 0.11219 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 264 |  TrainLoss: 0.01698 | TestLoss: 0.11315 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 265 |  TrainLoss: 0.02117 | TestLoss: 0.11509 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 266 |  TrainLoss: 0.02597 | TestLoss: 0.10262 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 267 |  TrainLoss: 0.02334 | TestLoss: 0.11765 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 268 |  TrainLoss: 0.02153 | TestLoss: 0.17477 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 269 |  TrainLoss: 0.03898 | TestLoss: 0.21811 | TestAcc: 0.95400 | TestF1: 0.96\n",
            "Epoch: 270 |  TrainLoss: 0.06883 | TestLoss: 0.37427 | TestAcc: 0.90434 | TestF1: 0.91\n",
            "Epoch: 271 |  TrainLoss: 0.31038 | TestLoss: 0.10781 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 272 |  TrainLoss: 0.17713 | TestLoss: 0.26768 | TestAcc: 0.89284 | TestF1: 0.90\n",
            "Epoch: 273 |  TrainLoss: 0.11789 | TestLoss: 0.11416 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 274 |  TrainLoss: 0.09408 | TestLoss: 0.10704 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 275 |  TrainLoss: 0.09622 | TestLoss: 0.10718 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 276 |  TrainLoss: 0.08894 | TestLoss: 0.10041 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 277 |  TrainLoss: 0.07371 | TestLoss: 0.09832 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 278 |  TrainLoss: 0.07026 | TestLoss: 0.09991 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 279 |  TrainLoss: 0.07237 | TestLoss: 0.11503 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 280 |  TrainLoss: 0.06445 | TestLoss: 0.09505 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 281 |  TrainLoss: 0.05875 | TestLoss: 0.09382 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 282 |  TrainLoss: 0.05733 | TestLoss: 0.10449 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 283 |  TrainLoss: 0.05917 | TestLoss: 0.10072 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 284 |  TrainLoss: 0.05608 | TestLoss: 0.09577 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 285 |  TrainLoss: 0.05259 | TestLoss: 0.09782 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 286 |  TrainLoss: 0.05249 | TestLoss: 0.11534 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 287 |  TrainLoss: 0.05556 | TestLoss: 0.09272 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 288 |  TrainLoss: 0.04607 | TestLoss: 0.09662 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 289 |  TrainLoss: 0.04551 | TestLoss: 0.15304 | TestAcc: 0.94694 | TestF1: 0.94\n",
            "Epoch: 290 |  TrainLoss: 0.08399 | TestLoss: 0.09830 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 291 |  TrainLoss: 0.05091 | TestLoss: 0.10314 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 292 |  TrainLoss: 0.05295 | TestLoss: 0.09366 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 293 |  TrainLoss: 0.04347 | TestLoss: 0.09646 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 294 |  TrainLoss: 0.03971 | TestLoss: 0.09286 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 295 |  TrainLoss: 0.03880 | TestLoss: 0.09054 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 296 |  TrainLoss: 0.03798 | TestLoss: 0.09344 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 297 |  TrainLoss: 0.04174 | TestLoss: 0.10264 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 298 |  TrainLoss: 0.03678 | TestLoss: 0.09518 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 299 |  TrainLoss: 0.03102 | TestLoss: 0.11421 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 300 |  TrainLoss: 0.03710 | TestLoss: 0.12032 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 301 |  TrainLoss: 0.03495 | TestLoss: 0.10303 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 302 |  TrainLoss: 0.02963 | TestLoss: 0.09728 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 303 |  TrainLoss: 0.02805 | TestLoss: 0.09837 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 304 |  TrainLoss: 0.02873 | TestLoss: 0.09955 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 305 |  TrainLoss: 0.03199 | TestLoss: 0.10741 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 306 |  TrainLoss: 0.03316 | TestLoss: 0.10062 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 307 |  TrainLoss: 0.03190 | TestLoss: 0.10089 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 308 |  TrainLoss: 0.02868 | TestLoss: 0.09936 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 309 |  TrainLoss: 0.02936 | TestLoss: 0.09903 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 310 |  TrainLoss: 0.03644 | TestLoss: 0.15034 | TestAcc: 0.95452 | TestF1: 0.95\n",
            "Epoch: 311 |  TrainLoss: 0.11636 | TestLoss: 0.13809 | TestAcc: 0.95923 | TestF1: 0.96\n",
            "Epoch: 312 |  TrainLoss: 0.06818 | TestLoss: 0.11297 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 313 |  TrainLoss: 0.06535 | TestLoss: 0.12027 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 314 |  TrainLoss: 0.05108 | TestLoss: 0.09359 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 315 |  TrainLoss: 0.04916 | TestLoss: 0.09563 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 316 |  TrainLoss: 0.04282 | TestLoss: 0.09519 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 317 |  TrainLoss: 0.03670 | TestLoss: 0.10145 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 318 |  TrainLoss: 0.03104 | TestLoss: 0.09679 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 319 |  TrainLoss: 0.02828 | TestLoss: 0.10879 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 320 |  TrainLoss: 0.03487 | TestLoss: 0.09602 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 321 |  TrainLoss: 0.03056 | TestLoss: 0.09829 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 322 |  TrainLoss: 0.02863 | TestLoss: 0.09825 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 323 |  TrainLoss: 0.03007 | TestLoss: 0.11052 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 324 |  TrainLoss: 0.02692 | TestLoss: 0.11259 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 325 |  TrainLoss: 0.02708 | TestLoss: 0.09939 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 326 |  TrainLoss: 0.02229 | TestLoss: 0.09902 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 327 |  TrainLoss: 0.02347 | TestLoss: 0.10804 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 328 |  TrainLoss: 0.02312 | TestLoss: 0.11919 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 329 |  TrainLoss: 0.02461 | TestLoss: 0.10328 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 330 |  TrainLoss: 0.02385 | TestLoss: 0.10007 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 331 |  TrainLoss: 0.02623 | TestLoss: 0.10729 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 332 |  TrainLoss: 0.02879 | TestLoss: 0.10538 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 333 |  TrainLoss: 0.02049 | TestLoss: 0.10286 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 334 |  TrainLoss: 0.01862 | TestLoss: 0.10538 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 335 |  TrainLoss: 0.02052 | TestLoss: 0.11084 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 336 |  TrainLoss: 0.01905 | TestLoss: 0.10370 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 337 |  TrainLoss: 0.01934 | TestLoss: 0.12593 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 338 |  TrainLoss: 0.01983 | TestLoss: 0.10625 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 339 |  TrainLoss: 0.01807 | TestLoss: 0.11054 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 340 |  TrainLoss: 0.01866 | TestLoss: 0.10259 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 341 |  TrainLoss: 0.01759 | TestLoss: 0.10681 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 342 |  TrainLoss: 0.01698 | TestLoss: 0.10479 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 343 |  TrainLoss: 0.01844 | TestLoss: 0.12335 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 344 |  TrainLoss: 0.01875 | TestLoss: 0.12321 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 345 |  TrainLoss: 0.01749 | TestLoss: 0.10623 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 346 |  TrainLoss: 0.01652 | TestLoss: 0.10759 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 347 |  TrainLoss: 0.01568 | TestLoss: 0.11242 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 348 |  TrainLoss: 0.01593 | TestLoss: 0.10696 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 349 |  TrainLoss: 0.01551 | TestLoss: 0.11064 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 350 |  TrainLoss: 0.01538 | TestLoss: 0.11572 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 351 |  TrainLoss: 0.01510 | TestLoss: 0.11243 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 352 |  TrainLoss: 0.01551 | TestLoss: 0.11450 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 353 |  TrainLoss: 0.01427 | TestLoss: 0.12327 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 354 |  TrainLoss: 0.01471 | TestLoss: 0.11340 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 355 |  TrainLoss: 0.01546 | TestLoss: 0.12316 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 356 |  TrainLoss: 0.01416 | TestLoss: 0.11034 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 357 |  TrainLoss: 0.01369 | TestLoss: 0.11235 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 358 |  TrainLoss: 0.01342 | TestLoss: 0.14501 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 359 |  TrainLoss: 0.01442 | TestLoss: 0.11320 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 360 |  TrainLoss: 0.01536 | TestLoss: 0.11473 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 361 |  TrainLoss: 0.01381 | TestLoss: 0.12337 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 362 |  TrainLoss: 0.01272 | TestLoss: 0.11560 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 363 |  TrainLoss: 0.01266 | TestLoss: 0.11485 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 364 |  TrainLoss: 0.01360 | TestLoss: 0.11483 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 365 |  TrainLoss: 0.01561 | TestLoss: 0.13955 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 366 |  TrainLoss: 0.02713 | TestLoss: 0.13210 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 367 |  TrainLoss: 0.02529 | TestLoss: 0.11362 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 368 |  TrainLoss: 0.02629 | TestLoss: 0.11117 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 369 |  TrainLoss: 0.01982 | TestLoss: 0.10840 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 370 |  TrainLoss: 0.01544 | TestLoss: 0.11445 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 371 |  TrainLoss: 0.01380 | TestLoss: 0.12995 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 372 |  TrainLoss: 0.01588 | TestLoss: 0.11240 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 373 |  TrainLoss: 0.01533 | TestLoss: 0.11628 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 374 |  TrainLoss: 0.01809 | TestLoss: 0.12414 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 375 |  TrainLoss: 0.01461 | TestLoss: 0.12583 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 376 |  TrainLoss: 0.01631 | TestLoss: 0.11566 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 377 |  TrainLoss: 0.01417 | TestLoss: 0.12035 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 378 |  TrainLoss: 0.01228 | TestLoss: 0.13585 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 379 |  TrainLoss: 0.01282 | TestLoss: 0.11652 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 380 |  TrainLoss: 0.01114 | TestLoss: 0.11836 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 381 |  TrainLoss: 0.01174 | TestLoss: 0.13319 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 382 |  TrainLoss: 0.01078 | TestLoss: 0.12694 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 383 |  TrainLoss: 0.00981 | TestLoss: 0.12606 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 384 |  TrainLoss: 0.01032 | TestLoss: 0.12383 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 385 |  TrainLoss: 0.01147 | TestLoss: 0.12180 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 386 |  TrainLoss: 0.00975 | TestLoss: 0.13067 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 387 |  TrainLoss: 0.00881 | TestLoss: 0.12228 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 388 |  TrainLoss: 0.01227 | TestLoss: 0.12722 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 389 |  TrainLoss: 0.01840 | TestLoss: 0.12726 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 390 |  TrainLoss: 0.02269 | TestLoss: 0.11864 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 391 |  TrainLoss: 0.02098 | TestLoss: 0.13581 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 392 |  TrainLoss: 0.01319 | TestLoss: 0.12719 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 393 |  TrainLoss: 0.00976 | TestLoss: 0.12304 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 394 |  TrainLoss: 0.00946 | TestLoss: 0.12583 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 395 |  TrainLoss: 0.00829 | TestLoss: 0.12590 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 396 |  TrainLoss: 0.01020 | TestLoss: 0.13881 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 397 |  TrainLoss: 0.00922 | TestLoss: 0.14089 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 398 |  TrainLoss: 0.00779 | TestLoss: 0.12778 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 399 |  TrainLoss: 0.00955 | TestLoss: 0.19252 | TestAcc: 0.96315 | TestF1: 0.96\n",
            "Epoch: 400 |  TrainLoss: 0.01588 | TestLoss: 0.16481 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 401 |  TrainLoss: 0.01182 | TestLoss: 0.13352 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 402 |  TrainLoss: 0.01549 | TestLoss: 0.12277 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 403 |  TrainLoss: 0.01107 | TestLoss: 0.13729 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 404 |  TrainLoss: 0.00912 | TestLoss: 0.12381 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 405 |  TrainLoss: 0.00896 | TestLoss: 0.13114 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 406 |  TrainLoss: 0.01044 | TestLoss: 0.16157 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 407 |  TrainLoss: 0.01153 | TestLoss: 0.15996 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 408 |  TrainLoss: 0.01167 | TestLoss: 0.13024 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 409 |  TrainLoss: 0.00860 | TestLoss: 0.13215 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 410 |  TrainLoss: 0.01068 | TestLoss: 0.16131 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 411 |  TrainLoss: 0.01174 | TestLoss: 0.15013 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 412 |  TrainLoss: 0.01038 | TestLoss: 0.12504 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 413 |  TrainLoss: 0.00629 | TestLoss: 0.14030 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 414 |  TrainLoss: 0.00712 | TestLoss: 0.13894 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 415 |  TrainLoss: 0.00666 | TestLoss: 0.12986 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 416 |  TrainLoss: 0.00733 | TestLoss: 0.15269 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 417 |  TrainLoss: 0.00776 | TestLoss: 0.13700 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 418 |  TrainLoss: 0.00759 | TestLoss: 0.13150 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 419 |  TrainLoss: 0.01095 | TestLoss: 0.13564 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 420 |  TrainLoss: 0.01429 | TestLoss: 0.15180 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 421 |  TrainLoss: 0.01038 | TestLoss: 0.14618 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 422 |  TrainLoss: 0.00774 | TestLoss: 0.12240 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 423 |  TrainLoss: 0.00691 | TestLoss: 0.15891 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 424 |  TrainLoss: 0.00681 | TestLoss: 0.14217 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 425 |  TrainLoss: 0.00661 | TestLoss: 0.16707 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 426 |  TrainLoss: 0.01202 | TestLoss: 0.13005 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 427 |  TrainLoss: 0.00881 | TestLoss: 0.13782 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 428 |  TrainLoss: 0.01326 | TestLoss: 0.16864 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 429 |  TrainLoss: 0.00818 | TestLoss: 0.13133 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 430 |  TrainLoss: 0.00587 | TestLoss: 0.14890 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 431 |  TrainLoss: 0.00614 | TestLoss: 0.13841 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 432 |  TrainLoss: 0.00527 | TestLoss: 0.15989 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 433 |  TrainLoss: 0.00579 | TestLoss: 0.14085 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 434 |  TrainLoss: 0.00635 | TestLoss: 0.15559 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 435 |  TrainLoss: 0.00447 | TestLoss: 0.13660 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 436 |  TrainLoss: 0.00586 | TestLoss: 0.15101 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 437 |  TrainLoss: 0.00575 | TestLoss: 0.14979 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 438 |  TrainLoss: 0.00471 | TestLoss: 0.14274 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 439 |  TrainLoss: 0.00546 | TestLoss: 0.14921 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 440 |  TrainLoss: 0.00493 | TestLoss: 0.14239 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 441 |  TrainLoss: 0.00448 | TestLoss: 0.14910 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 442 |  TrainLoss: 0.00467 | TestLoss: 0.14229 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 443 |  TrainLoss: 0.00512 | TestLoss: 0.16669 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 444 |  TrainLoss: 0.00529 | TestLoss: 0.14461 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 445 |  TrainLoss: 0.00511 | TestLoss: 0.15354 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 446 |  TrainLoss: 0.00489 | TestLoss: 0.13818 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 447 |  TrainLoss: 0.00494 | TestLoss: 0.15413 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 448 |  TrainLoss: 0.00435 | TestLoss: 0.14983 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 449 |  TrainLoss: 0.00463 | TestLoss: 0.15141 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 450 |  TrainLoss: 0.00396 | TestLoss: 0.14976 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 451 |  TrainLoss: 0.00373 | TestLoss: 0.15229 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 452 |  TrainLoss: 0.00385 | TestLoss: 0.15596 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 453 |  TrainLoss: 0.00422 | TestLoss: 0.15126 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 454 |  TrainLoss: 0.00403 | TestLoss: 0.15566 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 455 |  TrainLoss: 0.00444 | TestLoss: 0.15275 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 456 |  TrainLoss: 0.00484 | TestLoss: 0.14655 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 457 |  TrainLoss: 0.00509 | TestLoss: 0.24025 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 458 |  TrainLoss: 0.00454 | TestLoss: 0.15252 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 459 |  TrainLoss: 0.00533 | TestLoss: 0.22716 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 460 |  TrainLoss: 0.00458 | TestLoss: 0.15015 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 461 |  TrainLoss: 0.00485 | TestLoss: 0.15771 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 462 |  TrainLoss: 0.00386 | TestLoss: 0.15070 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 463 |  TrainLoss: 0.00362 | TestLoss: 0.15767 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 464 |  TrainLoss: 0.00384 | TestLoss: 0.15496 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 465 |  TrainLoss: 0.00391 | TestLoss: 0.15699 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 466 |  TrainLoss: 0.00347 | TestLoss: 0.15560 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 467 |  TrainLoss: 0.00411 | TestLoss: 0.16203 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 468 |  TrainLoss: 0.00384 | TestLoss: 0.15583 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 469 |  TrainLoss: 0.00408 | TestLoss: 0.16438 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 470 |  TrainLoss: 0.00433 | TestLoss: 0.15527 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 471 |  TrainLoss: 0.00373 | TestLoss: 0.18405 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 472 |  TrainLoss: 0.00351 | TestLoss: 0.15731 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 473 |  TrainLoss: 0.00337 | TestLoss: 0.15896 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 474 |  TrainLoss: 0.00337 | TestLoss: 0.18258 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 475 |  TrainLoss: 0.00339 | TestLoss: 0.15691 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 476 |  TrainLoss: 0.00365 | TestLoss: 0.15670 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 477 |  TrainLoss: 0.00392 | TestLoss: 0.21060 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 478 |  TrainLoss: 0.00410 | TestLoss: 0.15497 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 479 |  TrainLoss: 0.00334 | TestLoss: 0.18541 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 480 |  TrainLoss: 0.00362 | TestLoss: 0.16047 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 481 |  TrainLoss: 0.00305 | TestLoss: 0.18309 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 482 |  TrainLoss: 0.00322 | TestLoss: 0.18440 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 483 |  TrainLoss: 0.00387 | TestLoss: 0.18330 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 484 |  TrainLoss: 0.00369 | TestLoss: 0.18250 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 485 |  TrainLoss: 0.00358 | TestLoss: 0.15896 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 486 |  TrainLoss: 0.00321 | TestLoss: 0.23398 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 487 |  TrainLoss: 0.00321 | TestLoss: 0.20688 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 488 |  TrainLoss: 0.00355 | TestLoss: 0.20884 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 489 |  TrainLoss: 0.00344 | TestLoss: 0.15775 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 490 |  TrainLoss: 0.00490 | TestLoss: 0.20447 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 491 |  TrainLoss: 0.00550 | TestLoss: 0.26854 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 492 |  TrainLoss: 0.00675 | TestLoss: 0.27851 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 493 |  TrainLoss: 0.00432 | TestLoss: 0.23607 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 494 |  TrainLoss: 0.00467 | TestLoss: 0.21244 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 495 |  TrainLoss: 0.00437 | TestLoss: 0.16416 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 496 |  TrainLoss: 0.00378 | TestLoss: 0.16132 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 497 |  TrainLoss: 0.00341 | TestLoss: 0.15482 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 498 |  TrainLoss: 0.00393 | TestLoss: 0.15806 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 499 |  TrainLoss: 0.00418 | TestLoss: 0.31150 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Best WLoss: 0.00002 | Best Epoch: 98\n"
          ]
        }
      ],
      "source": [
        "#printing out the epoch at lowest wloss. \n",
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  #initialize with a large value\n",
        "\n",
        "#without dropout training results\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  #update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "#print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fffc015",
      "metadata": {
        "id": "5fffc015"
      },
      "source": [
        "# Model 2: \n",
        "GraphSage + Content with hyperparameters as defined by the paper and 3 Layer MLP\n",
        "\n",
        "(Embedding size = 128, batch size= 128, l2 Regularization = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b6971a3f",
      "metadata": {
        "id": "b6971a3f"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.l2_reg_weight = l2_reg_weight\n",
        "        \n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4], hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5], out_channels)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "        self.dp3 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h).relu()\n",
        "        h = self.dp3(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3f40d635",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f40d635",
        "outputId": "8bce3a7a-ea2c-4572-bbd7-aaeed166ce05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device) ##setting embedding size=128, batch size=128 and l2 regularization weight as 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5543ef89",
      "metadata": {
        "id": "5543ef89"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "01bde71a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01bde71a",
        "outputId": "54511b58-0b0e-4200-c99f-81a1eca04c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69325 | TestLoss: 0.69276 | TestAcc: 0.50078 | TestF1: 0.67\n",
            "Epoch: 01 |  TrainLoss: 0.69297 | TestLoss: 0.69222 | TestAcc: 0.50078 | TestF1: 0.67\n",
            "Epoch: 02 |  TrainLoss: 0.69206 | TestLoss: 0.69128 | TestAcc: 0.88892 | TestF1: 0.90\n",
            "Epoch: 03 |  TrainLoss: 0.69082 | TestLoss: 0.68942 | TestAcc: 0.77967 | TestF1: 0.72\n",
            "Epoch: 04 |  TrainLoss: 0.68892 | TestLoss: 0.68572 | TestAcc: 0.68688 | TestF1: 0.55\n",
            "Epoch: 05 |  TrainLoss: 0.68445 | TestLoss: 0.67806 | TestAcc: 0.73523 | TestF1: 0.64\n",
            "Epoch: 06 |  TrainLoss: 0.67336 | TestLoss: 0.66246 | TestAcc: 0.88657 | TestF1: 0.90\n",
            "Epoch: 07 |  TrainLoss: 0.65732 | TestLoss: 0.65103 | TestAcc: 0.58521 | TestF1: 0.29\n",
            "Epoch: 08 |  TrainLoss: 0.64153 | TestLoss: 0.61782 | TestAcc: 0.64428 | TestF1: 0.45\n",
            "Epoch: 09 |  TrainLoss: 0.61380 | TestLoss: 0.55917 | TestAcc: 0.81652 | TestF1: 0.78\n",
            "Epoch: 10 |  TrainLoss: 0.55531 | TestLoss: 0.50429 | TestAcc: 0.79613 | TestF1: 0.75\n",
            "Epoch: 11 |  TrainLoss: 0.48437 | TestLoss: 0.40933 | TestAcc: 0.93492 | TestF1: 0.94\n",
            "Epoch: 12 |  TrainLoss: 0.40528 | TestLoss: 0.35838 | TestAcc: 0.84631 | TestF1: 0.82\n",
            "Epoch: 13 |  TrainLoss: 0.33160 | TestLoss: 0.25214 | TestAcc: 0.93936 | TestF1: 0.94\n",
            "Epoch: 14 |  TrainLoss: 0.30351 | TestLoss: 0.21606 | TestAcc: 0.93779 | TestF1: 0.94\n",
            "Epoch: 15 |  TrainLoss: 0.26394 | TestLoss: 0.27468 | TestAcc: 0.89441 | TestF1: 0.90\n",
            "Epoch: 16 |  TrainLoss: 0.22574 | TestLoss: 0.17360 | TestAcc: 0.95139 | TestF1: 0.95\n",
            "Epoch: 17 |  TrainLoss: 0.18946 | TestLoss: 0.16330 | TestAcc: 0.94877 | TestF1: 0.95\n",
            "Epoch: 18 |  TrainLoss: 0.18806 | TestLoss: 0.20602 | TestAcc: 0.91871 | TestF1: 0.91\n",
            "Epoch: 19 |  TrainLoss: 0.16991 | TestLoss: 0.16590 | TestAcc: 0.94433 | TestF1: 0.94\n",
            "Epoch: 20 |  TrainLoss: 0.16515 | TestLoss: 0.15925 | TestAcc: 0.94668 | TestF1: 0.95\n",
            "Epoch: 21 |  TrainLoss: 0.15299 | TestLoss: 0.13199 | TestAcc: 0.96001 | TestF1: 0.96\n",
            "Epoch: 22 |  TrainLoss: 0.13736 | TestLoss: 0.12496 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 23 |  TrainLoss: 0.13934 | TestLoss: 0.13072 | TestAcc: 0.95714 | TestF1: 0.96\n",
            "Epoch: 24 |  TrainLoss: 0.13840 | TestLoss: 0.14670 | TestAcc: 0.95086 | TestF1: 0.95\n",
            "Epoch: 25 |  TrainLoss: 0.13594 | TestLoss: 0.11873 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 26 |  TrainLoss: 0.12994 | TestLoss: 0.11305 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 27 |  TrainLoss: 0.12324 | TestLoss: 0.11894 | TestAcc: 0.96315 | TestF1: 0.96\n",
            "Epoch: 28 |  TrainLoss: 0.12634 | TestLoss: 0.11158 | TestAcc: 0.96576 | TestF1: 0.97\n",
            "Epoch: 29 |  TrainLoss: 0.12198 | TestLoss: 0.10768 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 30 |  TrainLoss: 0.11212 | TestLoss: 0.10744 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 31 |  TrainLoss: 0.11364 | TestLoss: 0.10505 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 32 |  TrainLoss: 0.10771 | TestLoss: 0.10513 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 33 |  TrainLoss: 0.10726 | TestLoss: 0.10310 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 34 |  TrainLoss: 0.10765 | TestLoss: 0.10497 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 35 |  TrainLoss: 0.12428 | TestLoss: 0.14834 | TestAcc: 0.95321 | TestF1: 0.95\n",
            "Epoch: 36 |  TrainLoss: 0.12067 | TestLoss: 0.11989 | TestAcc: 0.96419 | TestF1: 0.96\n",
            "Epoch: 37 |  TrainLoss: 0.11017 | TestLoss: 0.10341 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 38 |  TrainLoss: 0.10594 | TestLoss: 0.11402 | TestAcc: 0.96550 | TestF1: 0.97\n",
            "Epoch: 39 |  TrainLoss: 0.11246 | TestLoss: 0.11151 | TestAcc: 0.96811 | TestF1: 0.97\n",
            "Epoch: 40 |  TrainLoss: 0.10942 | TestLoss: 0.09974 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 41 |  TrainLoss: 0.11830 | TestLoss: 0.11810 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 42 |  TrainLoss: 0.14174 | TestLoss: 0.19278 | TestAcc: 0.93753 | TestF1: 0.94\n",
            "Epoch: 43 |  TrainLoss: 0.16092 | TestLoss: 0.11785 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 44 |  TrainLoss: 0.11298 | TestLoss: 0.09971 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 45 |  TrainLoss: 0.10289 | TestLoss: 0.10282 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 46 |  TrainLoss: 0.09861 | TestLoss: 0.09663 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 47 |  TrainLoss: 0.09078 | TestLoss: 0.10011 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 48 |  TrainLoss: 0.09790 | TestLoss: 0.09314 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 49 |  TrainLoss: 0.08753 | TestLoss: 0.09527 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 50 |  TrainLoss: 0.09763 | TestLoss: 0.09282 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 51 |  TrainLoss: 0.08529 | TestLoss: 0.09872 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 52 |  TrainLoss: 0.09349 | TestLoss: 0.09303 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 53 |  TrainLoss: 0.09278 | TestLoss: 0.10203 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 54 |  TrainLoss: 0.09359 | TestLoss: 0.11114 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 55 |  TrainLoss: 0.10390 | TestLoss: 0.12118 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 56 |  TrainLoss: 0.09388 | TestLoss: 0.09373 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 57 |  TrainLoss: 0.08388 | TestLoss: 0.09054 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 58 |  TrainLoss: 0.08390 | TestLoss: 0.09248 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 59 |  TrainLoss: 0.08668 | TestLoss: 0.09175 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 60 |  TrainLoss: 0.07888 | TestLoss: 0.10178 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 61 |  TrainLoss: 0.09139 | TestLoss: 0.09744 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 62 |  TrainLoss: 0.08779 | TestLoss: 0.09219 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 63 |  TrainLoss: 0.08687 | TestLoss: 0.09327 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 64 |  TrainLoss: 0.08101 | TestLoss: 0.08843 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 65 |  TrainLoss: 0.09140 | TestLoss: 0.11452 | TestAcc: 0.96524 | TestF1: 0.96\n",
            "Epoch: 66 |  TrainLoss: 0.09463 | TestLoss: 0.08720 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 67 |  TrainLoss: 0.08097 | TestLoss: 0.08706 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 68 |  TrainLoss: 0.07719 | TestLoss: 0.09183 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 69 |  TrainLoss: 0.07885 | TestLoss: 0.08870 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 70 |  TrainLoss: 0.07838 | TestLoss: 0.09017 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 71 |  TrainLoss: 0.08045 | TestLoss: 0.08776 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 72 |  TrainLoss: 0.07731 | TestLoss: 0.09780 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 73 |  TrainLoss: 0.07272 | TestLoss: 0.09014 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 74 |  TrainLoss: 0.07128 | TestLoss: 0.08712 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 75 |  TrainLoss: 0.06862 | TestLoss: 0.08632 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 76 |  TrainLoss: 0.07119 | TestLoss: 0.08511 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 77 |  TrainLoss: 0.06829 | TestLoss: 0.10093 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 78 |  TrainLoss: 0.08507 | TestLoss: 0.12363 | TestAcc: 0.96262 | TestF1: 0.96\n",
            "Epoch: 79 |  TrainLoss: 0.10917 | TestLoss: 0.10168 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 80 |  TrainLoss: 0.08457 | TestLoss: 0.08543 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 81 |  TrainLoss: 0.08506 | TestLoss: 0.08750 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 82 |  TrainLoss: 0.07885 | TestLoss: 0.10094 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 83 |  TrainLoss: 0.08368 | TestLoss: 0.08601 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 84 |  TrainLoss: 0.07825 | TestLoss: 0.10117 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 85 |  TrainLoss: 0.07978 | TestLoss: 0.08520 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 86 |  TrainLoss: 0.07280 | TestLoss: 0.09153 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 87 |  TrainLoss: 0.07092 | TestLoss: 0.10444 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 88 |  TrainLoss: 0.07147 | TestLoss: 0.09047 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 89 |  TrainLoss: 0.06726 | TestLoss: 0.08292 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 90 |  TrainLoss: 0.06920 | TestLoss: 0.11457 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 91 |  TrainLoss: 0.08521 | TestLoss: 0.10851 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 92 |  TrainLoss: 0.08070 | TestLoss: 0.11581 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 93 |  TrainLoss: 0.07389 | TestLoss: 0.11176 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 94 |  TrainLoss: 0.06993 | TestLoss: 0.09035 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 95 |  TrainLoss: 0.06500 | TestLoss: 0.08346 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 96 |  TrainLoss: 0.06101 | TestLoss: 0.08603 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 97 |  TrainLoss: 0.06540 | TestLoss: 0.08293 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 98 |  TrainLoss: 0.06522 | TestLoss: 0.08348 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 99 |  TrainLoss: 0.06123 | TestLoss: 0.08347 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 100 |  TrainLoss: 0.05805 | TestLoss: 0.08557 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 101 |  TrainLoss: 0.05702 | TestLoss: 0.08265 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 102 |  TrainLoss: 0.05619 | TestLoss: 0.08421 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 103 |  TrainLoss: 0.05912 | TestLoss: 0.08352 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 104 |  TrainLoss: 0.07446 | TestLoss: 0.11523 | TestAcc: 0.96524 | TestF1: 0.96\n",
            "Epoch: 105 |  TrainLoss: 0.11173 | TestLoss: 0.13931 | TestAcc: 0.95818 | TestF1: 0.96\n",
            "Epoch: 106 |  TrainLoss: 0.10428 | TestLoss: 0.09984 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 107 |  TrainLoss: 0.08101 | TestLoss: 0.11410 | TestAcc: 0.96367 | TestF1: 0.96\n",
            "Epoch: 108 |  TrainLoss: 0.09616 | TestLoss: 0.08531 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 109 |  TrainLoss: 0.08495 | TestLoss: 0.09836 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 110 |  TrainLoss: 0.07513 | TestLoss: 0.08613 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 111 |  TrainLoss: 0.06260 | TestLoss: 0.08102 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 112 |  TrainLoss: 0.06161 | TestLoss: 0.08254 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 113 |  TrainLoss: 0.05880 | TestLoss: 0.08201 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 114 |  TrainLoss: 0.05934 | TestLoss: 0.08491 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 115 |  TrainLoss: 0.07407 | TestLoss: 0.09432 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 116 |  TrainLoss: 0.05931 | TestLoss: 0.08381 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 117 |  TrainLoss: 0.05295 | TestLoss: 0.09137 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 118 |  TrainLoss: 0.05456 | TestLoss: 0.09424 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 119 |  TrainLoss: 0.05716 | TestLoss: 0.08620 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 120 |  TrainLoss: 0.05319 | TestLoss: 0.08340 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 121 |  TrainLoss: 0.05156 | TestLoss: 0.08240 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 122 |  TrainLoss: 0.04712 | TestLoss: 0.08726 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 123 |  TrainLoss: 0.05493 | TestLoss: 0.10105 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 124 |  TrainLoss: 0.06753 | TestLoss: 0.09519 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 125 |  TrainLoss: 0.05702 | TestLoss: 0.08265 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 126 |  TrainLoss: 0.05292 | TestLoss: 0.08470 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 127 |  TrainLoss: 0.04806 | TestLoss: 0.08440 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 128 |  TrainLoss: 0.05535 | TestLoss: 0.08972 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 129 |  TrainLoss: 0.07121 | TestLoss: 0.09198 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 130 |  TrainLoss: 0.06146 | TestLoss: 0.09572 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 131 |  TrainLoss: 0.05030 | TestLoss: 0.08472 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 132 |  TrainLoss: 0.04842 | TestLoss: 0.08657 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 133 |  TrainLoss: 0.05501 | TestLoss: 0.09348 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 134 |  TrainLoss: 0.05385 | TestLoss: 0.08765 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 135 |  TrainLoss: 0.04968 | TestLoss: 0.08729 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 136 |  TrainLoss: 0.04752 | TestLoss: 0.08678 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 137 |  TrainLoss: 0.04801 | TestLoss: 0.08381 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 138 |  TrainLoss: 0.04972 | TestLoss: 0.10187 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 139 |  TrainLoss: 0.05747 | TestLoss: 0.11094 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 140 |  TrainLoss: 0.05214 | TestLoss: 0.11938 | TestAcc: 0.96707 | TestF1: 0.97\n",
            "Epoch: 141 |  TrainLoss: 0.07557 | TestLoss: 0.10069 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 142 |  TrainLoss: 0.07338 | TestLoss: 0.09461 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 143 |  TrainLoss: 0.05022 | TestLoss: 0.11641 | TestAcc: 0.96733 | TestF1: 0.97\n",
            "Epoch: 144 |  TrainLoss: 0.06050 | TestLoss: 0.09337 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 145 |  TrainLoss: 0.06350 | TestLoss: 0.11872 | TestAcc: 0.96759 | TestF1: 0.97\n",
            "Epoch: 146 |  TrainLoss: 0.06719 | TestLoss: 0.08207 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 147 |  TrainLoss: 0.04536 | TestLoss: 0.08655 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 148 |  TrainLoss: 0.05085 | TestLoss: 0.09833 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 149 |  TrainLoss: 0.05425 | TestLoss: 0.08955 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 150 |  TrainLoss: 0.04880 | TestLoss: 0.08581 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 151 |  TrainLoss: 0.04278 | TestLoss: 0.09002 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 152 |  TrainLoss: 0.04937 | TestLoss: 0.08962 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 153 |  TrainLoss: 0.04225 | TestLoss: 0.08445 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 154 |  TrainLoss: 0.04841 | TestLoss: 0.10494 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 155 |  TrainLoss: 0.05900 | TestLoss: 0.11623 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 156 |  TrainLoss: 0.05079 | TestLoss: 0.08448 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 157 |  TrainLoss: 0.04195 | TestLoss: 0.08225 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 158 |  TrainLoss: 0.04124 | TestLoss: 0.09287 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 159 |  TrainLoss: 0.03813 | TestLoss: 0.13843 | TestAcc: 0.96367 | TestF1: 0.96\n",
            "Epoch: 160 |  TrainLoss: 0.05064 | TestLoss: 0.14117 | TestAcc: 0.96498 | TestF1: 0.97\n",
            "Epoch: 161 |  TrainLoss: 0.05171 | TestLoss: 0.09888 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 162 |  TrainLoss: 0.05096 | TestLoss: 0.08694 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 163 |  TrainLoss: 0.04325 | TestLoss: 0.08358 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 164 |  TrainLoss: 0.04344 | TestLoss: 0.12101 | TestAcc: 0.96210 | TestF1: 0.96\n",
            "Epoch: 165 |  TrainLoss: 0.06091 | TestLoss: 0.10901 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 166 |  TrainLoss: 0.05516 | TestLoss: 0.10296 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 167 |  TrainLoss: 0.05538 | TestLoss: 0.09701 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 168 |  TrainLoss: 0.05333 | TestLoss: 0.08363 | TestAcc: 0.98040 | TestF1: 0.98\n",
            "Epoch: 169 |  TrainLoss: 0.04175 | TestLoss: 0.09409 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 170 |  TrainLoss: 0.03804 | TestLoss: 0.08687 | TestAcc: 0.98040 | TestF1: 0.98\n",
            "Epoch: 171 |  TrainLoss: 0.03682 | TestLoss: 0.11707 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 172 |  TrainLoss: 0.04116 | TestLoss: 0.08577 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 173 |  TrainLoss: 0.03491 | TestLoss: 0.08582 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 174 |  TrainLoss: 0.03589 | TestLoss: 0.08768 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 175 |  TrainLoss: 0.03262 | TestLoss: 0.09861 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 176 |  TrainLoss: 0.03885 | TestLoss: 0.10110 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 177 |  TrainLoss: 0.03520 | TestLoss: 0.10546 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 178 |  TrainLoss: 0.03815 | TestLoss: 0.09157 | TestAcc: 0.98040 | TestF1: 0.98\n",
            "Epoch: 179 |  TrainLoss: 0.03745 | TestLoss: 0.10329 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 180 |  TrainLoss: 0.03928 | TestLoss: 0.11591 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 181 |  TrainLoss: 0.03786 | TestLoss: 0.08963 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 182 |  TrainLoss: 0.03104 | TestLoss: 0.08829 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 183 |  TrainLoss: 0.03990 | TestLoss: 0.09181 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 184 |  TrainLoss: 0.05311 | TestLoss: 0.10032 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 185 |  TrainLoss: 0.03790 | TestLoss: 0.09630 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 186 |  TrainLoss: 0.03638 | TestLoss: 0.09984 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 187 |  TrainLoss: 0.03585 | TestLoss: 0.08872 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 188 |  TrainLoss: 0.03174 | TestLoss: 0.09372 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 189 |  TrainLoss: 0.03275 | TestLoss: 0.08931 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 190 |  TrainLoss: 0.02902 | TestLoss: 0.09021 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 191 |  TrainLoss: 0.02850 | TestLoss: 0.09167 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 192 |  TrainLoss: 0.04357 | TestLoss: 0.14702 | TestAcc: 0.95766 | TestF1: 0.96\n",
            "Epoch: 193 |  TrainLoss: 0.05632 | TestLoss: 0.11957 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 194 |  TrainLoss: 0.05223 | TestLoss: 0.09291 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 195 |  TrainLoss: 0.04930 | TestLoss: 0.08664 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 196 |  TrainLoss: 0.04321 | TestLoss: 0.09764 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 197 |  TrainLoss: 0.04335 | TestLoss: 0.10138 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 198 |  TrainLoss: 0.03388 | TestLoss: 0.10073 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 199 |  TrainLoss: 0.03445 | TestLoss: 0.09782 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 200 |  TrainLoss: 0.02880 | TestLoss: 0.09471 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 201 |  TrainLoss: 0.02679 | TestLoss: 0.09683 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 202 |  TrainLoss: 0.02603 | TestLoss: 0.10581 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 203 |  TrainLoss: 0.02497 | TestLoss: 0.09464 | TestAcc: 0.98040 | TestF1: 0.98\n",
            "Epoch: 204 |  TrainLoss: 0.02451 | TestLoss: 0.11780 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 205 |  TrainLoss: 0.02643 | TestLoss: 0.11782 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 206 |  TrainLoss: 0.02838 | TestLoss: 0.09432 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 207 |  TrainLoss: 0.02699 | TestLoss: 0.09497 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 208 |  TrainLoss: 0.02223 | TestLoss: 0.09547 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 209 |  TrainLoss: 0.02303 | TestLoss: 0.09528 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 210 |  TrainLoss: 0.02226 | TestLoss: 0.09951 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 211 |  TrainLoss: 0.02352 | TestLoss: 0.09788 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 212 |  TrainLoss: 0.02330 | TestLoss: 0.09936 | TestAcc: 0.98092 | TestF1: 0.98\n",
            "Epoch: 213 |  TrainLoss: 0.02513 | TestLoss: 0.09708 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 214 |  TrainLoss: 0.02224 | TestLoss: 0.09624 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 215 |  TrainLoss: 0.02321 | TestLoss: 0.10191 | TestAcc: 0.98066 | TestF1: 0.98\n",
            "Epoch: 216 |  TrainLoss: 0.02295 | TestLoss: 0.10282 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 217 |  TrainLoss: 0.02287 | TestLoss: 0.09974 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 218 |  TrainLoss: 0.02203 | TestLoss: 0.10564 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 219 |  TrainLoss: 0.02237 | TestLoss: 0.10554 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 220 |  TrainLoss: 0.02741 | TestLoss: 0.15584 | TestAcc: 0.96367 | TestF1: 0.96\n",
            "Epoch: 221 |  TrainLoss: 0.04241 | TestLoss: 0.10157 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 222 |  TrainLoss: 0.03126 | TestLoss: 0.09651 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 223 |  TrainLoss: 0.03073 | TestLoss: 0.10846 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 224 |  TrainLoss: 0.03005 | TestLoss: 0.12579 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 225 |  TrainLoss: 0.02643 | TestLoss: 0.10680 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 226 |  TrainLoss: 0.02715 | TestLoss: 0.11848 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 227 |  TrainLoss: 0.02305 | TestLoss: 0.12527 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 228 |  TrainLoss: 0.02789 | TestLoss: 0.12610 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 229 |  TrainLoss: 0.04157 | TestLoss: 0.14053 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 230 |  TrainLoss: 0.04837 | TestLoss: 0.14365 | TestAcc: 0.96367 | TestF1: 0.96\n",
            "Epoch: 231 |  TrainLoss: 0.04575 | TestLoss: 0.13299 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 232 |  TrainLoss: 0.03671 | TestLoss: 0.11168 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 233 |  TrainLoss: 0.02945 | TestLoss: 0.09966 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 234 |  TrainLoss: 0.02405 | TestLoss: 0.09732 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 235 |  TrainLoss: 0.02262 | TestLoss: 0.10300 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 236 |  TrainLoss: 0.01983 | TestLoss: 0.10279 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 237 |  TrainLoss: 0.02041 | TestLoss: 0.10893 | TestAcc: 0.97935 | TestF1: 0.98\n",
            "Epoch: 238 |  TrainLoss: 0.01996 | TestLoss: 0.11168 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 239 |  TrainLoss: 0.01866 | TestLoss: 0.10055 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 240 |  TrainLoss: 0.01770 | TestLoss: 0.10614 | TestAcc: 0.98014 | TestF1: 0.98\n",
            "Epoch: 241 |  TrainLoss: 0.01858 | TestLoss: 0.10932 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 242 |  TrainLoss: 0.01852 | TestLoss: 0.10796 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 243 |  TrainLoss: 0.01711 | TestLoss: 0.10992 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 244 |  TrainLoss: 0.01730 | TestLoss: 0.11339 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 245 |  TrainLoss: 0.01693 | TestLoss: 0.10533 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 246 |  TrainLoss: 0.01719 | TestLoss: 0.10593 | TestAcc: 0.97961 | TestF1: 0.98\n",
            "Epoch: 247 |  TrainLoss: 0.01856 | TestLoss: 0.10573 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 248 |  TrainLoss: 0.01936 | TestLoss: 0.12557 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 249 |  TrainLoss: 0.01719 | TestLoss: 0.12318 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 250 |  TrainLoss: 0.01690 | TestLoss: 0.12347 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 251 |  TrainLoss: 0.01706 | TestLoss: 0.10801 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 252 |  TrainLoss: 0.01793 | TestLoss: 0.10620 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 253 |  TrainLoss: 0.01680 | TestLoss: 0.10728 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 254 |  TrainLoss: 0.01627 | TestLoss: 0.11183 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 255 |  TrainLoss: 0.01430 | TestLoss: 0.11404 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 256 |  TrainLoss: 0.01387 | TestLoss: 0.10962 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 257 |  TrainLoss: 0.01467 | TestLoss: 0.11218 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 258 |  TrainLoss: 0.01528 | TestLoss: 0.11234 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 259 |  TrainLoss: 0.01403 | TestLoss: 0.12043 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 260 |  TrainLoss: 0.01301 | TestLoss: 0.11272 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 261 |  TrainLoss: 0.01558 | TestLoss: 0.12409 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 262 |  TrainLoss: 0.01375 | TestLoss: 0.12991 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 263 |  TrainLoss: 0.01335 | TestLoss: 0.11830 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 264 |  TrainLoss: 0.01511 | TestLoss: 0.12744 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 265 |  TrainLoss: 0.01376 | TestLoss: 0.12471 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 266 |  TrainLoss: 0.01380 | TestLoss: 0.12501 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 267 |  TrainLoss: 0.01328 | TestLoss: 0.11923 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 268 |  TrainLoss: 0.01317 | TestLoss: 0.12278 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 269 |  TrainLoss: 0.01301 | TestLoss: 0.13120 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 270 |  TrainLoss: 0.01174 | TestLoss: 0.12264 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 271 |  TrainLoss: 0.01504 | TestLoss: 0.11686 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 272 |  TrainLoss: 0.01235 | TestLoss: 0.12646 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 273 |  TrainLoss: 0.01401 | TestLoss: 0.13019 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 274 |  TrainLoss: 0.01345 | TestLoss: 0.13992 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 275 |  TrainLoss: 0.02347 | TestLoss: 0.22173 | TestAcc: 0.95452 | TestF1: 0.96\n",
            "Epoch: 276 |  TrainLoss: 0.03360 | TestLoss: 0.26848 | TestAcc: 0.93387 | TestF1: 0.94\n",
            "Epoch: 277 |  TrainLoss: 0.13457 | TestLoss: 0.17581 | TestAcc: 0.95243 | TestF1: 0.95\n",
            "Epoch: 278 |  TrainLoss: 0.07870 | TestLoss: 0.13052 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 279 |  TrainLoss: 0.06685 | TestLoss: 0.13521 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 280 |  TrainLoss: 0.04800 | TestLoss: 0.14853 | TestAcc: 0.95844 | TestF1: 0.96\n",
            "Epoch: 281 |  TrainLoss: 0.03591 | TestLoss: 0.10335 | TestAcc: 0.97909 | TestF1: 0.98\n",
            "Epoch: 282 |  TrainLoss: 0.03456 | TestLoss: 0.09005 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 283 |  TrainLoss: 0.02339 | TestLoss: 0.09615 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 284 |  TrainLoss: 0.02444 | TestLoss: 0.11073 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 285 |  TrainLoss: 0.02076 | TestLoss: 0.10247 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 286 |  TrainLoss: 0.01818 | TestLoss: 0.10859 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 287 |  TrainLoss: 0.01803 | TestLoss: 0.11414 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 288 |  TrainLoss: 0.01842 | TestLoss: 0.12308 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 289 |  TrainLoss: 0.01641 | TestLoss: 0.12371 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 290 |  TrainLoss: 0.01893 | TestLoss: 0.10677 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 291 |  TrainLoss: 0.01860 | TestLoss: 0.10772 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 292 |  TrainLoss: 0.01515 | TestLoss: 0.11131 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 293 |  TrainLoss: 0.01507 | TestLoss: 0.12821 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 294 |  TrainLoss: 0.01934 | TestLoss: 0.12979 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 295 |  TrainLoss: 0.01494 | TestLoss: 0.11091 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 296 |  TrainLoss: 0.01390 | TestLoss: 0.10950 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 297 |  TrainLoss: 0.01356 | TestLoss: 0.11710 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 298 |  TrainLoss: 0.01733 | TestLoss: 0.13943 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 299 |  TrainLoss: 0.01422 | TestLoss: 0.14468 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 300 |  TrainLoss: 0.01919 | TestLoss: 0.11613 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 301 |  TrainLoss: 0.01532 | TestLoss: 0.12533 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 302 |  TrainLoss: 0.01237 | TestLoss: 0.13035 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 303 |  TrainLoss: 0.01148 | TestLoss: 0.11348 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 304 |  TrainLoss: 0.01147 | TestLoss: 0.12624 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 305 |  TrainLoss: 0.01021 | TestLoss: 0.12152 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 306 |  TrainLoss: 0.01497 | TestLoss: 0.12228 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 307 |  TrainLoss: 0.01058 | TestLoss: 0.11756 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 308 |  TrainLoss: 0.01369 | TestLoss: 0.12363 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 309 |  TrainLoss: 0.01945 | TestLoss: 0.13780 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 310 |  TrainLoss: 0.03304 | TestLoss: 0.13764 | TestAcc: 0.96785 | TestF1: 0.97\n",
            "Epoch: 311 |  TrainLoss: 0.01806 | TestLoss: 0.11996 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 312 |  TrainLoss: 0.01433 | TestLoss: 0.13551 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 313 |  TrainLoss: 0.01366 | TestLoss: 0.14345 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 314 |  TrainLoss: 0.01750 | TestLoss: 0.13495 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 315 |  TrainLoss: 0.03654 | TestLoss: 0.20449 | TestAcc: 0.95949 | TestF1: 0.96\n",
            "Epoch: 316 |  TrainLoss: 0.02873 | TestLoss: 0.12736 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 317 |  TrainLoss: 0.01889 | TestLoss: 0.11547 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 318 |  TrainLoss: 0.02276 | TestLoss: 0.14133 | TestAcc: 0.96445 | TestF1: 0.96\n",
            "Epoch: 319 |  TrainLoss: 0.02832 | TestLoss: 0.10953 | TestAcc: 0.97987 | TestF1: 0.98\n",
            "Epoch: 320 |  TrainLoss: 0.02392 | TestLoss: 0.12171 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 321 |  TrainLoss: 0.02262 | TestLoss: 0.12260 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 322 |  TrainLoss: 0.01886 | TestLoss: 0.14842 | TestAcc: 0.96498 | TestF1: 0.97\n",
            "Epoch: 323 |  TrainLoss: 0.01581 | TestLoss: 0.11421 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 324 |  TrainLoss: 0.01117 | TestLoss: 0.14218 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 325 |  TrainLoss: 0.01143 | TestLoss: 0.11730 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 326 |  TrainLoss: 0.01147 | TestLoss: 0.12617 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 327 |  TrainLoss: 0.01018 | TestLoss: 0.13083 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 328 |  TrainLoss: 0.01081 | TestLoss: 0.12126 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 329 |  TrainLoss: 0.00952 | TestLoss: 0.13247 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 330 |  TrainLoss: 0.00783 | TestLoss: 0.12871 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 331 |  TrainLoss: 0.00742 | TestLoss: 0.13479 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 332 |  TrainLoss: 0.00755 | TestLoss: 0.13184 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 333 |  TrainLoss: 0.00745 | TestLoss: 0.13496 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 334 |  TrainLoss: 0.00709 | TestLoss: 0.13168 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 335 |  TrainLoss: 0.00805 | TestLoss: 0.16244 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 336 |  TrainLoss: 0.01056 | TestLoss: 0.13153 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 337 |  TrainLoss: 0.00894 | TestLoss: 0.12573 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 338 |  TrainLoss: 0.00660 | TestLoss: 0.14053 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 339 |  TrainLoss: 0.00825 | TestLoss: 0.14997 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 340 |  TrainLoss: 0.01261 | TestLoss: 0.12873 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 341 |  TrainLoss: 0.00908 | TestLoss: 0.14007 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 342 |  TrainLoss: 0.00784 | TestLoss: 0.14046 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 343 |  TrainLoss: 0.01652 | TestLoss: 0.18751 | TestAcc: 0.95870 | TestF1: 0.96\n",
            "Epoch: 344 |  TrainLoss: 0.07421 | TestLoss: 0.16461 | TestAcc: 0.96654 | TestF1: 0.97\n",
            "Epoch: 345 |  TrainLoss: 0.06692 | TestLoss: 0.15157 | TestAcc: 0.95792 | TestF1: 0.96\n",
            "Epoch: 346 |  TrainLoss: 0.08134 | TestLoss: 0.10852 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 347 |  TrainLoss: 0.06485 | TestLoss: 0.09685 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 348 |  TrainLoss: 0.03274 | TestLoss: 0.10553 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 349 |  TrainLoss: 0.02562 | TestLoss: 0.10485 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 350 |  TrainLoss: 0.01559 | TestLoss: 0.11078 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 351 |  TrainLoss: 0.01648 | TestLoss: 0.14066 | TestAcc: 0.96341 | TestF1: 0.96\n",
            "Epoch: 352 |  TrainLoss: 0.03921 | TestLoss: 0.13731 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 353 |  TrainLoss: 0.03365 | TestLoss: 0.10768 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 354 |  TrainLoss: 0.02119 | TestLoss: 0.11869 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 355 |  TrainLoss: 0.02178 | TestLoss: 0.10534 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 356 |  TrainLoss: 0.01653 | TestLoss: 0.12229 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 357 |  TrainLoss: 0.01249 | TestLoss: 0.13846 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 358 |  TrainLoss: 0.01240 | TestLoss: 0.11702 | TestAcc: 0.97883 | TestF1: 0.98\n",
            "Epoch: 359 |  TrainLoss: 0.00980 | TestLoss: 0.13744 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 360 |  TrainLoss: 0.00868 | TestLoss: 0.12475 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 361 |  TrainLoss: 0.00943 | TestLoss: 0.13995 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 362 |  TrainLoss: 0.00668 | TestLoss: 0.12590 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 363 |  TrainLoss: 0.00708 | TestLoss: 0.13145 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 364 |  TrainLoss: 0.00691 | TestLoss: 0.13924 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 365 |  TrainLoss: 0.00708 | TestLoss: 0.13729 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 366 |  TrainLoss: 0.00726 | TestLoss: 0.13444 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 367 |  TrainLoss: 0.00805 | TestLoss: 0.13099 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 368 |  TrainLoss: 0.00869 | TestLoss: 0.16492 | TestAcc: 0.96916 | TestF1: 0.97\n",
            "Epoch: 369 |  TrainLoss: 0.00770 | TestLoss: 0.13150 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 370 |  TrainLoss: 0.00705 | TestLoss: 0.13477 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 371 |  TrainLoss: 0.00700 | TestLoss: 0.15413 | TestAcc: 0.97151 | TestF1: 0.97\n",
            "Epoch: 372 |  TrainLoss: 0.01058 | TestLoss: 0.13389 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 373 |  TrainLoss: 0.00949 | TestLoss: 0.14041 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 374 |  TrainLoss: 0.01042 | TestLoss: 0.15572 | TestAcc: 0.96864 | TestF1: 0.97\n",
            "Epoch: 375 |  TrainLoss: 0.00555 | TestLoss: 0.14174 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 376 |  TrainLoss: 0.00725 | TestLoss: 0.14820 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 377 |  TrainLoss: 0.00537 | TestLoss: 0.13486 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 378 |  TrainLoss: 0.00608 | TestLoss: 0.13674 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 379 |  TrainLoss: 0.00586 | TestLoss: 0.14878 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 380 |  TrainLoss: 0.00494 | TestLoss: 0.13857 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 381 |  TrainLoss: 0.00517 | TestLoss: 0.14089 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 382 |  TrainLoss: 0.00618 | TestLoss: 0.18894 | TestAcc: 0.96472 | TestF1: 0.97\n",
            "Epoch: 383 |  TrainLoss: 0.00751 | TestLoss: 0.15405 | TestAcc: 0.97203 | TestF1: 0.97\n",
            "Epoch: 384 |  TrainLoss: 0.00843 | TestLoss: 0.14269 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 385 |  TrainLoss: 0.00666 | TestLoss: 0.14784 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 386 |  TrainLoss: 0.00508 | TestLoss: 0.14354 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 387 |  TrainLoss: 0.00473 | TestLoss: 0.15294 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 388 |  TrainLoss: 0.00420 | TestLoss: 0.14683 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 389 |  TrainLoss: 0.00364 | TestLoss: 0.15376 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 390 |  TrainLoss: 0.00318 | TestLoss: 0.14884 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 391 |  TrainLoss: 0.00557 | TestLoss: 0.16384 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 392 |  TrainLoss: 0.00498 | TestLoss: 0.14529 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 393 |  TrainLoss: 0.00483 | TestLoss: 0.16082 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 394 |  TrainLoss: 0.00300 | TestLoss: 0.15085 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 395 |  TrainLoss: 0.00315 | TestLoss: 0.15571 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 396 |  TrainLoss: 0.00268 | TestLoss: 0.15234 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 397 |  TrainLoss: 0.00263 | TestLoss: 0.14931 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 398 |  TrainLoss: 0.00284 | TestLoss: 0.15583 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 399 |  TrainLoss: 0.00228 | TestLoss: 0.15693 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 400 |  TrainLoss: 0.00295 | TestLoss: 0.15605 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 401 |  TrainLoss: 0.00149 | TestLoss: 0.16234 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 402 |  TrainLoss: 0.00201 | TestLoss: 0.16201 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 403 |  TrainLoss: 0.00158 | TestLoss: 0.15955 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 404 |  TrainLoss: 0.00371 | TestLoss: 0.17520 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 405 |  TrainLoss: 0.00404 | TestLoss: 0.16155 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 406 |  TrainLoss: 0.00708 | TestLoss: 0.15938 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 407 |  TrainLoss: 0.00208 | TestLoss: 0.16001 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 408 |  TrainLoss: 0.00400 | TestLoss: 0.16596 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 409 |  TrainLoss: 0.00227 | TestLoss: 0.15551 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 410 |  TrainLoss: 0.00202 | TestLoss: 0.15718 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 411 |  TrainLoss: 0.00207 | TestLoss: 0.15774 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 412 |  TrainLoss: 0.00209 | TestLoss: 0.17307 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 413 |  TrainLoss: 0.00164 | TestLoss: 0.15779 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 414 |  TrainLoss: 0.00217 | TestLoss: 0.18180 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 415 |  TrainLoss: 0.00188 | TestLoss: 0.16453 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 416 |  TrainLoss: 0.00255 | TestLoss: 0.18731 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 417 |  TrainLoss: 0.00322 | TestLoss: 0.16041 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 418 |  TrainLoss: 0.00091 | TestLoss: 0.17774 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 419 |  TrainLoss: 0.00093 | TestLoss: 0.16788 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 420 |  TrainLoss: 0.00248 | TestLoss: 0.17064 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 421 |  TrainLoss: 0.00103 | TestLoss: 0.16519 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 422 |  TrainLoss: 0.00069 | TestLoss: 0.17479 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 423 |  TrainLoss: 0.00057 | TestLoss: 0.17480 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 424 |  TrainLoss: 0.00087 | TestLoss: 0.17324 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 425 |  TrainLoss: 0.00102 | TestLoss: 0.17096 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 426 |  TrainLoss: 0.00056 | TestLoss: 0.17720 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 427 |  TrainLoss: 0.00055 | TestLoss: 0.17472 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 428 |  TrainLoss: 0.00080 | TestLoss: 0.18491 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 429 |  TrainLoss: 0.00058 | TestLoss: 0.17632 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 430 |  TrainLoss: 0.00091 | TestLoss: 0.18463 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 431 |  TrainLoss: 0.00083 | TestLoss: 0.17719 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 432 |  TrainLoss: 0.00124 | TestLoss: 0.17931 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 433 |  TrainLoss: 0.00078 | TestLoss: 0.17725 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 434 |  TrainLoss: 0.00051 | TestLoss: 0.17936 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 435 |  TrainLoss: 0.00043 | TestLoss: 0.17992 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 436 |  TrainLoss: 0.00032 | TestLoss: 0.17987 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 437 |  TrainLoss: 0.00039 | TestLoss: 0.18403 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 438 |  TrainLoss: 0.00035 | TestLoss: 0.18300 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 439 |  TrainLoss: 0.00027 | TestLoss: 0.17983 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 440 |  TrainLoss: 0.00050 | TestLoss: 0.18583 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 441 |  TrainLoss: 0.00034 | TestLoss: 0.18211 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 442 |  TrainLoss: 0.00040 | TestLoss: 0.18170 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 443 |  TrainLoss: 0.00030 | TestLoss: 0.18542 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 444 |  TrainLoss: 0.00030 | TestLoss: 0.18239 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 445 |  TrainLoss: 0.00037 | TestLoss: 0.18222 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 446 |  TrainLoss: 0.00035 | TestLoss: 0.23195 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 447 |  TrainLoss: 0.00031 | TestLoss: 0.23075 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 448 |  TrainLoss: 0.00018 | TestLoss: 0.22872 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 449 |  TrainLoss: 0.00050 | TestLoss: 0.23340 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 450 |  TrainLoss: 0.00032 | TestLoss: 0.18104 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 451 |  TrainLoss: 0.00066 | TestLoss: 0.24048 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 452 |  TrainLoss: 0.00074 | TestLoss: 0.18184 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 453 |  TrainLoss: 0.00099 | TestLoss: 0.29012 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 454 |  TrainLoss: 0.00128 | TestLoss: 0.19120 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 455 |  TrainLoss: 0.00205 | TestLoss: 0.23506 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 456 |  TrainLoss: 0.00061 | TestLoss: 0.17801 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 457 |  TrainLoss: 0.00040 | TestLoss: 0.25614 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 458 |  TrainLoss: 0.00063 | TestLoss: 0.18008 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 459 |  TrainLoss: 0.00047 | TestLoss: 0.18296 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 460 |  TrainLoss: 0.00037 | TestLoss: 0.21154 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 461 |  TrainLoss: 0.00031 | TestLoss: 0.20968 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 462 |  TrainLoss: 0.00024 | TestLoss: 0.20957 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 463 |  TrainLoss: 0.00019 | TestLoss: 0.25568 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 464 |  TrainLoss: 0.00019 | TestLoss: 0.25892 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 465 |  TrainLoss: 0.00023 | TestLoss: 0.25711 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 466 |  TrainLoss: 0.00033 | TestLoss: 0.25915 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 467 |  TrainLoss: 0.00019 | TestLoss: 0.25716 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 468 |  TrainLoss: 0.00020 | TestLoss: 0.25648 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 469 |  TrainLoss: 0.00017 | TestLoss: 0.25829 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 470 |  TrainLoss: 0.00019 | TestLoss: 0.28125 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 471 |  TrainLoss: 0.00065 | TestLoss: 0.28241 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 472 |  TrainLoss: 0.00028 | TestLoss: 0.23174 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 473 |  TrainLoss: 0.00028 | TestLoss: 0.27955 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 474 |  TrainLoss: 0.00033 | TestLoss: 0.25837 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 475 |  TrainLoss: 0.00026 | TestLoss: 0.23987 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 476 |  TrainLoss: 0.00078 | TestLoss: 0.34786 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 477 |  TrainLoss: 0.00067 | TestLoss: 0.22921 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 478 |  TrainLoss: 0.00058 | TestLoss: 0.23351 | TestAcc: 0.97360 | TestF1: 0.97\n",
            "Epoch: 479 |  TrainLoss: 0.00026 | TestLoss: 0.23267 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 480 |  TrainLoss: 0.00034 | TestLoss: 0.25847 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 481 |  TrainLoss: 0.00029 | TestLoss: 0.23255 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 482 |  TrainLoss: 0.00020 | TestLoss: 0.25440 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 483 |  TrainLoss: 0.00019 | TestLoss: 0.28066 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 484 |  TrainLoss: 0.00018 | TestLoss: 0.28116 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 485 |  TrainLoss: 0.00023 | TestLoss: 0.28178 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 486 |  TrainLoss: 0.00017 | TestLoss: 0.28126 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 487 |  TrainLoss: 0.00017 | TestLoss: 0.28206 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 488 |  TrainLoss: 0.00015 | TestLoss: 0.28507 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 489 |  TrainLoss: 0.00017 | TestLoss: 0.33060 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 490 |  TrainLoss: 0.00018 | TestLoss: 0.28528 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 491 |  TrainLoss: 0.00016 | TestLoss: 0.30892 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 492 |  TrainLoss: 0.00014 | TestLoss: 0.30889 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 493 |  TrainLoss: 0.00014 | TestLoss: 0.28600 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 494 |  TrainLoss: 0.00012 | TestLoss: 0.28670 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 495 |  TrainLoss: 0.00013 | TestLoss: 0.33307 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 496 |  TrainLoss: 0.00011 | TestLoss: 0.33277 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 497 |  TrainLoss: 0.00012 | TestLoss: 0.33170 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 498 |  TrainLoss: 0.00010 | TestLoss: 0.33177 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 499 |  TrainLoss: 0.00011 | TestLoss: 0.33313 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Best WLoss: 0.00002 | Best Epoch: 103\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  # Initialize with a large value\n",
        "\n",
        "# Without dropout training results\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  # Update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "# Print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123c672d",
      "metadata": {
        "id": "123c672d"
      },
      "source": [
        "# Model 3: \n",
        "GraphSage + Content with 2 Layer MLP as defined by the paper. (This is the model implemented in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ff9b1eff",
      "metadata": {
        "id": "ff9b1eff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.l2_reg_weight = l2_reg_weight\n",
        "        \n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2], hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.softmax = Linear(hidden_channels[4], out_channels)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "adfc56c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adfc56c1",
        "outputId": "4ef9f72a-a318-4e78-b947-49ebe7470ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_gos.num_features,[512,512,512,256,256,256],1, 128, 128, 0.001).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d0839056",
      "metadata": {
        "id": "d0839056"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "920e5727",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "920e5727",
        "outputId": "12a48c37-73a0-42dc-b12b-a9661eea896a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.00096 | TestLoss: 0.15501 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 01 |  TrainLoss: 0.00091 | TestLoss: 0.15731 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 02 |  TrainLoss: 0.00075 | TestLoss: 0.15227 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 03 |  TrainLoss: 0.00092 | TestLoss: 0.16120 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 04 |  TrainLoss: 0.00109 | TestLoss: 0.15399 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 05 |  TrainLoss: 0.00074 | TestLoss: 0.15953 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 06 |  TrainLoss: 0.00072 | TestLoss: 0.15307 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 07 |  TrainLoss: 0.00075 | TestLoss: 0.15550 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 08 |  TrainLoss: 0.00067 | TestLoss: 0.15654 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 09 |  TrainLoss: 0.00069 | TestLoss: 0.16039 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 10 |  TrainLoss: 0.00082 | TestLoss: 0.15843 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 11 |  TrainLoss: 0.00076 | TestLoss: 0.15594 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 12 |  TrainLoss: 0.00076 | TestLoss: 0.15779 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 13 |  TrainLoss: 0.00073 | TestLoss: 0.15888 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 14 |  TrainLoss: 0.00070 | TestLoss: 0.16144 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 15 |  TrainLoss: 0.00059 | TestLoss: 0.15844 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 16 |  TrainLoss: 0.00064 | TestLoss: 0.16702 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 17 |  TrainLoss: 0.00063 | TestLoss: 0.16386 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 18 |  TrainLoss: 0.00078 | TestLoss: 0.16457 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 19 |  TrainLoss: 0.00057 | TestLoss: 0.15709 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 20 |  TrainLoss: 0.00094 | TestLoss: 0.16123 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 21 |  TrainLoss: 0.00094 | TestLoss: 0.16460 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 22 |  TrainLoss: 0.00080 | TestLoss: 0.16263 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 23 |  TrainLoss: 0.00072 | TestLoss: 0.16448 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 24 |  TrainLoss: 0.00048 | TestLoss: 0.16237 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 25 |  TrainLoss: 0.00046 | TestLoss: 0.16465 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 26 |  TrainLoss: 0.00056 | TestLoss: 0.16548 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 27 |  TrainLoss: 0.00087 | TestLoss: 0.15811 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 28 |  TrainLoss: 0.00090 | TestLoss: 0.16514 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 29 |  TrainLoss: 0.00057 | TestLoss: 0.15977 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 30 |  TrainLoss: 0.00071 | TestLoss: 0.17366 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 31 |  TrainLoss: 0.00116 | TestLoss: 0.16081 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 32 |  TrainLoss: 0.00088 | TestLoss: 0.18142 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 33 |  TrainLoss: 0.00088 | TestLoss: 0.16111 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 34 |  TrainLoss: 0.00107 | TestLoss: 0.18023 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 35 |  TrainLoss: 0.00072 | TestLoss: 0.16309 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 36 |  TrainLoss: 0.00096 | TestLoss: 0.17314 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 37 |  TrainLoss: 0.00062 | TestLoss: 0.16420 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 38 |  TrainLoss: 0.00038 | TestLoss: 0.17286 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 39 |  TrainLoss: 0.00037 | TestLoss: 0.17024 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 40 |  TrainLoss: 0.00040 | TestLoss: 0.16855 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 41 |  TrainLoss: 0.00035 | TestLoss: 0.16763 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 42 |  TrainLoss: 0.00028 | TestLoss: 0.16742 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 43 |  TrainLoss: 0.00034 | TestLoss: 0.16951 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 44 |  TrainLoss: 0.00033 | TestLoss: 0.17362 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 45 |  TrainLoss: 0.00034 | TestLoss: 0.17458 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 46 |  TrainLoss: 0.00033 | TestLoss: 0.17044 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 47 |  TrainLoss: 0.00028 | TestLoss: 0.17064 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 48 |  TrainLoss: 0.00031 | TestLoss: 0.17355 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 49 |  TrainLoss: 0.00031 | TestLoss: 0.17416 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 50 |  TrainLoss: 0.00029 | TestLoss: 0.17221 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 51 |  TrainLoss: 0.00029 | TestLoss: 0.17367 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 52 |  TrainLoss: 0.00034 | TestLoss: 0.17550 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 53 |  TrainLoss: 0.00032 | TestLoss: 0.17360 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 54 |  TrainLoss: 0.00032 | TestLoss: 0.17360 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 55 |  TrainLoss: 0.00028 | TestLoss: 0.17386 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 56 |  TrainLoss: 0.00026 | TestLoss: 0.17315 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 57 |  TrainLoss: 0.00026 | TestLoss: 0.17590 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 58 |  TrainLoss: 0.00038 | TestLoss: 0.17815 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 59 |  TrainLoss: 0.00024 | TestLoss: 0.17360 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 60 |  TrainLoss: 0.00038 | TestLoss: 0.18106 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 61 |  TrainLoss: 0.00027 | TestLoss: 0.17335 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 62 |  TrainLoss: 0.00030 | TestLoss: 0.17668 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 63 |  TrainLoss: 0.00028 | TestLoss: 0.20151 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 64 |  TrainLoss: 0.00028 | TestLoss: 0.17788 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 65 |  TrainLoss: 0.00026 | TestLoss: 0.20254 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 66 |  TrainLoss: 0.00021 | TestLoss: 0.17928 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 67 |  TrainLoss: 0.00021 | TestLoss: 0.17795 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 68 |  TrainLoss: 0.00035 | TestLoss: 0.18055 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 69 |  TrainLoss: 0.00032 | TestLoss: 0.18017 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 70 |  TrainLoss: 0.00031 | TestLoss: 0.17340 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 71 |  TrainLoss: 0.00022 | TestLoss: 0.17899 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 72 |  TrainLoss: 0.00022 | TestLoss: 0.17793 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 73 |  TrainLoss: 0.00022 | TestLoss: 0.17797 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 74 |  TrainLoss: 0.00023 | TestLoss: 0.20205 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 75 |  TrainLoss: 0.00019 | TestLoss: 0.20321 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 76 |  TrainLoss: 0.00018 | TestLoss: 0.20175 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 77 |  TrainLoss: 0.00022 | TestLoss: 0.20372 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 78 |  TrainLoss: 0.00029 | TestLoss: 0.17923 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 79 |  TrainLoss: 0.00020 | TestLoss: 0.17696 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 80 |  TrainLoss: 0.00021 | TestLoss: 0.18069 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 81 |  TrainLoss: 0.00027 | TestLoss: 0.25039 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 82 |  TrainLoss: 0.00023 | TestLoss: 0.20229 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 83 |  TrainLoss: 0.00026 | TestLoss: 0.22684 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 84 |  TrainLoss: 0.00025 | TestLoss: 0.25279 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 85 |  TrainLoss: 0.00022 | TestLoss: 0.17846 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 86 |  TrainLoss: 0.00025 | TestLoss: 0.20456 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 87 |  TrainLoss: 0.00025 | TestLoss: 0.22892 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 88 |  TrainLoss: 0.00023 | TestLoss: 0.17893 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 89 |  TrainLoss: 0.00034 | TestLoss: 0.25326 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 90 |  TrainLoss: 0.00025 | TestLoss: 0.17734 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 91 |  TrainLoss: 0.00027 | TestLoss: 0.25029 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 92 |  TrainLoss: 0.00022 | TestLoss: 0.25009 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 93 |  TrainLoss: 0.00020 | TestLoss: 0.22650 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 94 |  TrainLoss: 0.00016 | TestLoss: 0.25021 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 95 |  TrainLoss: 0.00018 | TestLoss: 0.24988 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 96 |  TrainLoss: 0.00020 | TestLoss: 0.25072 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 97 |  TrainLoss: 0.00018 | TestLoss: 0.24876 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 98 |  TrainLoss: 0.00021 | TestLoss: 0.25090 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 99 |  TrainLoss: 0.00016 | TestLoss: 0.27531 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 100 |  TrainLoss: 0.00014 | TestLoss: 0.25270 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 101 |  TrainLoss: 0.00022 | TestLoss: 0.25019 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 102 |  TrainLoss: 0.00016 | TestLoss: 0.25340 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 103 |  TrainLoss: 0.00018 | TestLoss: 0.24988 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 104 |  TrainLoss: 0.00022 | TestLoss: 0.25358 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 105 |  TrainLoss: 0.00019 | TestLoss: 0.25443 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 106 |  TrainLoss: 0.00017 | TestLoss: 0.25015 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 107 |  TrainLoss: 0.00019 | TestLoss: 0.25314 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 108 |  TrainLoss: 0.00019 | TestLoss: 0.25293 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 109 |  TrainLoss: 0.00014 | TestLoss: 0.24902 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 110 |  TrainLoss: 0.00017 | TestLoss: 0.25246 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 111 |  TrainLoss: 0.00016 | TestLoss: 0.25112 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 112 |  TrainLoss: 0.00020 | TestLoss: 0.25218 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 113 |  TrainLoss: 0.00014 | TestLoss: 0.25242 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 114 |  TrainLoss: 0.00014 | TestLoss: 0.25282 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 115 |  TrainLoss: 0.00015 | TestLoss: 0.27567 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 116 |  TrainLoss: 0.00017 | TestLoss: 0.27891 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 117 |  TrainLoss: 0.00015 | TestLoss: 0.27667 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 118 |  TrainLoss: 0.00017 | TestLoss: 0.25170 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 119 |  TrainLoss: 0.00016 | TestLoss: 0.27732 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 120 |  TrainLoss: 0.00013 | TestLoss: 0.27734 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 121 |  TrainLoss: 0.00012 | TestLoss: 0.25282 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 122 |  TrainLoss: 0.00015 | TestLoss: 0.27633 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 123 |  TrainLoss: 0.00015 | TestLoss: 0.25459 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 124 |  TrainLoss: 0.00017 | TestLoss: 0.25537 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 125 |  TrainLoss: 0.00019 | TestLoss: 0.27757 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 126 |  TrainLoss: 0.00021 | TestLoss: 0.27500 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 127 |  TrainLoss: 0.00015 | TestLoss: 0.24966 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 128 |  TrainLoss: 0.00016 | TestLoss: 0.29961 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 129 |  TrainLoss: 0.00016 | TestLoss: 0.27609 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 130 |  TrainLoss: 0.00012 | TestLoss: 0.27643 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 131 |  TrainLoss: 0.00013 | TestLoss: 0.27760 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 132 |  TrainLoss: 0.00014 | TestLoss: 0.30162 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 133 |  TrainLoss: 0.00015 | TestLoss: 0.30025 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 134 |  TrainLoss: 0.00013 | TestLoss: 0.30174 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 135 |  TrainLoss: 0.00012 | TestLoss: 0.27796 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 136 |  TrainLoss: 0.00012 | TestLoss: 0.27676 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 137 |  TrainLoss: 0.00013 | TestLoss: 0.30148 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 138 |  TrainLoss: 0.00011 | TestLoss: 0.30116 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 139 |  TrainLoss: 0.00013 | TestLoss: 0.30217 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 140 |  TrainLoss: 0.00012 | TestLoss: 0.30186 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 141 |  TrainLoss: 0.00012 | TestLoss: 0.30250 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 142 |  TrainLoss: 0.00013 | TestLoss: 0.30254 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 143 |  TrainLoss: 0.00012 | TestLoss: 0.30279 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 144 |  TrainLoss: 0.00013 | TestLoss: 0.30415 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 145 |  TrainLoss: 0.00016 | TestLoss: 0.30116 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 146 |  TrainLoss: 0.00014 | TestLoss: 0.30412 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 147 |  TrainLoss: 0.00010 | TestLoss: 0.30198 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 148 |  TrainLoss: 0.00011 | TestLoss: 0.30212 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 149 |  TrainLoss: 0.00013 | TestLoss: 0.30650 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 150 |  TrainLoss: 0.00012 | TestLoss: 0.27908 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 151 |  TrainLoss: 0.00012 | TestLoss: 0.30172 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 152 |  TrainLoss: 0.00011 | TestLoss: 0.30436 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 153 |  TrainLoss: 0.00011 | TestLoss: 0.30335 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 154 |  TrainLoss: 0.00010 | TestLoss: 0.30310 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 155 |  TrainLoss: 0.00011 | TestLoss: 0.30219 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 156 |  TrainLoss: 0.00010 | TestLoss: 0.30403 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 157 |  TrainLoss: 0.00012 | TestLoss: 0.30403 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 158 |  TrainLoss: 0.00009 | TestLoss: 0.30501 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 159 |  TrainLoss: 0.00011 | TestLoss: 0.30366 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 160 |  TrainLoss: 0.00009 | TestLoss: 0.30352 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 161 |  TrainLoss: 0.00008 | TestLoss: 0.30258 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 162 |  TrainLoss: 0.00009 | TestLoss: 0.30679 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 163 |  TrainLoss: 0.00012 | TestLoss: 0.30599 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 164 |  TrainLoss: 0.00011 | TestLoss: 0.30411 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 165 |  TrainLoss: 0.00011 | TestLoss: 0.30407 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 166 |  TrainLoss: 0.00014 | TestLoss: 0.30710 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 167 |  TrainLoss: 0.00010 | TestLoss: 0.30663 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 168 |  TrainLoss: 0.00010 | TestLoss: 0.30333 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 169 |  TrainLoss: 0.00010 | TestLoss: 0.30514 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 170 |  TrainLoss: 0.00009 | TestLoss: 0.30599 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 171 |  TrainLoss: 0.00011 | TestLoss: 0.30726 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 172 |  TrainLoss: 0.00009 | TestLoss: 0.30505 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 173 |  TrainLoss: 0.00009 | TestLoss: 0.30501 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 174 |  TrainLoss: 0.00010 | TestLoss: 0.30680 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 175 |  TrainLoss: 0.00011 | TestLoss: 0.31282 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 176 |  TrainLoss: 0.00010 | TestLoss: 0.30478 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 177 |  TrainLoss: 0.00011 | TestLoss: 0.30850 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 178 |  TrainLoss: 0.00009 | TestLoss: 0.30818 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 179 |  TrainLoss: 0.00009 | TestLoss: 0.30583 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 180 |  TrainLoss: 0.00010 | TestLoss: 0.30908 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 181 |  TrainLoss: 0.00008 | TestLoss: 0.30779 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 182 |  TrainLoss: 0.00009 | TestLoss: 0.30898 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 183 |  TrainLoss: 0.00009 | TestLoss: 0.30787 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 184 |  TrainLoss: 0.00007 | TestLoss: 0.30748 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 185 |  TrainLoss: 0.00007 | TestLoss: 0.30736 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 186 |  TrainLoss: 0.00009 | TestLoss: 0.30818 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 187 |  TrainLoss: 0.00008 | TestLoss: 0.30932 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 188 |  TrainLoss: 0.00009 | TestLoss: 0.30747 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 189 |  TrainLoss: 0.00008 | TestLoss: 0.30933 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 190 |  TrainLoss: 0.00012 | TestLoss: 0.31202 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 191 |  TrainLoss: 0.00010 | TestLoss: 0.30763 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 192 |  TrainLoss: 0.00009 | TestLoss: 0.30661 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 193 |  TrainLoss: 0.00008 | TestLoss: 0.30848 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 194 |  TrainLoss: 0.00008 | TestLoss: 0.31059 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 195 |  TrainLoss: 0.00007 | TestLoss: 0.30783 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 196 |  TrainLoss: 0.00007 | TestLoss: 0.30837 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 197 |  TrainLoss: 0.00010 | TestLoss: 0.31424 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 198 |  TrainLoss: 0.00008 | TestLoss: 0.30817 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 199 |  TrainLoss: 0.00008 | TestLoss: 0.30962 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 200 |  TrainLoss: 0.00008 | TestLoss: 0.31246 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 201 |  TrainLoss: 0.00007 | TestLoss: 0.30837 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 202 |  TrainLoss: 0.00008 | TestLoss: 0.31081 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 203 |  TrainLoss: 0.00007 | TestLoss: 0.31114 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 204 |  TrainLoss: 0.00008 | TestLoss: 0.30905 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 205 |  TrainLoss: 0.00007 | TestLoss: 0.31239 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 206 |  TrainLoss: 0.00006 | TestLoss: 0.31167 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 207 |  TrainLoss: 0.00010 | TestLoss: 0.31339 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 208 |  TrainLoss: 0.00008 | TestLoss: 0.31311 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 209 |  TrainLoss: 0.00006 | TestLoss: 0.31083 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 210 |  TrainLoss: 0.00007 | TestLoss: 0.31036 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 211 |  TrainLoss: 0.00006 | TestLoss: 0.31454 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 212 |  TrainLoss: 0.00007 | TestLoss: 0.31266 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 213 |  TrainLoss: 0.00006 | TestLoss: 0.30964 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 214 |  TrainLoss: 0.00007 | TestLoss: 0.31321 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 215 |  TrainLoss: 0.00008 | TestLoss: 0.31543 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 216 |  TrainLoss: 0.00007 | TestLoss: 0.31506 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 217 |  TrainLoss: 0.00006 | TestLoss: 0.31161 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 218 |  TrainLoss: 0.00007 | TestLoss: 0.31220 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 219 |  TrainLoss: 0.00006 | TestLoss: 0.31283 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 220 |  TrainLoss: 0.00006 | TestLoss: 0.31196 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 221 |  TrainLoss: 0.00007 | TestLoss: 0.33918 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 222 |  TrainLoss: 0.00006 | TestLoss: 0.31141 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 223 |  TrainLoss: 0.00008 | TestLoss: 0.31396 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 224 |  TrainLoss: 0.00006 | TestLoss: 0.31505 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 225 |  TrainLoss: 0.00004 | TestLoss: 0.31237 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 226 |  TrainLoss: 0.00006 | TestLoss: 0.31241 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 227 |  TrainLoss: 0.00006 | TestLoss: 0.31324 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 228 |  TrainLoss: 0.00006 | TestLoss: 0.33768 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 229 |  TrainLoss: 0.00005 | TestLoss: 0.31425 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 230 |  TrainLoss: 0.00007 | TestLoss: 0.31479 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 231 |  TrainLoss: 0.00005 | TestLoss: 0.31489 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 232 |  TrainLoss: 0.00005 | TestLoss: 0.31376 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 233 |  TrainLoss: 0.00005 | TestLoss: 0.31598 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 234 |  TrainLoss: 0.00005 | TestLoss: 0.31444 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 235 |  TrainLoss: 0.00006 | TestLoss: 0.31377 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 236 |  TrainLoss: 0.00006 | TestLoss: 0.31618 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 237 |  TrainLoss: 0.00005 | TestLoss: 0.31440 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 238 |  TrainLoss: 0.00005 | TestLoss: 0.31389 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 239 |  TrainLoss: 0.00008 | TestLoss: 0.31504 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 240 |  TrainLoss: 0.00006 | TestLoss: 0.31584 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 241 |  TrainLoss: 0.00006 | TestLoss: 0.31624 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 242 |  TrainLoss: 0.00005 | TestLoss: 0.31614 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 243 |  TrainLoss: 0.00005 | TestLoss: 0.31672 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 244 |  TrainLoss: 0.00006 | TestLoss: 0.34114 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 245 |  TrainLoss: 0.00006 | TestLoss: 0.31543 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 246 |  TrainLoss: 0.00006 | TestLoss: 0.31569 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 247 |  TrainLoss: 0.00005 | TestLoss: 0.31679 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 248 |  TrainLoss: 0.00008 | TestLoss: 0.33882 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 249 |  TrainLoss: 0.00007 | TestLoss: 0.33913 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 250 |  TrainLoss: 0.00007 | TestLoss: 0.31327 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 251 |  TrainLoss: 0.00006 | TestLoss: 0.31552 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 252 |  TrainLoss: 0.00005 | TestLoss: 0.31700 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 253 |  TrainLoss: 0.00005 | TestLoss: 0.31586 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 254 |  TrainLoss: 0.00006 | TestLoss: 0.33989 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 255 |  TrainLoss: 0.00006 | TestLoss: 0.33956 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 256 |  TrainLoss: 0.00005 | TestLoss: 0.33869 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 257 |  TrainLoss: 0.00010 | TestLoss: 0.31407 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 258 |  TrainLoss: 0.00006 | TestLoss: 0.31750 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 259 |  TrainLoss: 0.00006 | TestLoss: 0.31285 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 260 |  TrainLoss: 0.00013 | TestLoss: 0.34426 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 261 |  TrainLoss: 0.00010 | TestLoss: 0.31293 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 262 |  TrainLoss: 0.00005 | TestLoss: 0.34096 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 263 |  TrainLoss: 0.00006 | TestLoss: 0.31571 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 264 |  TrainLoss: 0.00004 | TestLoss: 0.31639 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 265 |  TrainLoss: 0.00005 | TestLoss: 0.31715 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 266 |  TrainLoss: 0.00005 | TestLoss: 0.34161 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 267 |  TrainLoss: 0.00005 | TestLoss: 0.34000 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 268 |  TrainLoss: 0.00007 | TestLoss: 0.34389 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 269 |  TrainLoss: 0.00006 | TestLoss: 0.34183 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 270 |  TrainLoss: 0.00005 | TestLoss: 0.33924 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 271 |  TrainLoss: 0.00005 | TestLoss: 0.34064 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 272 |  TrainLoss: 0.00004 | TestLoss: 0.34079 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 273 |  TrainLoss: 0.00004 | TestLoss: 0.34128 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 274 |  TrainLoss: 0.00005 | TestLoss: 0.34165 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 275 |  TrainLoss: 0.00006 | TestLoss: 0.31656 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 276 |  TrainLoss: 0.00008 | TestLoss: 0.34173 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 277 |  TrainLoss: 0.00007 | TestLoss: 0.33751 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 278 |  TrainLoss: 0.00005 | TestLoss: 0.31585 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 279 |  TrainLoss: 0.00005 | TestLoss: 0.34209 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 280 |  TrainLoss: 0.00006 | TestLoss: 0.34295 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 281 |  TrainLoss: 0.00004 | TestLoss: 0.34157 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 282 |  TrainLoss: 0.00005 | TestLoss: 0.34026 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 283 |  TrainLoss: 0.00004 | TestLoss: 0.34076 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 284 |  TrainLoss: 0.00004 | TestLoss: 0.34094 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 285 |  TrainLoss: 0.00004 | TestLoss: 0.34215 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 286 |  TrainLoss: 0.00004 | TestLoss: 0.31734 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 287 |  TrainLoss: 0.00005 | TestLoss: 0.34510 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 288 |  TrainLoss: 0.00007 | TestLoss: 0.34096 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 289 |  TrainLoss: 0.00005 | TestLoss: 0.34075 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 290 |  TrainLoss: 0.00004 | TestLoss: 0.34385 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 291 |  TrainLoss: 0.00004 | TestLoss: 0.34410 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 292 |  TrainLoss: 0.00005 | TestLoss: 0.34292 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 293 |  TrainLoss: 0.00003 | TestLoss: 0.34111 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 294 |  TrainLoss: 0.00004 | TestLoss: 0.34447 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 295 |  TrainLoss: 0.00004 | TestLoss: 0.39112 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 296 |  TrainLoss: 0.00004 | TestLoss: 0.36807 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 297 |  TrainLoss: 0.00004 | TestLoss: 0.34313 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 298 |  TrainLoss: 0.00003 | TestLoss: 0.34304 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 299 |  TrainLoss: 0.00004 | TestLoss: 0.36805 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 300 |  TrainLoss: 0.00004 | TestLoss: 0.34299 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 301 |  TrainLoss: 0.00004 | TestLoss: 0.34294 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 302 |  TrainLoss: 0.00003 | TestLoss: 0.34510 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 303 |  TrainLoss: 0.00004 | TestLoss: 0.34437 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 304 |  TrainLoss: 0.00003 | TestLoss: 0.34368 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 305 |  TrainLoss: 0.00004 | TestLoss: 0.34433 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 306 |  TrainLoss: 0.00003 | TestLoss: 0.36792 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 307 |  TrainLoss: 0.00004 | TestLoss: 0.34502 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 308 |  TrainLoss: 0.00003 | TestLoss: 0.34440 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 309 |  TrainLoss: 0.00004 | TestLoss: 0.34502 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 310 |  TrainLoss: 0.00004 | TestLoss: 0.34508 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 311 |  TrainLoss: 0.00003 | TestLoss: 0.36801 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 312 |  TrainLoss: 0.00003 | TestLoss: 0.34383 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 313 |  TrainLoss: 0.00004 | TestLoss: 0.34255 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 314 |  TrainLoss: 0.00005 | TestLoss: 0.36789 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 315 |  TrainLoss: 0.00003 | TestLoss: 0.39114 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 316 |  TrainLoss: 0.00004 | TestLoss: 0.34318 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 317 |  TrainLoss: 0.00004 | TestLoss: 0.36697 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 318 |  TrainLoss: 0.00003 | TestLoss: 0.39158 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 319 |  TrainLoss: 0.00004 | TestLoss: 0.36800 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 320 |  TrainLoss: 0.00004 | TestLoss: 0.36730 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 321 |  TrainLoss: 0.00005 | TestLoss: 0.39170 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 322 |  TrainLoss: 0.00004 | TestLoss: 0.39145 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 323 |  TrainLoss: 0.00004 | TestLoss: 0.39125 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 324 |  TrainLoss: 0.00004 | TestLoss: 0.36866 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 325 |  TrainLoss: 0.00004 | TestLoss: 0.34431 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 326 |  TrainLoss: 0.00004 | TestLoss: 0.36921 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 327 |  TrainLoss: 0.00003 | TestLoss: 0.39167 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 328 |  TrainLoss: 0.00003 | TestLoss: 0.36733 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 329 |  TrainLoss: 0.00004 | TestLoss: 0.39217 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 330 |  TrainLoss: 0.00003 | TestLoss: 0.36911 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 331 |  TrainLoss: 0.00004 | TestLoss: 0.36866 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 332 |  TrainLoss: 0.00004 | TestLoss: 0.39355 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 333 |  TrainLoss: 0.00003 | TestLoss: 0.39246 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 334 |  TrainLoss: 0.00003 | TestLoss: 0.39273 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 335 |  TrainLoss: 0.00004 | TestLoss: 0.39386 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 336 |  TrainLoss: 0.00004 | TestLoss: 0.39230 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 337 |  TrainLoss: 0.00003 | TestLoss: 0.39153 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 338 |  TrainLoss: 0.00003 | TestLoss: 0.36926 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 339 |  TrainLoss: 0.00004 | TestLoss: 0.39182 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 340 |  TrainLoss: 0.00004 | TestLoss: 0.39208 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 341 |  TrainLoss: 0.00002 | TestLoss: 0.36937 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 342 |  TrainLoss: 0.00004 | TestLoss: 0.39381 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 343 |  TrainLoss: 0.00003 | TestLoss: 0.39307 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 344 |  TrainLoss: 0.00003 | TestLoss: 0.39137 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 345 |  TrainLoss: 0.00004 | TestLoss: 0.39534 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 346 |  TrainLoss: 0.00003 | TestLoss: 0.39358 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 347 |  TrainLoss: 0.00003 | TestLoss: 0.36967 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 348 |  TrainLoss: 0.00005 | TestLoss: 0.39530 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 349 |  TrainLoss: 0.00004 | TestLoss: 0.39396 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 350 |  TrainLoss: 0.00004 | TestLoss: 0.36833 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 351 |  TrainLoss: 0.00004 | TestLoss: 0.39246 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 352 |  TrainLoss: 0.00003 | TestLoss: 0.39200 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 353 |  TrainLoss: 0.00003 | TestLoss: 0.39416 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 354 |  TrainLoss: 0.00004 | TestLoss: 0.39534 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 355 |  TrainLoss: 0.00003 | TestLoss: 0.39389 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 356 |  TrainLoss: 0.00003 | TestLoss: 0.39293 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 357 |  TrainLoss: 0.00003 | TestLoss: 0.39592 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 358 |  TrainLoss: 0.00003 | TestLoss: 0.39318 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 359 |  TrainLoss: 0.00003 | TestLoss: 0.39502 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 360 |  TrainLoss: 0.00004 | TestLoss: 0.39693 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 361 |  TrainLoss: 0.00004 | TestLoss: 0.39594 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 362 |  TrainLoss: 0.00003 | TestLoss: 0.41909 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 363 |  TrainLoss: 0.00003 | TestLoss: 0.39426 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 364 |  TrainLoss: 0.00003 | TestLoss: 0.39493 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 365 |  TrainLoss: 0.00002 | TestLoss: 0.39614 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 366 |  TrainLoss: 0.00004 | TestLoss: 0.39617 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 367 |  TrainLoss: 0.00003 | TestLoss: 0.39482 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 368 |  TrainLoss: 0.00002 | TestLoss: 0.39642 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 369 |  TrainLoss: 0.00003 | TestLoss: 0.39509 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 370 |  TrainLoss: 0.00002 | TestLoss: 0.39539 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 371 |  TrainLoss: 0.00002 | TestLoss: 0.39552 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 372 |  TrainLoss: 0.00002 | TestLoss: 0.39686 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 373 |  TrainLoss: 0.00003 | TestLoss: 0.39752 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 374 |  TrainLoss: 0.00003 | TestLoss: 0.39469 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 375 |  TrainLoss: 0.00003 | TestLoss: 0.39564 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 376 |  TrainLoss: 0.00003 | TestLoss: 0.39770 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 377 |  TrainLoss: 0.00002 | TestLoss: 0.39520 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 378 |  TrainLoss: 0.00002 | TestLoss: 0.39661 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 379 |  TrainLoss: 0.00003 | TestLoss: 0.42151 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 380 |  TrainLoss: 0.00003 | TestLoss: 0.39617 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 381 |  TrainLoss: 0.00003 | TestLoss: 0.39554 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 382 |  TrainLoss: 0.00003 | TestLoss: 0.39623 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 383 |  TrainLoss: 0.00002 | TestLoss: 0.37221 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 384 |  TrainLoss: 0.00003 | TestLoss: 0.39662 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 385 |  TrainLoss: 0.00003 | TestLoss: 0.39775 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 386 |  TrainLoss: 0.00002 | TestLoss: 0.39558 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 387 |  TrainLoss: 0.00004 | TestLoss: 0.42141 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 388 |  TrainLoss: 0.00003 | TestLoss: 0.39672 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 389 |  TrainLoss: 0.00003 | TestLoss: 0.39503 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 390 |  TrainLoss: 0.00003 | TestLoss: 0.39575 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 391 |  TrainLoss: 0.00002 | TestLoss: 0.39713 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 392 |  TrainLoss: 0.00003 | TestLoss: 0.39827 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 393 |  TrainLoss: 0.00003 | TestLoss: 0.39831 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 394 |  TrainLoss: 0.00002 | TestLoss: 0.39689 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 395 |  TrainLoss: 0.00002 | TestLoss: 0.39730 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 396 |  TrainLoss: 0.00003 | TestLoss: 0.39753 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 397 |  TrainLoss: 0.00003 | TestLoss: 0.42010 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 398 |  TrainLoss: 0.00002 | TestLoss: 0.42144 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 399 |  TrainLoss: 0.00003 | TestLoss: 0.42050 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 400 |  TrainLoss: 0.00002 | TestLoss: 0.39778 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 401 |  TrainLoss: 0.00002 | TestLoss: 0.39687 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 402 |  TrainLoss: 0.00002 | TestLoss: 0.39795 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 403 |  TrainLoss: 0.00002 | TestLoss: 0.39701 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 404 |  TrainLoss: 0.00002 | TestLoss: 0.39720 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 405 |  TrainLoss: 0.00002 | TestLoss: 0.39800 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 406 |  TrainLoss: 0.00002 | TestLoss: 0.42236 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 407 |  TrainLoss: 0.00003 | TestLoss: 0.42080 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 408 |  TrainLoss: 0.00002 | TestLoss: 0.39828 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 409 |  TrainLoss: 0.00002 | TestLoss: 0.39838 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 410 |  TrainLoss: 0.00002 | TestLoss: 0.39868 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 411 |  TrainLoss: 0.00002 | TestLoss: 0.42280 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 412 |  TrainLoss: 0.00002 | TestLoss: 0.42169 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 413 |  TrainLoss: 0.00002 | TestLoss: 0.42167 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 414 |  TrainLoss: 0.00002 | TestLoss: 0.42224 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 415 |  TrainLoss: 0.00002 | TestLoss: 0.42164 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 416 |  TrainLoss: 0.00002 | TestLoss: 0.42304 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 417 |  TrainLoss: 0.00002 | TestLoss: 0.39973 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 418 |  TrainLoss: 0.00002 | TestLoss: 0.39937 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 419 |  TrainLoss: 0.00003 | TestLoss: 0.44537 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 420 |  TrainLoss: 0.00002 | TestLoss: 0.42143 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 421 |  TrainLoss: 0.00002 | TestLoss: 0.42183 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 422 |  TrainLoss: 0.00002 | TestLoss: 0.39884 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 0.42315 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 424 |  TrainLoss: 0.00002 | TestLoss: 0.42333 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 425 |  TrainLoss: 0.00002 | TestLoss: 0.42277 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 426 |  TrainLoss: 0.00003 | TestLoss: 0.44529 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 427 |  TrainLoss: 0.00002 | TestLoss: 0.42059 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 428 |  TrainLoss: 0.00003 | TestLoss: 0.44423 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 429 |  TrainLoss: 0.00002 | TestLoss: 0.46940 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 430 |  TrainLoss: 0.00002 | TestLoss: 0.39830 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 431 |  TrainLoss: 0.00002 | TestLoss: 0.39909 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 432 |  TrainLoss: 0.00002 | TestLoss: 0.42423 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 433 |  TrainLoss: 0.00002 | TestLoss: 0.42434 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 434 |  TrainLoss: 0.00002 | TestLoss: 0.42197 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 435 |  TrainLoss: 0.00002 | TestLoss: 0.46876 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 436 |  TrainLoss: 0.00002 | TestLoss: 0.46894 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 437 |  TrainLoss: 0.00002 | TestLoss: 0.42310 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 438 |  TrainLoss: 0.00002 | TestLoss: 0.42320 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 439 |  TrainLoss: 0.00002 | TestLoss: 0.44746 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 440 |  TrainLoss: 0.00002 | TestLoss: 0.42368 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 441 |  TrainLoss: 0.00002 | TestLoss: 0.42306 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 442 |  TrainLoss: 0.00002 | TestLoss: 0.46914 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 443 |  TrainLoss: 0.00002 | TestLoss: 0.44620 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 444 |  TrainLoss: 0.00002 | TestLoss: 0.46892 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 445 |  TrainLoss: 0.00002 | TestLoss: 0.46915 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 446 |  TrainLoss: 0.00001 | TestLoss: 0.44634 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 447 |  TrainLoss: 0.00002 | TestLoss: 0.44655 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 0.49256 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 449 |  TrainLoss: 0.00003 | TestLoss: 0.42278 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 450 |  TrainLoss: 0.00003 | TestLoss: 0.44765 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 451 |  TrainLoss: 0.00003 | TestLoss: 0.49386 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 452 |  TrainLoss: 0.00002 | TestLoss: 0.42191 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 453 |  TrainLoss: 0.00002 | TestLoss: 0.44734 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 454 |  TrainLoss: 0.00002 | TestLoss: 0.47048 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 455 |  TrainLoss: 0.00001 | TestLoss: 0.42462 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 456 |  TrainLoss: 0.00002 | TestLoss: 0.42478 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 457 |  TrainLoss: 0.00001 | TestLoss: 0.44790 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 458 |  TrainLoss: 0.00002 | TestLoss: 0.44771 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 459 |  TrainLoss: 0.00002 | TestLoss: 0.42583 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 460 |  TrainLoss: 0.00003 | TestLoss: 0.49435 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 461 |  TrainLoss: 0.00002 | TestLoss: 0.44779 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 462 |  TrainLoss: 0.00002 | TestLoss: 0.40025 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 463 |  TrainLoss: 0.00002 | TestLoss: 0.44681 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 464 |  TrainLoss: 0.00002 | TestLoss: 0.42532 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 465 |  TrainLoss: 0.00002 | TestLoss: 0.42585 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 466 |  TrainLoss: 0.00001 | TestLoss: 0.42609 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 467 |  TrainLoss: 0.00001 | TestLoss: 0.47054 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 468 |  TrainLoss: 0.00002 | TestLoss: 0.44818 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 469 |  TrainLoss: 0.00001 | TestLoss: 0.44549 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 470 |  TrainLoss: 0.00002 | TestLoss: 0.44592 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 471 |  TrainLoss: 0.00002 | TestLoss: 0.44642 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 472 |  TrainLoss: 0.00001 | TestLoss: 0.42513 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 473 |  TrainLoss: 0.00002 | TestLoss: 0.42640 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 474 |  TrainLoss: 0.00001 | TestLoss: 0.47426 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 475 |  TrainLoss: 0.00001 | TestLoss: 0.49726 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 476 |  TrainLoss: 0.00001 | TestLoss: 0.49349 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 477 |  TrainLoss: 0.00002 | TestLoss: 0.49363 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 478 |  TrainLoss: 0.00001 | TestLoss: 0.49514 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 479 |  TrainLoss: 0.00001 | TestLoss: 0.42738 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 480 |  TrainLoss: 0.00002 | TestLoss: 0.44954 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 481 |  TrainLoss: 0.00002 | TestLoss: 0.49619 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 482 |  TrainLoss: 0.00003 | TestLoss: 0.49570 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 483 |  TrainLoss: 0.00001 | TestLoss: 0.44737 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 484 |  TrainLoss: 0.00002 | TestLoss: 0.51810 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 485 |  TrainLoss: 0.00001 | TestLoss: 0.51796 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 486 |  TrainLoss: 0.00001 | TestLoss: 0.42676 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 487 |  TrainLoss: 0.00002 | TestLoss: 0.51828 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 488 |  TrainLoss: 0.00002 | TestLoss: 0.51713 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 489 |  TrainLoss: 0.00002 | TestLoss: 0.44728 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 490 |  TrainLoss: 0.00001 | TestLoss: 0.44864 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 491 |  TrainLoss: 0.00002 | TestLoss: 0.49537 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 492 |  TrainLoss: 0.00001 | TestLoss: 0.49597 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 493 |  TrainLoss: 0.00001 | TestLoss: 0.49645 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 494 |  TrainLoss: 0.00001 | TestLoss: 0.49392 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 495 |  TrainLoss: 0.00001 | TestLoss: 0.44965 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 496 |  TrainLoss: 0.00001 | TestLoss: 0.49628 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 497 |  TrainLoss: 0.00001 | TestLoss: 0.51906 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 498 |  TrainLoss: 0.00001 | TestLoss: 0.49545 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 499 |  TrainLoss: 0.00001 | TestLoss: 0.49547 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Best WLoss: 0.00004 | Best Epoch: 2\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  # Initialize with a large value\n",
        "\n",
        "# Without dropout training results\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  # Update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "# Print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b2230f",
      "metadata": {
        "id": "19b2230f"
      },
      "source": [
        "# Code 4: \n",
        "GraphSage + Content with hyperparameters as defined in the paper and replacemenent of 1 MLP layer with 1 RNN Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "320093fd",
      "metadata": {
        "id": "320093fd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, embedding_size, batch_size, l2_reg_weight):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.l2_reg_weight = l2_reg_weight\n",
        "        \n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_size, hidden_channels[3], batch_first=True)\n",
        "        self.full1 = nn.Linear(hidden_channels[3], hidden_channels[4])\n",
        "        self.softmax = nn.Linear(hidden_channels[4], out_channels)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dp1 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h, batch)\n",
        "        \n",
        "        # Reshape the input tensor for RNN\n",
        "        h = h.unsqueeze(0)  # Add a time dimension\n",
        "        h = self.dp1(h)\n",
        "        \n",
        "        # Apply RNN\n",
        "        h, _ = self.rnn(h)\n",
        "        h = h.squeeze(0)  # Remove the time dimension\n",
        "        \n",
        "        h = self.full1(h).relu()\n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2ad01364",
      "metadata": {
        "id": "2ad01364"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "dc94d9b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc94d9b0",
        "outputId": "02435b94-fa1d-4cca-c593-85dad7754b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.00002 | TestLoss: 0.49768 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 01 |  TrainLoss: 0.00001 | TestLoss: 0.49612 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 02 |  TrainLoss: 0.00001 | TestLoss: 0.49493 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 03 |  TrainLoss: 0.00001 | TestLoss: 0.49738 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 04 |  TrainLoss: 0.00001 | TestLoss: 0.52107 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 05 |  TrainLoss: 0.00001 | TestLoss: 0.49754 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 06 |  TrainLoss: 0.00001 | TestLoss: 0.49657 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 07 |  TrainLoss: 0.00001 | TestLoss: 0.49817 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 08 |  TrainLoss: 0.00002 | TestLoss: 0.52212 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 09 |  TrainLoss: 0.00002 | TestLoss: 0.49617 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 10 |  TrainLoss: 0.00001 | TestLoss: 0.49514 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 11 |  TrainLoss: 0.00001 | TestLoss: 0.49751 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 12 |  TrainLoss: 0.00001 | TestLoss: 0.49786 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 13 |  TrainLoss: 0.00001 | TestLoss: 0.49752 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 14 |  TrainLoss: 0.00001 | TestLoss: 0.49722 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 15 |  TrainLoss: 0.00001 | TestLoss: 0.49707 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 16 |  TrainLoss: 0.00001 | TestLoss: 0.49732 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 17 |  TrainLoss: 0.00001 | TestLoss: 0.51967 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 18 |  TrainLoss: 0.00001 | TestLoss: 0.52039 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 19 |  TrainLoss: 0.00001 | TestLoss: 0.52068 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 20 |  TrainLoss: 0.00001 | TestLoss: 0.49745 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 21 |  TrainLoss: 0.00002 | TestLoss: 0.52349 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 22 |  TrainLoss: 0.00001 | TestLoss: 0.52120 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 23 |  TrainLoss: 0.00002 | TestLoss: 0.52108 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 24 |  TrainLoss: 0.00003 | TestLoss: 0.45019 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 25 |  TrainLoss: 0.00003 | TestLoss: 0.47376 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 26 |  TrainLoss: 0.00002 | TestLoss: 0.50046 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 27 |  TrainLoss: 0.00001 | TestLoss: 0.45454 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 28 |  TrainLoss: 0.00001 | TestLoss: 0.49936 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 29 |  TrainLoss: 0.00001 | TestLoss: 0.49959 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 30 |  TrainLoss: 0.00001 | TestLoss: 0.49885 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 31 |  TrainLoss: 0.00001 | TestLoss: 0.50014 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 32 |  TrainLoss: 0.00001 | TestLoss: 0.50191 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 33 |  TrainLoss: 0.00001 | TestLoss: 0.50036 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 34 |  TrainLoss: 0.00001 | TestLoss: 0.50066 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 35 |  TrainLoss: 0.00001 | TestLoss: 0.52385 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 36 |  TrainLoss: 0.00002 | TestLoss: 0.52335 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 37 |  TrainLoss: 0.00001 | TestLoss: 0.49871 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 38 |  TrainLoss: 0.00001 | TestLoss: 0.49927 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 39 |  TrainLoss: 0.00001 | TestLoss: 0.52376 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 40 |  TrainLoss: 0.00001 | TestLoss: 0.52279 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 41 |  TrainLoss: 0.00001 | TestLoss: 0.49849 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 42 |  TrainLoss: 0.00001 | TestLoss: 0.52415 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 43 |  TrainLoss: 0.00001 | TestLoss: 0.52440 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 44 |  TrainLoss: 0.00001 | TestLoss: 0.52188 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 45 |  TrainLoss: 0.00002 | TestLoss: 0.49938 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 46 |  TrainLoss: 0.00001 | TestLoss: 0.52640 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 47 |  TrainLoss: 0.00001 | TestLoss: 0.52679 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 48 |  TrainLoss: 0.00001 | TestLoss: 0.50039 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 49 |  TrainLoss: 0.00001 | TestLoss: 0.52402 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 50 |  TrainLoss: 0.00001 | TestLoss: 0.52263 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 51 |  TrainLoss: 0.00001 | TestLoss: 0.49903 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 52 |  TrainLoss: 0.00001 | TestLoss: 0.49941 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 53 |  TrainLoss: 0.00001 | TestLoss: 0.50070 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 54 |  TrainLoss: 0.00001 | TestLoss: 0.50086 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 55 |  TrainLoss: 0.00001 | TestLoss: 0.52464 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 56 |  TrainLoss: 0.00001 | TestLoss: 0.52406 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 57 |  TrainLoss: 0.00001 | TestLoss: 0.52372 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 58 |  TrainLoss: 0.00001 | TestLoss: 0.52442 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 59 |  TrainLoss: 0.00001 | TestLoss: 0.52526 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 60 |  TrainLoss: 0.00001 | TestLoss: 0.52430 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 61 |  TrainLoss: 0.00001 | TestLoss: 0.52318 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 62 |  TrainLoss: 0.00001 | TestLoss: 0.52474 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 63 |  TrainLoss: 0.00001 | TestLoss: 0.52717 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 64 |  TrainLoss: 0.00001 | TestLoss: 0.52662 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 65 |  TrainLoss: 0.00001 | TestLoss: 0.52513 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 66 |  TrainLoss: 0.00001 | TestLoss: 0.52421 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 67 |  TrainLoss: 0.00001 | TestLoss: 0.52582 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 68 |  TrainLoss: 0.00001 | TestLoss: 0.52537 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 69 |  TrainLoss: 0.00001 | TestLoss: 0.52470 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 70 |  TrainLoss: 0.00001 | TestLoss: 0.52556 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 71 |  TrainLoss: 0.00001 | TestLoss: 0.55305 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 72 |  TrainLoss: 0.00002 | TestLoss: 0.50100 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 73 |  TrainLoss: 0.00002 | TestLoss: 0.52781 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 74 |  TrainLoss: 0.00002 | TestLoss: 0.53081 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 75 |  TrainLoss: 0.00001 | TestLoss: 0.50058 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 76 |  TrainLoss: 0.00001 | TestLoss: 0.55205 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 77 |  TrainLoss: 0.00001 | TestLoss: 0.52567 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 78 |  TrainLoss: 0.00001 | TestLoss: 0.52408 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 79 |  TrainLoss: 0.00001 | TestLoss: 0.52753 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 80 |  TrainLoss: 0.00001 | TestLoss: 0.55306 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 81 |  TrainLoss: 0.00001 | TestLoss: 0.52634 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 82 |  TrainLoss: 0.00001 | TestLoss: 0.52572 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 83 |  TrainLoss: 0.00001 | TestLoss: 0.52621 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 84 |  TrainLoss: 0.00001 | TestLoss: 0.55114 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 85 |  TrainLoss: 0.00001 | TestLoss: 0.55162 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 86 |  TrainLoss: 0.00001 | TestLoss: 0.54893 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 87 |  TrainLoss: 0.00001 | TestLoss: 0.52453 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 88 |  TrainLoss: 0.00001 | TestLoss: 0.55131 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 89 |  TrainLoss: 0.00001 | TestLoss: 0.55170 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 90 |  TrainLoss: 0.00001 | TestLoss: 0.52600 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 91 |  TrainLoss: 0.00001 | TestLoss: 0.52627 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 92 |  TrainLoss: 0.00001 | TestLoss: 0.55020 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 93 |  TrainLoss: 0.00001 | TestLoss: 0.52707 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 94 |  TrainLoss: 0.00001 | TestLoss: 0.55057 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 95 |  TrainLoss: 0.00001 | TestLoss: 0.52778 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 96 |  TrainLoss: 0.00001 | TestLoss: 0.52726 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 97 |  TrainLoss: 0.00001 | TestLoss: 0.55118 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 98 |  TrainLoss: 0.00001 | TestLoss: 0.52825 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 99 |  TrainLoss: 0.00001 | TestLoss: 0.52685 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 100 |  TrainLoss: 0.00001 | TestLoss: 0.52846 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 101 |  TrainLoss: 0.00001 | TestLoss: 0.55381 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 102 |  TrainLoss: 0.00001 | TestLoss: 0.55103 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 103 |  TrainLoss: 0.00001 | TestLoss: 0.52629 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 104 |  TrainLoss: 0.00001 | TestLoss: 0.55305 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 105 |  TrainLoss: 0.00001 | TestLoss: 0.55141 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 106 |  TrainLoss: 0.00000 | TestLoss: 0.52748 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 107 |  TrainLoss: 0.00001 | TestLoss: 0.55225 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 108 |  TrainLoss: 0.00001 | TestLoss: 0.55371 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 109 |  TrainLoss: 0.00001 | TestLoss: 0.55181 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 110 |  TrainLoss: 0.00001 | TestLoss: 0.55256 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 111 |  TrainLoss: 0.00001 | TestLoss: 0.55318 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 112 |  TrainLoss: 0.00001 | TestLoss: 0.55011 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 113 |  TrainLoss: 0.00001 | TestLoss: 0.55165 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 114 |  TrainLoss: 0.00001 | TestLoss: 0.54997 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 115 |  TrainLoss: 0.00000 | TestLoss: 0.54982 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 116 |  TrainLoss: 0.00001 | TestLoss: 0.55079 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 117 |  TrainLoss: 0.00001 | TestLoss: 0.55141 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 118 |  TrainLoss: 0.00001 | TestLoss: 0.55030 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 119 |  TrainLoss: 0.00001 | TestLoss: 0.55121 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 120 |  TrainLoss: 0.00001 | TestLoss: 0.55248 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 121 |  TrainLoss: 0.00001 | TestLoss: 0.55312 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 122 |  TrainLoss: 0.00001 | TestLoss: 0.55312 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 123 |  TrainLoss: 0.00000 | TestLoss: 0.55261 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 124 |  TrainLoss: 0.00001 | TestLoss: 0.55203 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 125 |  TrainLoss: 0.00001 | TestLoss: 0.55255 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 126 |  TrainLoss: 0.00001 | TestLoss: 0.55342 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 127 |  TrainLoss: 0.00001 | TestLoss: 0.55396 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 128 |  TrainLoss: 0.00001 | TestLoss: 0.55433 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 129 |  TrainLoss: 0.00001 | TestLoss: 0.55260 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 130 |  TrainLoss: 0.00001 | TestLoss: 0.55245 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 131 |  TrainLoss: 0.00001 | TestLoss: 0.55387 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 132 |  TrainLoss: 0.00001 | TestLoss: 0.55520 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 133 |  TrainLoss: 0.00001 | TestLoss: 0.55457 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 134 |  TrainLoss: 0.00000 | TestLoss: 0.55384 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 135 |  TrainLoss: 0.00001 | TestLoss: 0.55459 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 136 |  TrainLoss: 0.00001 | TestLoss: 0.55411 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 137 |  TrainLoss: 0.00001 | TestLoss: 0.55449 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 138 |  TrainLoss: 0.00000 | TestLoss: 0.55451 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 139 |  TrainLoss: 0.00001 | TestLoss: 0.55447 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 140 |  TrainLoss: 0.00001 | TestLoss: 0.55532 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 141 |  TrainLoss: 0.00001 | TestLoss: 0.55591 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 142 |  TrainLoss: 0.00001 | TestLoss: 0.55423 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 143 |  TrainLoss: 0.00000 | TestLoss: 0.55748 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 144 |  TrainLoss: 0.00001 | TestLoss: 0.55621 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 145 |  TrainLoss: 0.00001 | TestLoss: 0.55562 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 146 |  TrainLoss: 0.00001 | TestLoss: 0.55560 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 147 |  TrainLoss: 0.00001 | TestLoss: 0.55401 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 148 |  TrainLoss: 0.00000 | TestLoss: 0.55253 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 149 |  TrainLoss: 0.00001 | TestLoss: 0.55366 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 150 |  TrainLoss: 0.00001 | TestLoss: 0.55435 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 151 |  TrainLoss: 0.00000 | TestLoss: 0.55255 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 152 |  TrainLoss: 0.00001 | TestLoss: 0.55477 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 153 |  TrainLoss: 0.00000 | TestLoss: 0.55788 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 154 |  TrainLoss: 0.00001 | TestLoss: 0.55697 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 155 |  TrainLoss: 0.00000 | TestLoss: 0.55522 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 156 |  TrainLoss: 0.00000 | TestLoss: 0.55548 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 157 |  TrainLoss: 0.00001 | TestLoss: 0.55736 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 158 |  TrainLoss: 0.00001 | TestLoss: 0.55703 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 159 |  TrainLoss: 0.00001 | TestLoss: 0.55660 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 160 |  TrainLoss: 0.00000 | TestLoss: 0.55573 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 161 |  TrainLoss: 0.00001 | TestLoss: 0.55690 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 162 |  TrainLoss: 0.00000 | TestLoss: 0.55874 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 163 |  TrainLoss: 0.00001 | TestLoss: 0.55751 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 164 |  TrainLoss: 0.00000 | TestLoss: 0.55635 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 165 |  TrainLoss: 0.00001 | TestLoss: 0.55975 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 166 |  TrainLoss: 0.00000 | TestLoss: 0.55608 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 167 |  TrainLoss: 0.00000 | TestLoss: 0.55610 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 168 |  TrainLoss: 0.00001 | TestLoss: 0.55757 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 169 |  TrainLoss: 0.00000 | TestLoss: 0.55741 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 170 |  TrainLoss: 0.00001 | TestLoss: 0.55683 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 171 |  TrainLoss: 0.00000 | TestLoss: 0.55792 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 172 |  TrainLoss: 0.00001 | TestLoss: 0.56059 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 173 |  TrainLoss: 0.00001 | TestLoss: 0.55902 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 174 |  TrainLoss: 0.00001 | TestLoss: 0.55482 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 175 |  TrainLoss: 0.00000 | TestLoss: 0.55626 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 176 |  TrainLoss: 0.00000 | TestLoss: 0.55839 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 177 |  TrainLoss: 0.00000 | TestLoss: 0.55883 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 178 |  TrainLoss: 0.00000 | TestLoss: 0.55865 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 179 |  TrainLoss: 0.00000 | TestLoss: 0.55916 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 180 |  TrainLoss: 0.00000 | TestLoss: 0.55697 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 181 |  TrainLoss: 0.00000 | TestLoss: 0.55580 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 182 |  TrainLoss: 0.00000 | TestLoss: 0.55720 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 183 |  TrainLoss: 0.00000 | TestLoss: 0.55930 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 184 |  TrainLoss: 0.00001 | TestLoss: 0.56173 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 185 |  TrainLoss: 0.00000 | TestLoss: 0.55812 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 186 |  TrainLoss: 0.00000 | TestLoss: 0.55795 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 187 |  TrainLoss: 0.00001 | TestLoss: 0.56093 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 188 |  TrainLoss: 0.00000 | TestLoss: 0.56163 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 189 |  TrainLoss: 0.00000 | TestLoss: 0.55857 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 190 |  TrainLoss: 0.00001 | TestLoss: 0.56112 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 191 |  TrainLoss: 0.00001 | TestLoss: 0.56043 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 192 |  TrainLoss: 0.00000 | TestLoss: 0.55751 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 193 |  TrainLoss: 0.00000 | TestLoss: 0.55940 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 194 |  TrainLoss: 0.00000 | TestLoss: 0.55945 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 195 |  TrainLoss: 0.00000 | TestLoss: 0.55885 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 196 |  TrainLoss: 0.00001 | TestLoss: 0.56142 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 197 |  TrainLoss: 0.00000 | TestLoss: 0.56012 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 198 |  TrainLoss: 0.00000 | TestLoss: 0.55867 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 199 |  TrainLoss: 0.00000 | TestLoss: 0.55971 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 200 |  TrainLoss: 0.00000 | TestLoss: 0.56101 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 201 |  TrainLoss: 0.00000 | TestLoss: 0.55954 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 202 |  TrainLoss: 0.00000 | TestLoss: 0.55830 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 203 |  TrainLoss: 0.00000 | TestLoss: 0.56080 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 204 |  TrainLoss: 0.00000 | TestLoss: 0.58499 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 205 |  TrainLoss: 0.00001 | TestLoss: 0.56035 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 206 |  TrainLoss: 0.00000 | TestLoss: 0.56220 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 207 |  TrainLoss: 0.00001 | TestLoss: 0.56145 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 208 |  TrainLoss: 0.00000 | TestLoss: 0.55966 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 209 |  TrainLoss: 0.00000 | TestLoss: 0.56021 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 210 |  TrainLoss: 0.00000 | TestLoss: 0.56090 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 211 |  TrainLoss: 0.00000 | TestLoss: 0.55975 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 212 |  TrainLoss: 0.00000 | TestLoss: 0.58479 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 213 |  TrainLoss: 0.00000 | TestLoss: 0.58572 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 214 |  TrainLoss: 0.00001 | TestLoss: 0.56126 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 215 |  TrainLoss: 0.00001 | TestLoss: 0.56205 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 216 |  TrainLoss: 0.00001 | TestLoss: 0.61233 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 217 |  TrainLoss: 0.00001 | TestLoss: 0.58650 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 218 |  TrainLoss: 0.00000 | TestLoss: 0.55949 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 219 |  TrainLoss: 0.00001 | TestLoss: 0.56258 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 220 |  TrainLoss: 0.00001 | TestLoss: 0.61117 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 221 |  TrainLoss: 0.00001 | TestLoss: 0.56050 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 222 |  TrainLoss: 0.00000 | TestLoss: 0.55999 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 223 |  TrainLoss: 0.00001 | TestLoss: 0.58627 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 224 |  TrainLoss: 0.00000 | TestLoss: 0.56338 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 225 |  TrainLoss: 0.00000 | TestLoss: 0.56044 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 226 |  TrainLoss: 0.00000 | TestLoss: 0.56112 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 227 |  TrainLoss: 0.00000 | TestLoss: 0.58620 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 228 |  TrainLoss: 0.00000 | TestLoss: 0.56281 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 229 |  TrainLoss: 0.00000 | TestLoss: 0.56132 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 230 |  TrainLoss: 0.00000 | TestLoss: 0.56215 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 231 |  TrainLoss: 0.00000 | TestLoss: 0.58595 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 232 |  TrainLoss: 0.00000 | TestLoss: 0.58587 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 233 |  TrainLoss: 0.00000 | TestLoss: 0.56322 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 234 |  TrainLoss: 0.00000 | TestLoss: 0.56230 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 235 |  TrainLoss: 0.00001 | TestLoss: 0.61003 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 236 |  TrainLoss: 0.00000 | TestLoss: 0.58487 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 237 |  TrainLoss: 0.00000 | TestLoss: 0.56111 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 238 |  TrainLoss: 0.00000 | TestLoss: 0.56195 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 239 |  TrainLoss: 0.00001 | TestLoss: 0.61054 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 240 |  TrainLoss: 0.00000 | TestLoss: 0.58545 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 241 |  TrainLoss: 0.00000 | TestLoss: 0.56087 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 242 |  TrainLoss: 0.00000 | TestLoss: 0.56269 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 243 |  TrainLoss: 0.00000 | TestLoss: 0.56362 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 244 |  TrainLoss: 0.00000 | TestLoss: 0.56321 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 245 |  TrainLoss: 0.00000 | TestLoss: 0.56389 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 246 |  TrainLoss: 0.00000 | TestLoss: 0.58587 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 247 |  TrainLoss: 0.00000 | TestLoss: 0.56327 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 248 |  TrainLoss: 0.00000 | TestLoss: 0.56295 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 249 |  TrainLoss: 0.00001 | TestLoss: 0.58776 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 250 |  TrainLoss: 0.00000 | TestLoss: 0.58925 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 251 |  TrainLoss: 0.00000 | TestLoss: 0.58684 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 252 |  TrainLoss: 0.00000 | TestLoss: 0.56385 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 253 |  TrainLoss: 0.00003 | TestLoss: 0.66123 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 254 |  TrainLoss: 0.00001 | TestLoss: 0.56364 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 255 |  TrainLoss: 0.00001 | TestLoss: 0.67198 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 256 |  TrainLoss: 0.00002 | TestLoss: 0.56028 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 257 |  TrainLoss: 0.00001 | TestLoss: 0.58572 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 258 |  TrainLoss: 0.00001 | TestLoss: 0.61130 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 259 |  TrainLoss: 0.00000 | TestLoss: 0.56341 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 260 |  TrainLoss: 0.00000 | TestLoss: 0.56455 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 261 |  TrainLoss: 0.00001 | TestLoss: 0.58721 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 262 |  TrainLoss: 0.00000 | TestLoss: 0.58525 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 263 |  TrainLoss: 0.00000 | TestLoss: 0.56078 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 264 |  TrainLoss: 0.00000 | TestLoss: 0.58499 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 265 |  TrainLoss: 0.00001 | TestLoss: 0.61355 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 266 |  TrainLoss: 0.00001 | TestLoss: 0.56334 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 267 |  TrainLoss: 0.00000 | TestLoss: 0.56341 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 268 |  TrainLoss: 0.00000 | TestLoss: 0.58736 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 269 |  TrainLoss: 0.00000 | TestLoss: 0.58703 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 270 |  TrainLoss: 0.00000 | TestLoss: 0.58684 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 271 |  TrainLoss: 0.00000 | TestLoss: 0.58737 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 272 |  TrainLoss: 0.00000 | TestLoss: 0.58671 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 273 |  TrainLoss: 0.00000 | TestLoss: 0.58615 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 274 |  TrainLoss: 0.00000 | TestLoss: 0.58699 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 275 |  TrainLoss: 0.00000 | TestLoss: 0.61068 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 276 |  TrainLoss: 0.00000 | TestLoss: 0.61296 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 277 |  TrainLoss: 0.00000 | TestLoss: 0.61112 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 278 |  TrainLoss: 0.00000 | TestLoss: 0.58764 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 279 |  TrainLoss: 0.00000 | TestLoss: 0.61182 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 280 |  TrainLoss: 0.00000 | TestLoss: 0.61189 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 281 |  TrainLoss: 0.00000 | TestLoss: 0.60981 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 282 |  TrainLoss: 0.00000 | TestLoss: 0.58764 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 283 |  TrainLoss: 0.00000 | TestLoss: 0.61217 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 284 |  TrainLoss: 0.00000 | TestLoss: 0.58743 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 285 |  TrainLoss: 0.00000 | TestLoss: 0.56349 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 286 |  TrainLoss: 0.00000 | TestLoss: 0.61078 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 287 |  TrainLoss: 0.00000 | TestLoss: 0.61123 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 288 |  TrainLoss: 0.00000 | TestLoss: 0.60984 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 289 |  TrainLoss: 0.00000 | TestLoss: 0.61143 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 290 |  TrainLoss: 0.00000 | TestLoss: 0.61336 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 291 |  TrainLoss: 0.00000 | TestLoss: 0.61182 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 292 |  TrainLoss: 0.00000 | TestLoss: 0.61233 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 293 |  TrainLoss: 0.00000 | TestLoss: 0.61212 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 294 |  TrainLoss: 0.00000 | TestLoss: 0.61149 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 295 |  TrainLoss: 0.00000 | TestLoss: 0.58918 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 296 |  TrainLoss: 0.00000 | TestLoss: 0.61173 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 297 |  TrainLoss: 0.00000 | TestLoss: 0.63843 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 298 |  TrainLoss: 0.00000 | TestLoss: 0.61300 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 299 |  TrainLoss: 0.00000 | TestLoss: 0.56627 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 300 |  TrainLoss: 0.00000 | TestLoss: 0.58936 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 301 |  TrainLoss: 0.00000 | TestLoss: 0.61375 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 302 |  TrainLoss: 0.00000 | TestLoss: 0.61384 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 303 |  TrainLoss: 0.00000 | TestLoss: 0.56783 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 304 |  TrainLoss: 0.00000 | TestLoss: 0.59065 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 305 |  TrainLoss: 0.00000 | TestLoss: 0.61463 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 306 |  TrainLoss: 0.00000 | TestLoss: 0.59147 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 307 |  TrainLoss: 0.00000 | TestLoss: 0.59087 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 308 |  TrainLoss: 0.00000 | TestLoss: 0.59119 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 309 |  TrainLoss: 0.00000 | TestLoss: 0.59085 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 310 |  TrainLoss: 0.00000 | TestLoss: 0.59156 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 311 |  TrainLoss: 0.00000 | TestLoss: 0.61466 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 312 |  TrainLoss: 0.00000 | TestLoss: 0.61504 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 313 |  TrainLoss: 0.00000 | TestLoss: 0.59233 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 314 |  TrainLoss: 0.00000 | TestLoss: 0.59192 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 315 |  TrainLoss: 0.00000 | TestLoss: 0.61512 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 316 |  TrainLoss: 0.00000 | TestLoss: 0.61622 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 317 |  TrainLoss: 0.00000 | TestLoss: 0.59213 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 318 |  TrainLoss: 0.00000 | TestLoss: 0.59166 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 319 |  TrainLoss: 0.00001 | TestLoss: 0.59065 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 320 |  TrainLoss: 0.00003 | TestLoss: 0.75030 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 321 |  TrainLoss: 0.00030 | TestLoss: 3.03289 | TestAcc: 0.79744 | TestF1: 0.75\n",
            "Epoch: 322 |  TrainLoss: 5.62716 | TestLoss: 0.25070 | TestAcc: 0.94773 | TestF1: 0.95\n",
            "Epoch: 323 |  TrainLoss: 0.26286 | TestLoss: 0.27538 | TestAcc: 0.93884 | TestF1: 0.94\n",
            "Epoch: 324 |  TrainLoss: 0.21878 | TestLoss: 0.15543 | TestAcc: 0.93832 | TestF1: 0.94\n",
            "Epoch: 325 |  TrainLoss: 0.11860 | TestLoss: 0.10245 | TestAcc: 0.97099 | TestF1: 0.97\n",
            "Epoch: 326 |  TrainLoss: 0.09452 | TestLoss: 0.10053 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 327 |  TrainLoss: 0.07647 | TestLoss: 0.12083 | TestAcc: 0.96759 | TestF1: 0.97\n",
            "Epoch: 328 |  TrainLoss: 0.07533 | TestLoss: 0.09400 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 329 |  TrainLoss: 0.06727 | TestLoss: 0.08743 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 330 |  TrainLoss: 0.06729 | TestLoss: 0.09130 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 331 |  TrainLoss: 0.06975 | TestLoss: 0.12099 | TestAcc: 0.96759 | TestF1: 0.97\n",
            "Epoch: 332 |  TrainLoss: 0.07611 | TestLoss: 0.08345 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 333 |  TrainLoss: 0.05624 | TestLoss: 0.08022 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 334 |  TrainLoss: 0.04932 | TestLoss: 0.08075 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 335 |  TrainLoss: 0.04640 | TestLoss: 0.08628 | TestAcc: 0.97857 | TestF1: 0.98\n",
            "Epoch: 336 |  TrainLoss: 0.04375 | TestLoss: 0.09789 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 337 |  TrainLoss: 0.04866 | TestLoss: 0.08734 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 338 |  TrainLoss: 0.04075 | TestLoss: 0.09364 | TestAcc: 0.97308 | TestF1: 0.97\n",
            "Epoch: 339 |  TrainLoss: 0.04171 | TestLoss: 0.09310 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 340 |  TrainLoss: 0.03705 | TestLoss: 0.08814 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 341 |  TrainLoss: 0.04005 | TestLoss: 0.09437 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 342 |  TrainLoss: 0.06714 | TestLoss: 0.09173 | TestAcc: 0.97831 | TestF1: 0.98\n",
            "Epoch: 343 |  TrainLoss: 0.05378 | TestLoss: 0.09144 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 344 |  TrainLoss: 0.05916 | TestLoss: 0.08442 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 345 |  TrainLoss: 0.04974 | TestLoss: 0.12669 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 346 |  TrainLoss: 0.06190 | TestLoss: 0.08843 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 347 |  TrainLoss: 0.03944 | TestLoss: 0.09542 | TestAcc: 0.96811 | TestF1: 0.97\n",
            "Epoch: 348 |  TrainLoss: 0.04026 | TestLoss: 0.09377 | TestAcc: 0.97778 | TestF1: 0.98\n",
            "Epoch: 349 |  TrainLoss: 0.04371 | TestLoss: 0.10963 | TestAcc: 0.96837 | TestF1: 0.97\n",
            "Epoch: 350 |  TrainLoss: 0.04143 | TestLoss: 0.08778 | TestAcc: 0.97804 | TestF1: 0.98\n",
            "Epoch: 351 |  TrainLoss: 0.04206 | TestLoss: 0.12401 | TestAcc: 0.97047 | TestF1: 0.97\n",
            "Epoch: 352 |  TrainLoss: 0.05997 | TestLoss: 0.08882 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 353 |  TrainLoss: 0.03978 | TestLoss: 0.09131 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 354 |  TrainLoss: 0.03408 | TestLoss: 0.10147 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 355 |  TrainLoss: 0.03331 | TestLoss: 0.10638 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 356 |  TrainLoss: 0.03600 | TestLoss: 0.08961 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 357 |  TrainLoss: 0.03025 | TestLoss: 0.08933 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 358 |  TrainLoss: 0.03019 | TestLoss: 0.09311 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 359 |  TrainLoss: 0.02432 | TestLoss: 0.09614 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 360 |  TrainLoss: 0.02260 | TestLoss: 0.10215 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 361 |  TrainLoss: 0.02663 | TestLoss: 0.09349 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 362 |  TrainLoss: 0.02344 | TestLoss: 0.09718 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 363 |  TrainLoss: 0.02263 | TestLoss: 0.09716 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 364 |  TrainLoss: 0.01894 | TestLoss: 0.10112 | TestAcc: 0.97282 | TestF1: 0.97\n",
            "Epoch: 365 |  TrainLoss: 0.01945 | TestLoss: 0.12826 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 366 |  TrainLoss: 0.02488 | TestLoss: 0.10244 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 367 |  TrainLoss: 0.02373 | TestLoss: 0.10393 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 368 |  TrainLoss: 0.02664 | TestLoss: 0.10232 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 369 |  TrainLoss: 0.03009 | TestLoss: 0.11135 | TestAcc: 0.97177 | TestF1: 0.97\n",
            "Epoch: 370 |  TrainLoss: 0.02955 | TestLoss: 0.09952 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 371 |  TrainLoss: 0.02429 | TestLoss: 0.09574 | TestAcc: 0.97491 | TestF1: 0.97\n",
            "Epoch: 372 |  TrainLoss: 0.02533 | TestLoss: 0.10409 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 373 |  TrainLoss: 0.01712 | TestLoss: 0.10638 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 374 |  TrainLoss: 0.01958 | TestLoss: 0.09953 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 375 |  TrainLoss: 0.01214 | TestLoss: 0.10854 | TestAcc: 0.96942 | TestF1: 0.97\n",
            "Epoch: 376 |  TrainLoss: 0.01507 | TestLoss: 0.11482 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 377 |  TrainLoss: 0.02392 | TestLoss: 0.12717 | TestAcc: 0.96759 | TestF1: 0.97\n",
            "Epoch: 378 |  TrainLoss: 0.01721 | TestLoss: 0.11773 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 379 |  TrainLoss: 0.01883 | TestLoss: 0.10326 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 380 |  TrainLoss: 0.01402 | TestLoss: 0.10365 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 381 |  TrainLoss: 0.01204 | TestLoss: 0.10418 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 382 |  TrainLoss: 0.01839 | TestLoss: 0.10861 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 383 |  TrainLoss: 0.01925 | TestLoss: 0.11611 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 384 |  TrainLoss: 0.01422 | TestLoss: 0.10441 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 385 |  TrainLoss: 0.02062 | TestLoss: 0.10444 | TestAcc: 0.97752 | TestF1: 0.98\n",
            "Epoch: 386 |  TrainLoss: 0.01439 | TestLoss: 0.10269 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 387 |  TrainLoss: 0.02450 | TestLoss: 0.11353 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 388 |  TrainLoss: 0.03812 | TestLoss: 0.13082 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 389 |  TrainLoss: 0.02690 | TestLoss: 0.10877 | TestAcc: 0.97439 | TestF1: 0.97\n",
            "Epoch: 390 |  TrainLoss: 0.01630 | TestLoss: 0.11684 | TestAcc: 0.97256 | TestF1: 0.97\n",
            "Epoch: 391 |  TrainLoss: 0.01673 | TestLoss: 0.10650 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 392 |  TrainLoss: 0.01145 | TestLoss: 0.11504 | TestAcc: 0.97020 | TestF1: 0.97\n",
            "Epoch: 393 |  TrainLoss: 0.01174 | TestLoss: 0.12591 | TestAcc: 0.97229 | TestF1: 0.97\n",
            "Epoch: 394 |  TrainLoss: 0.01558 | TestLoss: 0.10968 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 395 |  TrainLoss: 0.01155 | TestLoss: 0.10487 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 396 |  TrainLoss: 0.01364 | TestLoss: 0.11818 | TestAcc: 0.97073 | TestF1: 0.97\n",
            "Epoch: 397 |  TrainLoss: 0.02777 | TestLoss: 0.11570 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 398 |  TrainLoss: 0.02133 | TestLoss: 0.13005 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 399 |  TrainLoss: 0.02026 | TestLoss: 0.16358 | TestAcc: 0.96053 | TestF1: 0.96\n",
            "Epoch: 400 |  TrainLoss: 0.02118 | TestLoss: 0.10699 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 401 |  TrainLoss: 0.00904 | TestLoss: 0.10576 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 402 |  TrainLoss: 0.00788 | TestLoss: 0.10685 | TestAcc: 0.97386 | TestF1: 0.97\n",
            "Epoch: 403 |  TrainLoss: 0.00870 | TestLoss: 0.11656 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 404 |  TrainLoss: 0.00704 | TestLoss: 0.10655 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 405 |  TrainLoss: 0.00722 | TestLoss: 0.11059 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 406 |  TrainLoss: 0.00800 | TestLoss: 0.10746 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 407 |  TrainLoss: 0.00775 | TestLoss: 0.11312 | TestAcc: 0.97334 | TestF1: 0.97\n",
            "Epoch: 408 |  TrainLoss: 0.01336 | TestLoss: 0.10878 | TestAcc: 0.97700 | TestF1: 0.98\n",
            "Epoch: 409 |  TrainLoss: 0.01200 | TestLoss: 0.10998 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 410 |  TrainLoss: 0.00822 | TestLoss: 0.12065 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 411 |  TrainLoss: 0.00764 | TestLoss: 0.13253 | TestAcc: 0.96968 | TestF1: 0.97\n",
            "Epoch: 412 |  TrainLoss: 0.00807 | TestLoss: 0.10906 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 413 |  TrainLoss: 0.00600 | TestLoss: 0.11054 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 414 |  TrainLoss: 0.00565 | TestLoss: 0.11577 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 415 |  TrainLoss: 0.00512 | TestLoss: 0.11466 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 416 |  TrainLoss: 0.00539 | TestLoss: 0.11595 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 417 |  TrainLoss: 0.00396 | TestLoss: 0.11373 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 418 |  TrainLoss: 0.00523 | TestLoss: 0.11415 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 419 |  TrainLoss: 0.00654 | TestLoss: 0.13556 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 420 |  TrainLoss: 0.00701 | TestLoss: 0.12442 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 421 |  TrainLoss: 0.00565 | TestLoss: 0.11542 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 422 |  TrainLoss: 0.00459 | TestLoss: 0.11434 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 423 |  TrainLoss: 0.00424 | TestLoss: 0.11727 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 424 |  TrainLoss: 0.00376 | TestLoss: 0.12088 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 425 |  TrainLoss: 0.00337 | TestLoss: 0.13304 | TestAcc: 0.97125 | TestF1: 0.97\n",
            "Epoch: 426 |  TrainLoss: 0.00791 | TestLoss: 0.12459 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 427 |  TrainLoss: 0.00601 | TestLoss: 0.12092 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 428 |  TrainLoss: 0.00399 | TestLoss: 0.11794 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 429 |  TrainLoss: 0.00361 | TestLoss: 0.13296 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 430 |  TrainLoss: 0.00479 | TestLoss: 0.12107 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 431 |  TrainLoss: 0.00325 | TestLoss: 0.12840 | TestAcc: 0.97465 | TestF1: 0.97\n",
            "Epoch: 432 |  TrainLoss: 0.00324 | TestLoss: 0.12329 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 433 |  TrainLoss: 0.00293 | TestLoss: 0.12935 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 434 |  TrainLoss: 0.00337 | TestLoss: 0.13028 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 435 |  TrainLoss: 0.00503 | TestLoss: 0.12221 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 436 |  TrainLoss: 0.00652 | TestLoss: 0.12097 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 437 |  TrainLoss: 0.00257 | TestLoss: 0.12959 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 438 |  TrainLoss: 0.00310 | TestLoss: 0.12259 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 439 |  TrainLoss: 0.00267 | TestLoss: 0.12490 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 440 |  TrainLoss: 0.00227 | TestLoss: 0.13395 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 441 |  TrainLoss: 0.00262 | TestLoss: 0.12621 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 442 |  TrainLoss: 0.00227 | TestLoss: 0.13209 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 443 |  TrainLoss: 0.00237 | TestLoss: 0.12957 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 444 |  TrainLoss: 0.00213 | TestLoss: 0.13179 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 445 |  TrainLoss: 0.00224 | TestLoss: 0.12785 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 446 |  TrainLoss: 0.00216 | TestLoss: 0.12793 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 447 |  TrainLoss: 0.00219 | TestLoss: 0.12775 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 448 |  TrainLoss: 0.00289 | TestLoss: 0.13003 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 449 |  TrainLoss: 0.00190 | TestLoss: 0.12885 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 450 |  TrainLoss: 0.00205 | TestLoss: 0.13360 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 451 |  TrainLoss: 0.00203 | TestLoss: 0.13582 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 452 |  TrainLoss: 0.00199 | TestLoss: 0.12968 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 453 |  TrainLoss: 0.00207 | TestLoss: 0.13031 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 454 |  TrainLoss: 0.00280 | TestLoss: 0.13122 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 455 |  TrainLoss: 0.00200 | TestLoss: 0.14069 | TestAcc: 0.97412 | TestF1: 0.97\n",
            "Epoch: 456 |  TrainLoss: 0.00217 | TestLoss: 0.13820 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 457 |  TrainLoss: 0.00214 | TestLoss: 0.13173 | TestAcc: 0.97726 | TestF1: 0.98\n",
            "Epoch: 458 |  TrainLoss: 0.00266 | TestLoss: 0.13040 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 459 |  TrainLoss: 0.00280 | TestLoss: 0.16367 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 460 |  TrainLoss: 0.00658 | TestLoss: 0.15759 | TestAcc: 0.96994 | TestF1: 0.97\n",
            "Epoch: 461 |  TrainLoss: 0.00509 | TestLoss: 0.15405 | TestAcc: 0.96890 | TestF1: 0.97\n",
            "Epoch: 462 |  TrainLoss: 0.00537 | TestLoss: 0.13405 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 463 |  TrainLoss: 0.00196 | TestLoss: 0.12821 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 464 |  TrainLoss: 0.00195 | TestLoss: 0.13773 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 465 |  TrainLoss: 0.00185 | TestLoss: 0.13176 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 466 |  TrainLoss: 0.00171 | TestLoss: 0.13588 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 467 |  TrainLoss: 0.00157 | TestLoss: 0.13467 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 468 |  TrainLoss: 0.00146 | TestLoss: 0.14072 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 469 |  TrainLoss: 0.00140 | TestLoss: 0.13308 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 470 |  TrainLoss: 0.00140 | TestLoss: 0.14080 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 471 |  TrainLoss: 0.00188 | TestLoss: 0.14231 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 472 |  TrainLoss: 0.00246 | TestLoss: 0.13444 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 473 |  TrainLoss: 0.00154 | TestLoss: 0.13783 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 474 |  TrainLoss: 0.00168 | TestLoss: 0.14124 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 475 |  TrainLoss: 0.00149 | TestLoss: 0.13546 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 476 |  TrainLoss: 0.00121 | TestLoss: 0.13963 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 477 |  TrainLoss: 0.00136 | TestLoss: 0.13739 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 478 |  TrainLoss: 0.00196 | TestLoss: 0.14396 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 479 |  TrainLoss: 0.00146 | TestLoss: 0.14057 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 480 |  TrainLoss: 0.00148 | TestLoss: 0.14035 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 481 |  TrainLoss: 0.00127 | TestLoss: 0.14249 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 482 |  TrainLoss: 0.00123 | TestLoss: 0.14184 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 483 |  TrainLoss: 0.00127 | TestLoss: 0.14624 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 484 |  TrainLoss: 0.00121 | TestLoss: 0.14007 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 485 |  TrainLoss: 0.00106 | TestLoss: 0.14584 | TestAcc: 0.97543 | TestF1: 0.98\n",
            "Epoch: 486 |  TrainLoss: 0.00112 | TestLoss: 0.14271 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 487 |  TrainLoss: 0.00097 | TestLoss: 0.14293 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 488 |  TrainLoss: 0.00105 | TestLoss: 0.14372 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 489 |  TrainLoss: 0.00086 | TestLoss: 0.14454 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 490 |  TrainLoss: 0.00100 | TestLoss: 0.14854 | TestAcc: 0.97491 | TestF1: 0.98\n",
            "Epoch: 491 |  TrainLoss: 0.00103 | TestLoss: 0.14213 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Epoch: 492 |  TrainLoss: 0.00112 | TestLoss: 0.14700 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 493 |  TrainLoss: 0.00109 | TestLoss: 0.14870 | TestAcc: 0.97517 | TestF1: 0.98\n",
            "Epoch: 494 |  TrainLoss: 0.00117 | TestLoss: 0.14198 | TestAcc: 0.97648 | TestF1: 0.98\n",
            "Epoch: 495 |  TrainLoss: 0.00107 | TestLoss: 0.14679 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 496 |  TrainLoss: 0.00091 | TestLoss: 0.14995 | TestAcc: 0.97595 | TestF1: 0.98\n",
            "Epoch: 497 |  TrainLoss: 0.00089 | TestLoss: 0.14628 | TestAcc: 0.97622 | TestF1: 0.98\n",
            "Epoch: 498 |  TrainLoss: 0.00093 | TestLoss: 0.14900 | TestAcc: 0.97569 | TestF1: 0.98\n",
            "Epoch: 499 |  TrainLoss: 0.00104 | TestLoss: 0.14609 | TestAcc: 0.97674 | TestF1: 0.98\n",
            "Best WLoss: 0.00003 | Best Epoch: 358\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "best_test_loss = float('inf')  # Initialize with a large value\n",
        "\n",
        "# Without dropout training results\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  weighted_loss = exp_param * (weighted_loss) + (1 - exp_param) * (test_loss / len(test_loader.dataset))\n",
        "  \n",
        "  wloss.append(weighted_loss / (1 - exp_param ** (epoch + 1)))\n",
        "  \n",
        "  if test_loss < best_test_loss:\n",
        "    best_test_loss = test_loss  # Update the best test loss\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "        f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "# Print the best values\n",
        "best_wloss = min(wloss)\n",
        "best_epoch = wloss.index(best_wloss)\n",
        "print(f'Best WLoss: {best_wloss:.5f} | Best Epoch: {best_epoch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f820ec1",
      "metadata": {
        "id": "4f820ec1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}